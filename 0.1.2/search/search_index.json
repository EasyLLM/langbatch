{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>LangBatch is a unified API for accessing Batch APIs from providers like OpenAI, Anthropic, Azure OpenAI, GCP Vertex AI, AWS Bedrock, etc.  </p> <ul> <li> <p>Utlize Batch APIs for:</p> <ul> <li>Requests that don't require immediate responses.</li> <li>Low cost (usually 50% discount)</li> <li>Higher rate limits</li> <li>Example use cases: Data processing pipelines, Evaluations, Classifying huge data, Creating embeddings for large text contents</li> </ul> </li> <li> <p>Features:</p> <ul> <li>OpenAI format for requests and responses</li> <li>Utilities for handling the complete lifecycle of a batch job: Creating, Starting, Monitoring, Retrying and Processing</li> <li>Pipeline to convert stream of incoming requests into batch jobs</li> </ul> </li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>\ud83d\ude80 Get Started</p> <p>Learn the basics and become familiar with the LangBatch and how to utilize it.</p> <p> Get Started</p> </li> <li> <p>\ud83d\udcda Core Concepts</p> <p>The high-level explanations for building a better understand about the important topics such as how to utlize Batch classes for batch generations.</p> <p> Core Concepts</p> </li> <li> <p>\ud83d\udee0\ufe0f How-to Guides</p> <p>Practical guides to help you achieve a specific goals. Take a look at these guides to learn how to use LangBatch to solve real-world problems.</p> <p> How-to Guides</p> </li> <li> <p>\ud83d\udcd6 References</p> <p>Technical descriptions of how LangBatch classes and methods work.</p> <p> References</p> </li> </ul>"},{"location":"community/","title":"\u2764\ufe0f Community","text":"<p>\"Individually, we are one drop. Together, we are an ocean.\" - Ryunosuke Satoro</p>"},{"location":"community/#contributing","title":"Contributing","text":"<p>We welcome contributions to LangBatch! If you're looking to contribute to LangBatch, please submit a PR.</p> <p>If you have any issues or suggestions, please open an issue on our Issues. Please check out our Roadmap to see what we're working on.</p>"},{"location":"community/#shoutouts","title":"Shoutouts","text":"<p>We'd like to thank the following projects that inspired us to build LangBatch.</p> <ul> <li>LangChain</li> <li>Ragas - This documentation is heavily inspired by the Ragas documentation.</li> </ul>"},{"location":"concepts/","title":"\ud83d\udcda Core Concepts","text":"<p>LangBatch gives you building blocks to create pipelines for Batch AI inference. The core concepts are:</p> <ul> <li> <p>Batch Types</p> <p>What is a batch?</p> </li> <li> <p>Pipeline</p> <p>How to create a batch AI inference pipeline?</p> </li> </ul>"},{"location":"concepts/pipeline/","title":"Pipeline","text":"<p>LangBatch provides building blocks to create pipelines for AI inference. You can create a pipeline by combining these components. BatchHandler and BatchDispatcher are the core components of the pipeline.</p> <ul> <li> <p>Batch Handler</p> <p>Handler for the batches</p> </li> <li> <p>Batch Dispatcher</p> <p>Batching incoming requests</p> </li> </ul>"},{"location":"concepts/pipeline/batch_dispatcher/","title":"Batch Dispatcher","text":""},{"location":"concepts/pipeline/batch_dispatcher/#why-do-we-need-batch-dispatcher","title":"Why do we need Batch Dispatcher?","text":"<p>Batch Dispatcher is responsible for creating batches from incoming stream of requests and dispatching them to the batch handler in a balanced manner. This is useful in situations where you need to setup a API service and listen to incoming requests.</p> <p>Also, we need to keep track of all the incoming requests and maintain a queue.</p>"},{"location":"concepts/pipeline/batch_dispatcher/#initialize-a-batch-dispatcher","title":"Initialize a Batch Dispatcher","text":"<p>You can initialize a batch dispatcher by passing the batch handler, request queue, queue threshold, time threshold, time interval, and request kwargs.</p> <pre><code>from langbatch import BatchHandler\nfrom langbatch.request_queues import InMemoryRequestQueue\nfrom langbatch import BatchDispatcher\n\n# Create a batch handler\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch\n)\n\n# Create a request queue\nrequest_queue = InMemoryRequestQueue()\n\n# Create a batch dispatcher\nbatch_dispatcher = BatchDispatcher(\n    batch_handler=batch_handler,\n    queue=request_queue,\n    queue_threshold=50000, # 50000 requests\n    time_threshold=3600 * 2, # 2 hours\n    time_interval=600, # 10 minutes,\n    requests_type=\"partial\",\n    request_kwargs=request_kwargs\n)\n</code></pre> <p>Here, we are initializing a batch dispatcher with a InMemoryRequestQueue. </p> <p><code>queue_threshold</code> is the maximum number of requests that can be added to the queue. If the queue threshold is reached, then the requests will be converted into a batch and sent to the batch handler.</p> <p><code>time_threshold</code> is the maximum time interval for which a request can be waited in queue. Even if the queue threshold is not reached. Once the time threshold is reached, the requests in the queue will be converted into a batch and sent to the batch handler.</p> <p><code>time_interval</code> is the time interval for which the queue will be checked for the queue threshold and time threshold.</p> <p><code>requests_type</code> is the type of requests that will be added to the queue. It can be \"partial\" or \"full\". If it is 'partial', Batch.create method is used to create the batch, and if it is 'full', Batch.create_from_requests method is used.</p> <p><code>request_kwargs</code> is the kwargs that will be passed to the Batch.create method in Batch class to create a batch. Ex. temperature, max_tokens, etc. Used when <code>requests_type</code> is 'partial'.</p>"},{"location":"concepts/pipeline/batch_dispatcher/#run-the-batch-dispatcher","title":"Run the Batch Dispatcher","text":"<pre><code>asyncio.create_task(batch_dispatcher.run())\n</code></pre> <p>This will start a background task that will run indefinitely until the program is terminated.</p>"},{"location":"concepts/pipeline/batch_dispatcher/#add-requests-to-the-queue","title":"Add Requests to the Queue","text":"<pre><code># Add multiple requests to the queue\nawait request_queue.add_requests(requests)\n</code></pre>"},{"location":"concepts/pipeline/batch_dispatcher/#redis-request-queue","title":"Redis Request Queue","text":"<p>You can also use RedisRequestQueue to add requests to the queue. With RedisRequestQueue,  * you can add requests to the queue in a persisted, distributed manner * it can be shared across multiple processes and machines to add requests to the queue.</p> <pre><code>from langbatch.request_queues import RedisRequestQueue\nimport redis\n\nREDIS_URL = os.environ.get('REDIS_URL')\nredis_client = redis.from_url(REDIS_URL)\n\nrequest_queue = RedisRequestQueue(\n    redis_client=redis_client,\n    queue_name='gemini_requests'\n)\n</code></pre>"},{"location":"concepts/pipeline/batch_dispatcher/#custom-request-queue","title":"Custom Request Queue","text":"<p>You can also implement your own request queue by implementing the <code>RequestQueue</code>.</p> <pre><code>from langbatch.request_queues import RequestQueue\n\nclass CustomRequestQueue(RequestQueue):\n    pass\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/","title":"Batch Handler","text":""},{"location":"concepts/pipeline/batch_handler/#why-do-we-need-batch-handler","title":"Why do we need Batch Handler?","text":"<p>When we start a batch job in OpenAI or in other providers, it can fail due to various reasons like rate limits, quota limits, etc. We need a mechanism to check the status of the batch periodically and retry the failed batches. And we need to process the completed batches. Also we may need to keep track of the batch jobs and make sure of successful completion of all the batch jobs. </p> <p>And due to the Quota and Rate Limits, we need to handle the batches in a queue manner to avoid Rate Limit errors.</p> <p>BatchHandler is designed to handle all these things in a production environment.</p>"},{"location":"concepts/pipeline/batch_handler/#initialize-a-batch-handler","title":"Initialize a Batch Handler","text":"<p>You can initialize a batch handler by passing the batch process function and the batch type.</p> <pre><code>from langbatch import BatchHandler\n\n# Create a batch handler process\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch\n)\nasyncio.create_task(batch_handler.run())\n</code></pre> <p>batch_process_func is the function that will be called to process the batch. It should accept batch as the argument. You can put the logic to process the batch in this function.</p> <pre><code>def process_batch(batch: OpenAIChatCompletionBatch):\n    successful_results, _ = batch.get_results()\n    for result in successful_results:\n        # Process the result\n        pass\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#add-batches-to-the-queue","title":"Add Batches to the Queue","text":"<p>You can add batches to the queue by calling the <code>add_batch</code> method.</p> <pre><code>await batch_handler.add_batch(\"55d506ef-2a1f-4ca1-9c6c-3fd2415c83f7\")\nawait batch_handler.add_batch(\"10d71a17-6e29-4b1a-ba3d-245bd7cdf4f0\")\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#wait-time","title":"Wait Time","text":"<p>Wait time is the time in seconds to wait for processing the next set of batches in time intervals. It is used to avoid Rate Limit errors.</p> <pre><code>batch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    wait_time=1800 # 30 minutes\n)\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#with-custom-storage","title":"With Custom Storage","text":"<p>By default, BatchHandler uses <code>FileBatchQueue</code> to handle the batch queue. And <code>FileBatchStorage</code> to store the batches. You can implement and use custom implemetations of <code>BatchQueue</code> and <code>BatchStorage</code> for the batch handler by passing them to the <code>BatchHandler</code> constructor.</p> <pre><code>custom_batch_queue = MyCustomBatchQueue()\ncustom_batch_storage = MyCustomBatchStorage()\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=custom_batch_queue,\n    batch_storage=custom_batch_storage\n)\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#batch-kwargs","title":"Batch Kwargs","text":"<p>You can pass additional kwargs to the batch process function by passing the <code>batch_kwargs</code> parameter to the <code>BatchHandler</code> constructor. These kwargs are used to initialize the batch object.</p> <pre><code>batch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=VertexAIChatCompletionBatch,\n    batch_kwargs={\n        \"model\": \"gemini-2.0-flash-001\",\n        \"project\": \"my-project\",\n        \"location\": \"us-central1\",\n        \"bigquery_input_dataset\": \"input-dataset\",\n        \"bigquery_output_dataset\": \"output-dataset\"\n    }\n)\n</code></pre>"},{"location":"concepts/providers/","title":"\ud83d\udcda Providers","text":"<p>LangBatch currently supports the following providers:</p> <ul> <li> <p>OpenAI</p> </li> <li> <p>Anthropic</p> </li> <li> <p>Azure OpenAI</p> </li> <li> <p>VertexAI</p> </li> <li> <p>Bedrock</p> </li> <li> <p>Providers with OpenAI Batch API Support</p> </li> </ul> <p>Info</p> <p>LangBatch is designed to support multiple providers. We will add support for more providers in the future. Please raise an Issue in GitHub, if you need a support for a new provider.</p>"},{"location":"concepts/providers/Anthropic/","title":"Anthropic","text":"<p>You can run batch inference jobs on Anthropic models like Claude 3.5 Sonnet via LangBatch.</p>"},{"location":"concepts/providers/Anthropic/#data-format","title":"Data Format","text":"<p>OpenAI batch data format can be used for Anthropic. But make sure to use the Claude model names in the batch file.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"claude-3-5-haiku-20241022\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"claude-3-5-haiku-20241022\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/Anthropic/#create-chat-completion-batch","title":"Create Chat Completion Batch","text":"<pre><code>import os\nfrom langbatch import chat_completion_batch\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-api-key\"\n\nbatch = chat_completion_batch(\"path/to/batch-file.jsonl\", provider=\"anthropic\")\n</code></pre> <p>You can also pass the Anthropic client as an argument to the <code>chat_completion_batch</code> function.</p> <pre><code>from langbatch import chat_completion_batch\nfrom anthropic import Anthropic\n\nclient = Anthropic(api_key=\"your-anthropic-api-key\")\nbatch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"anthropic\", \n    client=client\n)\n</code></pre> <p>Refer to Anthropic Batch API Documentation for more information.</p>"},{"location":"concepts/providers/AzureOpenAI/","title":"Azure OpenAI","text":"<p>You can run batch inference jobs on Azure OpenAI models via LangBatch.</p>"},{"location":"concepts/providers/AzureOpenAI/#data-format","title":"Data Format","text":"<p>Our default OpenAI data format can be used for Azure OpenAI. But you need to replace the model name with the model deployment name.</p> Note <p>You need to choose 'Global Batch' for deployment type when creating the model deployment in Azure OpenAI. And by setting the deployment name as the model name (For example, <code>gpt-4o</code> or <code>gpt-4o-mini</code>), you can use the same OpenAI model names in the batch file.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"REPLACE-WITH-MODEL-DEPLOYMENT-NAME\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"REPLACE-WITH-MODEL-DEPLOYMENT-NAME\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/AzureOpenAI/#create-chat-completion-batch","title":"Create Chat Completion Batch","text":"<pre><code>import os\nfrom langbatch import chat_completion_batch\nos.environ[\"AZURE_API_KEY\"] = \"your-azure-openai-api-key\"\nos.environ[\"AZURE_API_BASE\"] = \"https://{resource-name}.openai.azure.com/\"\nos.environ[\"AZURE_API_VERSION\"] = \"2024-02-15-preview\"\n\nbatch = chat_completion_batch(\"path/to/batch-file.jsonl\", provider=\"azure\")\n</code></pre> <p>You can also pass the configuration values as arguments.</p> <pre><code>batch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"azure\", \n    api_key=\"your-azure-openai-api-key\",\n    azure_endpoint=\"https://{resource-name}.openai.azure.com/\", \n    api_version=\"2024-02-15-preview\"\n)\n</code></pre> <p>You can pass 'azure_ad_token_provider' instead of 'api_key'.</p> <pre><code>from openai import AzureOpenAI\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\n\ntoken_provider = get_bearer_token_provider(\n    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n)\n\nbatch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"azure\", \n    azure_ad_token_provider=token_provider,\n    azure_endpoint=\"https://{resource-name}.openai.azure.com/\", \n    api_version=\"2024-02-15-preview\"\n)\n</code></pre> <p>Info</p> <p>Azure OpenAI does not support Text Embedding models and Finetuned LLMs yet. Refer to Azure OpenAI Batch API Documentation for more information.</p>"},{"location":"concepts/providers/Bedrock/","title":"Bedrock","text":"<p>You can run batch inference jobs on Claude and Nova models available in Bedrock via LangBatch.</p>"},{"location":"concepts/providers/Bedrock/#data-format","title":"Data Format","text":"<p>OpenAI data format can be used in LangBatch for Bedrock. But the model name can be skipped here.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre> Note <p>In Bedrock, you can only send requests to a single model in a batch. If you want to use multiple models, you need to create multiple batches.</p>"},{"location":"concepts/providers/Bedrock/#bedrock-setup","title":"Bedrock Setup","text":"Info <p>Make sure you have the access to the Foundation Models, follow this guide to get access to the Foundation Models: Getting Access to Bedrock Foundation Models</p> <p>To use Bedrock, you need to setup few things. Please follow these steps:</p> <ul> <li>You need to set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables (From user with Bedrock Batch Inference permissions)</li> <li>Create two new S3 buckets. One for storing batch input and another one for storing batch output: For example <code>batch-input</code> and <code>batch-output</code></li> <li>And you need to create a service role using the instructions here</li> </ul> Note <p>You need to use the correct region according to the model you are using. Check this link for more available regions. Also, you need to create new S3 buckets for each region.</p>"},{"location":"concepts/providers/Bedrock/#create-chat-completion-batch","title":"Create Chat Completion Batch","text":"<pre><code>import os\nfrom langbatch import chat_completion_batch\n\n# Set the access credentials\nos.environ[\"AWS_ACCESS_KEY_ID\"] = \"your-aws-access-key-id\"\nos.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"your-aws-secret-access-key\"\n\n# Set the configuration values\nos.environ[\"AWS_INPUT_BUCKET\"] = \"your-input-bucket\"\nos.environ[\"AWS_OUTPUT_BUCKET\"] = \"your-output-bucket\"\nos.environ[\"AWS_REGION\"] = \"your-aws-region\" # us-west-2\nos.environ[\"AWS_SERVICE_ROLE\"] = \"your-service-role-arn\"\n\nbatch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"bedrock\",\n    model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n)\n</code></pre> <p>You can also pass the configuration values as arguments:</p> <pre><code>batch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"bedrock\",\n    model=\"us.amazon.nova-pro-v1:0\",\n    input_bucket=\"your-input-bucket\",\n    output_bucket=\"your-output-bucket\",\n    region=\"your-aws-region\", # us-east-1\n    service_role=\"your-service-role-arn\"\n)\n</code></pre>"},{"location":"concepts/providers/Bedrock/#supported-models","title":"Supported Models","text":"<p>You need to enable the models you want to use in Bedrock before using them. </p> <ul> <li>Claude 3.5 Sonnet v2, Claude 3.5 Haiku</li> <li>Nova Lite, Nova Micro, Nova Pro</li> </ul>"},{"location":"concepts/providers/Bedrock/#create-service-role","title":"Create Service Role","text":"<ol> <li>Go to Identity and Access Management (IAM)</li> <li>Click on \"Policies\" on the left sidebar -&gt; Click on \"Create policy\" -&gt; Click on \"JSON\" tab -&gt; Add the following JSON. Modify the bucket names accordingly -&gt; Click on \"Next\" -&gt; Provide the Name (For example <code>RolePolicyForBedrockBatchInference</code>) -&gt; Click on \"Create Policy\" <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::{batch-input-bucket-name}\",\n                \"arn:aws:s3:::{batch-input-bucket-name}/*\",\n                \"arn:aws:s3:::{batch-output-bucket-name}\",\n                \"arn:aws:s3:::{batch-output-bucket-name}/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre></li> <li>Click on \"Roles\" on the left sidebar -&gt; Click on \"Create role\" -&gt; Click on \"Custom trust policy\" -&gt; Paste the below JSON into Custom trust policy field-&gt; Click on \"Next\" -&gt; Click on \"Next\" <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"bedrock.amazonaws.com\"\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n</code></pre></li> <li>In the Add permissions section, Search for the policy you just created in the previous step and Select it. Then, Click on \"Next\".</li> <li>Provide the Role Name (For example <code>BedrockBatchInferenceServiceRole</code>) and Click on \"Create role\"</li> <li>Copy the Role ARN and use it in the Bedrock Batch creation.</li> </ol> Tip <p>If you are using multiple regions, include the region specific S3 buckets in the policy.</p>"},{"location":"concepts/providers/Bedrock/#user-permissions","title":"User Permissions","text":"Note <p>Make sure the AWS User you are using has access to Bedrock Batch Inference permissions. You can create the below policy and attach it to the user.</p> <ol> <li>Go to Identity and Access Management (IAM)</li> <li>Click on \"Policies\" on the left sidebar -&gt; Click on \"Create policy\" -&gt; Click on \"JSON\" tab -&gt; Add the following JSON. Modify the bucket names accordingly -&gt; Click on \"Next\" -&gt; Provide the Name (For example <code>UserPolicyForBedrockBatchInference</code>) -&gt; Click on \"Create Policy\" <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:GetFoundationModel\",\n                \"bedrock:GetFoundationModelAvailability\",\n                \"bedrock:ListFoundationModels\",\n                \"bedrock:InvokeModel\",\n                \"bedrock:StopModelInvocationJob\",\n                \"bedrock:CreateModelInvocationJob\",\n                \"bedrock:ListModelInvocationJobs\",\n                \"bedrock:GetModelInvocationJob\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::{batch-input-bucket-name}/*\",\n                \"arn:aws:s3:::{batch-input-bucket-name}\",\n                \"arn:aws:s3:::{batch-output-bucket-name}/*\",\n                \"arn:aws:s3:::{batch-output-bucket-name}\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"iam:PassRole\"\n            ],\n            \"Resource\": [\n                \"arn:aws:iam::{aws_account_id}:role/BedrockBatchInferenceServiceRole\"\n            ]\n        }\n    ]\n}\n</code></pre></li> <li>Click on \"Users\" on the left sidebar -&gt; Click on the user you want to attach the policy to -&gt; Click on \"Add permissions\" -&gt; Click on \"Attach existing policies directly\" -&gt; Click on \"Next\" -&gt; Click on \"Add permissions\"</li> </ol>"},{"location":"concepts/providers/OpenAI/","title":"OpenAI","text":""},{"location":"concepts/providers/OpenAI/#data-format","title":"Data Format","text":"<pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/OpenAI/#create-chat-completion-batch","title":"Create Chat Completion Batch","text":"<pre><code>import os\nfrom langbatch import chat_completion_batch\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n\n# Create a batch object\nbatch = chat_completion_batch(\"path/to/batch-file.jsonl\", provider=\"openai\")\n\nbatch.start()\n</code></pre> <p>You can pass the OpenAI client to the <code>chat_completion_batch</code> function. This also allows you to use other providers with OpenAI Batch API compatible API. Model name should be changed to the provider's model name in requests in the batch file.</p> <pre><code>from langbatch import chat_completion_batch\nfrom openai import OpenAI\nclient = OpenAI(\n    api_key='your-api-key',\n    base_url=\"https://your-custom-endpoint.com/\"\n)\n\nbatch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"openai\", \n    client=client\n)\n</code></pre>"},{"location":"concepts/providers/OpenAI/#create-embedding-batch","title":"Create Embedding Batch","text":"embedding-batch-file.jsonl<pre><code>{\"custom_id\": \"6d292082-5c65-4b70-9900-a1418e49d5e7\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"model\": \"text-embedding-3-small\", \"input\": \"I did not have a good day yesterday\"}}\n{\"custom_id\": \"b8bf1ed2-2ec3-45f9-81b0-97813ad70ae4\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"model\": \"text-embedding-3-small\", \"input\": \"I am feeling good today\"}}\n{\"custom_id\": \"3cb5b179-464f-4ebe-b405-531caba76dd7\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"model\": \"text-embedding-3-small\", \"input\": \"Ill be having fun tomorrow\"}}\n</code></pre> <pre><code>from langbatch import embedding_batch\n\nbatch = embedding_batch(\"path/to/embedding-batch-file.jsonl\", provider=\"openai\")\n</code></pre> Tip <p>Refer to OpenAI Batch API Documentation for more information.</p>"},{"location":"concepts/providers/VertexAI/","title":"Vertex AI","text":"<p>You can run batch inference jobs on Gemini, Claude and Llama models available in Vertex AI via LangBatch.</p>"},{"location":"concepts/providers/VertexAI/#data-format","title":"Data Format","text":"<p>OpenAI data format can be used in LangBatch for Vertex AI. But the model name can be skipped here.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre> Note <p>In VertexAI, you can only send requests to a single model in a batch. If you want to use multiple models, you need to create multiple batches.</p>"},{"location":"concepts/providers/VertexAI/#vertex-ai-initialization","title":"Vertex AI Initialization","text":"<p>Vertex AI should be initialized with the project id and location.</p> Note <p>You need to use the correct location according to the model you are using. Check this link for more available locations.</p> <pre><code>import os\nimport vertexai\n\nGCP_PROJECT = os.environ.get('GCP_PROJECT')\nGCP_LOCATION = os.environ.get('GCP_LOCATION')\nvertexai.init(project=GCP_PROJECT, location=GCP_LOCATION)\n</code></pre> <p>Tip</p> <p>You can use a service account to avoid the frequent authentication error <code>google.auth.exceptions.RefreshError: Reauthentication is needed. Please run \"gcloud auth application-default login\" to reauthenticate</code>. A service account is long-lived as it does not have an expiry time. You can create a service account with only required permissions - <code>Vertex AI user</code>, <code>BigQuery Data Editor</code> and <code>BigQuery User</code>. Check Service Account Creation for more information.</p> <p>You can either set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable to the path of the service account key file or use the following code to set the credentials.</p> <pre><code>import os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path/to/service-account-key.json\"\n</code></pre>"},{"location":"concepts/providers/VertexAI/#create-chat-completion-batch","title":"Create Chat Completion Batch","text":"<p>Vertex AI requires project and BigQuery datasets configuration. In Vertex AI, you can only use a single model for all requests in a batch. So you need to pass the model name to the <code>chat_completion_batch</code> function.</p> <pre><code>import os\nfrom langbatch import chat_completion_batch\n\nos.environ[\"GCP_PROJECT\"] = \"your-gcp-project\"\nos.environ[\"GCP_BIGQUERY_INPUT_DATASET\"] = \"your-input-dataset\"\nos.environ[\"GCP_BIGQUERY_OUTPUT_DATASET\"] = \"your-output-dataset\"\n\nbatch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"vertex_ai\", \n    model=\"gemini-2.0-flash-001\"\n)\n</code></pre> <p>You can also pass the configuration values as arguments:</p> <pre><code>batch = chat_completion_batch(\n    \"path/to/batch-file.jsonl\", \n    provider=\"vertex_ai\",\n    model=\"gemini-2.0-flash-001\",\n    gcp_project=\"your-gcp-project\",\n    bigquery_input_dataset=\"your-input-dataset\",\n    bigquery_output_dataset=\"your-output-dataset\"\n)\n</code></pre> <p>Info</p> <p>You need to make sure that the BigQuery datasets are created before running the batch. They need to be in the same project and location as the Vertex AI Batch.</p>"},{"location":"concepts/providers/VertexAI/#partner-models","title":"Partner Models","text":"<p>You can also use the partner models available in VertexAI. Claude from Anthropic and Llama from Meta are available in VertexAI. You need to enable them in below links before using them. </p> <ul> <li>Claude 3.5 Sonnet v2, Claude 3.5 Haiku, Llama 3.1 models, Llama 3.3 70B</li> </ul> <p>Available models:</p> <ul> <li>Claude 3.5 Sonnet v2 (claude-3-5-sonnet-v2@20241022)</li> <li>Claude 3.5 Haiku (claude-3-5-haiku@20241022)</li> <li>Llama 3.1 405B (llama-3.1-405b-instruct-maas)</li> <li>Llama 3.3 70B (llama-3.3-70b-instruct-maas)</li> <li>Llama 3.1 70B (llama-3.1-70b-instruct-maas)</li> <li>Llama 3.1 8B (llama-3.1-8b-instruct-maas)</li> </ul>"},{"location":"concepts/types/","title":"Batch and Batch Types","text":"<p>Batch is the core building block of LangBatch. A batch instance is created with a JSONL file containing lot of individual requests.  This collection of requests is sent to an AI provider as a single unit when we start a batch. </p> <p>There are different types of batches in LangBatch for different types of AI tasks. For Example, LangBatch has <code>ChatCompletionBatch</code> for Chat Completion tasks with Large Language Models and <code>EmbeddingBatch</code> for creating text embeddings. We will add more batch types in future to support more AI tasks.</p> <ul> <li> <p>Batch Class</p> <p>How to create batch?</p> </li> <li> <p>Chat Completion</p> <p>How to create a batch for Chat Completion?</p> </li> <li> <p>Embedding</p> <p>How to create a batch for Embedding?</p> </li> </ul>"},{"location":"concepts/types/batch/","title":"Batch","text":""},{"location":"concepts/types/batch/#initialize-a-batch","title":"Initialize a Batch","text":"<p>You can initialize a batch by passing the path to a JSONL file. File should be in OpenAI format.</p> <pre><code>from langbatch import OpenAIChatCompletionBatch\n\nbatch = OpenAIChatCompletionBatch(\"data.jsonl\")\nprint(batch.id)\n</code></pre> <p>You can also pass a list of requests to the <code>create</code> method to create a batch.</p> <pre><code>batch = OpenAIChatCompletionBatch.create([\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]},\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n        ]\n    }\n])\n</code></pre> <p>Info</p> <p>When you initialize a batch, it is not started automatically. You need to call the <code>start</code> method to start the batch job.</p>"},{"location":"concepts/types/batch/#start-a-batch","title":"Start a Batch","text":"<p>You can start a batch by calling the <code>start</code> method.</p> <pre><code>batch.start()\n</code></pre>"},{"location":"concepts/types/batch/#get-batch-status","title":"Get Batch Status","text":"<p>You can get the status of a batch by calling the <code>get_status</code> method.</p> <pre><code>batch.get_status()\n</code></pre>"},{"location":"concepts/types/batch/#retry-batch","title":"Retry Batch","text":"<p>Incase of any failure with batch job due to rate limits, exceeded 24h wait time or other issues, you can retry the same batch by calling the <code>retry</code> method.</p> <pre><code>batch.retry()\n</code></pre> <p>Info</p> <p>A batch instance in LangBatch can be retried multiple times. It will internally create multiple batch jobs until the batch is successful.</p>"},{"location":"concepts/types/batch/#get-batch-results","title":"Get Batch Results","text":"<p>You can get the results of a completed batch by calling the <code>get_results</code> method.</p> <pre><code>successful_results, unsuccessful_results = batch.get_results()\n\nfor result in successful_results:\n    print(result)\n</code></pre>"},{"location":"concepts/types/batch/#get-batch-results-file","title":"Get Batch Results File","text":"<p>You can get the results file of a completed batch by calling the <code>get_results_file</code> method. This file will be in OpenAI Batch Result JSONL format.</p> <pre><code>file_path = batch.get_results_file()\n\nwith open(file_path, \"r\") as file:\n    results = file.readlines()\n\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"concepts/types/batch/#get-unsuccessful-requests","title":"Get Unsuccessful Requests","text":"<p>You can get the unsuccessful requests of a batch by calling the <code>get_unsuccessful_requests</code> method. This will be useful to retry failed requests or to debug the issues with the requests.</p> <pre><code>unsuccessful_requests = batch.get_unsuccessful_requests()\n\nfor request in unsuccessful_requests:\n    print(request)\n</code></pre>"},{"location":"concepts/types/batch/#get-requests-by-custom-ids","title":"Get Requests by Custom IDs","text":"<p>You can get the requests of a batch by passing the custom ids to the <code>get_requests_by_custom_ids</code> method. This will be useful to get the requests to debug the issues with the requests.</p> <pre><code>custom_ids = [\"custom_id_1\", \"custom_id_2\"]\nrequests = batch.get_requests_by_custom_ids(custom_ids)\n\nfor request in requests:\n    print(request)\n</code></pre>"},{"location":"concepts/types/batch/#save-batch","title":"Save Batch","text":"<p>You can save a batch by calling the <code>save</code> method. This will be useful to keep track of created batches and resume the batch job later.</p> <pre><code># save batch\nbatch.save()\n\n# Save Batch to Custom Storage\nstorage = CustomStorage()\nbatch = OpenAIChatCompletionBatch(\"data.jsonl\")\nbatch.save(storage=storage)\n</code></pre> <p>Info</p> <p>By default, File based storage will be used to save the batch. You can also pass a custom storage instance when initializing the batch. For example, you can save the batch to a cloud storage.</p>"},{"location":"concepts/types/batch/#load-batch","title":"Load Batch","text":"<p>You can load a batch by calling the <code>load</code> method. This will be useful to resume the batch job later.</p> <pre><code># load batch\nbatch = OpenAIChatCompletionBatch.load(\"batch_id\")\n\n# Load Batch from Custom Storage\nstorage = CustomStorage()\nbatch = OpenAIChatCompletionBatch.load(\"batch_id\", storage=storage)\n</code></pre> <p>You can pass additional kwargs to the batch when loading the batch.</p> <pre><code>from openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\", base_url=\"your-base-url\")\n\n# Load batch\nbatch = OpenAIChatCompletionBatch.load(\"batch_id\", batch_kwargs={\"client\": client})\n</code></pre> Info <p>This is needed only for provider classes where you usually pass client objects when initializing the batch. Usually for OpenAI API supported providers like Azure OpenAI.</p>"},{"location":"concepts/types/chat_completion/","title":"Chat Completion Batch","text":"<p><code>ChatCompletionBatch</code> is the abstract batch class for processing chat completion requests. It's designed to utilize various Language Models (LLMs), using the OpenAI Chat Completion API format for requests and responses.</p> <p>By standardizing on the OpenAI format, <code>ChatCompletionBatch</code> ensures consistency and interoperability across different LLM providers within the LangBatch.</p>"},{"location":"concepts/types/chat_completion/#initialize-a-chat-completion-batch","title":"Initialize a Chat Completion Batch","text":"<p>You can initialize a ChatCompletionBatch by passing the path to a JSONL file. File should be in OpenAI batch File format and requests should be in OpenAI Chat Completion format.</p> <pre><code>from langbatch import OpenAIChatCompletionBatch\n\nbatch = OpenAIChatCompletionBatch(\"data.jsonl\")\n</code></pre> <p>You can also pass a list of requests to the batch.</p> <pre><code>messages_list = [\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]},\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]}\n]\nbatch = OpenAIChatCompletionBatch.create(messages_list)\n\n# Initializing with request kwargs and batch kwargs\nfrom openai import OpenAI\nclient = OpenAI(api_key=\"your-api-key\", base_url=\"provider-base-url\")\nbatch = OpenAIChatCompletionBatch.create(\n    messages_list, \n    request_kwargs={\"temperature\": 0.3, \"max_tokens\": 500},\n    batch_kwargs={ \"client\": client }\n)\n</code></pre> Info <p>You can only pass the 'messages' list in the requests here. And the provided <code>request_kwargs</code> will be applied to all requests in the batch. This is useful for cases where you want to use the same inference configuration to all requests in the batch and only the 'messages' list is different for each request.</p>"},{"location":"concepts/types/chat_completion/#get-results","title":"Get Results","text":"<p>In ChatCompletionBatch, the successful results contain <code>choices</code> and <code>custom_id</code> keys.</p> <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(f\"Custom ID: {result['custom_id']}\")\n    print(f\"Content: {result['choices'][0]['message']['content']}\")\n</code></pre>"},{"location":"concepts/types/embedding/","title":"Embedding Batch","text":"<p><code>EmbeddingBatch</code> is the abstract batch class for processing embedding requests. Same as <code>ChatCompletionBatch</code>, it uses the OpenAI Batch File format for requests and responses.</p>"},{"location":"concepts/types/embedding/#initialize-an-embedding-batch","title":"Initialize an Embedding Batch","text":"<p>You can initialize an <code>EmbeddingBatch</code> by passing the path to a file. The file should be in OpenAI Batch File format.</p> <pre><code>from langbatch import OpenAIEmbeddingBatch\n\nbatch = OpenAIEmbeddingBatch(\"data.jsonl\")\n</code></pre> <p>You can also pass a list of texts to the batch.</p> <pre><code>batch = OpenAIEmbeddingBatch.create([\n    \"Lincoln was the 16th President of the United States. His face is on Mount Rushmore.\", \n    \"Steve Jobs was the co-founder of Apple Inc. He was considered a visionary and a pioneer in the personal computer revolution.\"\n])\n\n# Initializing with request kwargs\nbatch = OpenAIEmbeddingBatch.create([\n    \"Hello World\",\n    \"Hello LangBatch\"\n], request_kwargs={\"model\": \"text-embedding-3-large\"})\n</code></pre>"},{"location":"concepts/types/embedding/#get-results","title":"Get Results","text":"<p>In EmbeddingBatch, the successful results contain <code>embedding</code> and <code>custom_id</code> keys.</p> <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(f\"Custom ID: {result['custom_id']}\")\n    print(result[\"embedding\"])\n</code></pre>"},{"location":"getstarted/","title":"\ud83d\ude80 Get Started","text":"<p>Welcome to the LangBatch! If you're new to LangBatch, the Get Started guides will walk you through the fundamentals of working with LangBatch. These tutorials assume basic knowledge of Python.</p> <p>Let's get started!</p> <ul> <li> <p>Installation</p> <p>Learn how to install LangBatch.</p> </li> <li> <p>Running a Batch with LangBatch</p> <p>Learn how to successfully run a batch job using LangBatch.</p> </li> </ul> <p>Note</p> <p>The tutorials only provide an overview of what you can accomplish with LangBatch and the basic skills needed to utilize it effectively. For an in-depth explanation of the core concepts behind LangBatch, check out the Core Concepts page. You can also explore the How-to Guides for specific applications of LangBatch.</p>"},{"location":"getstarted/batch/","title":"Quickstart","text":""},{"location":"getstarted/batch/#prepare-the-batch-file","title":"Prepare the batch file","text":"batch-file.jsonl<pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"getstarted/batch/#create-a-batch-object","title":"Create a batch object","text":"<pre><code>from langbatch import chat_completion_batch\n\nbatch = chat_completion_batch(\"path/to/batch-file.jsonl\", provider=\"openai\")\n</code></pre>"},{"location":"getstarted/batch/#start-the-batch-job","title":"Start the batch job","text":"<p>Creating a batch object will not start the batch job. You need to start the batch job explicitly.</p> <pre><code>batch.start()\n</code></pre>"},{"location":"getstarted/batch/#check-the-status-of-the-batch-job","title":"Check the status of the batch job","text":"<p>To check the status of the batch job, use the <code>get_status</code> method:</p> <pre><code>status = batch.get_status()\nprint(status)\n</code></pre>"},{"location":"getstarted/batch/#get-the-results-of-the-batch-job","title":"Get the results of the batch job","text":"<p>After the batch job is successful, you can get the results using the <code>get_results</code> method:</p> <pre><code>if batch.get_status() == \"completed\":\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(f\"Custom ID: {result['custom_id']}\")\n        print(f\"Content: {result['choices'][0]['message']['content']}\")\n</code></pre> Tip <p>Learn more about the batch actions in the Batch page.</p>"},{"location":"getstarted/batch/#data-path","title":"Data Path","text":"<p>By default, LangBatch will save the batch related files in the <code>langbatch_data</code> directory in the current working directory. You can change this by setting the <code>LANGBATCH_DATA_PATH</code> environment variable.</p> <pre><code>import os\nos.environ[\"LANGBATCH_DATA_PATH\"] = \"/path/to/your/data\"\n</code></pre>"},{"location":"getstarted/install/","title":"Installation","text":"<p>To get started, install LangBatch using <code>pip</code> with the following command:</p> <pre><code>pip install langbatch\n</code></pre> <p>This will install the core LangBatch package. </p>"},{"location":"getstarted/install/#optional-provider-dependencies","title":"Optional Provider Dependencies","text":"<p>Core LangBatch package only supports OpenAI and OpenAI compatible providers. For Other providers, you need to install additional dependencies.</p> <ul> <li> <p>Anthropic: <pre><code>pip install langbatch[Anthropic]\n</code></pre></p> </li> <li> <p>GCP Vertex AI: <pre><code>pip install langbatch[VertexAI]\n</code></pre></p> </li> <li> <p>AWS Bedrock: <pre><code>pip install langbatch[Bedrock]\n</code></pre></p> </li> </ul>"},{"location":"getstarted/install/#utility-and-integration-dependencies","title":"Utility and Integration Dependencies","text":"<p>These are optional dependencies for using other utilities and integrations.</p> <ul> <li>Redis: <pre><code>pip install langbatch[redis]\n</code></pre></li> </ul> <p>This will install the dependencies for using RedisRequestQueue with LangBatch.</p>"},{"location":"getstarted/install/#install-all-dependencies","title":"Install all dependencies","text":"<pre><code>pip install langbatch[all]\n</code></pre> <p>This will install the dependencies for using all the utilities and integrations with LangBatch.</p>"},{"location":"getstarted/install/#install-from-main-branch","title":"Install from main branch","text":"<p>If you'd like to experiment with the latest features, install the most recent version from the main branch:</p> <pre><code>pip install git+https://github.com/EasyLLM/langbatch.git\n</code></pre>"},{"location":"getstarted/install/#install-from-source","title":"Install from source","text":"<p>If you're planning to contribute and make modifications to the code, ensure that you clone the repository and set it up as an editable install.</p> <pre><code>git clone https://github.com/EasyLLM/langbatch.git \ncd langbatch \npip install -e .\n</code></pre> <p>This will enable you to run LangBatch locally from the source code with immediate effect on changes without needing to reinstall.</p> <p>Next, let's create a ChatCompletionBatch object and perform batch generations.</p>"},{"location":"getstarted/provider_batches/","title":"Running a Batch with LangBatch","text":"<p>LangBatch provides a simple interface to run batch jobs.</p> <pre><code>from langbatch import OpenAIChatCompletionBatch\n\n# Create a batch object\nbatch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n\n# Start the batch job\nbatch.start()\n</code></pre> <p>To check the status of the batch job, use the <code>get_status</code> method:</p> <pre><code>status = batch.get_status()\nprint(status)\n</code></pre> <p>To get the results of the batch job, use the <code>get_results</code> method:</p> <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(f\"Custom ID: {result['custom_id']}\")\n    print(f\"Content: {result['choices'][0]['message']['content']}\")\n</code></pre> <p>Tip</p> <p>You can perform the same actions with other providers and models.  For example, use the <code>AnthropicChatCompletionBatch</code> class to run batches with the Anthropic models. Check out the Providers section to learn more.</p> <pre><code>from langbatch import AnthropicChatCompletionBatch\n\nbatch = AnthropicChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre>"},{"location":"howtos/","title":"\ud83d\udee0\ufe0f How-to Guides","text":"<p>The how-to guides offer a more comprehensive overview of all the tools LangBatch offers and how to use them. This will help you tackle messier real-world usecases.</p> <p>The guides assume you are familiar and confortable with the LangBatch basics. If your not feel free to checkout the Get Started sections first.</p>"},{"location":"howtos/batch_dispatcher_service/","title":"A sample API service - Stream to Batch pipeline","text":"<p>In production, you may want to implement a REST API service that accepts invidual requests or list of requests and process them as batches later. </p> <p>Refer to Batch Dispatcher for more details.</p> <pre><code># main.py\nimport logging\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel\n\nfrom fastapi import FastAPI, HTTPException\nfrom langbatch import BatchHandler\nfrom langbatch.batch_queues import FileBatchQueue\nfrom langbatch.batch_storages import FileBatchStorage\nfrom langbatch.openai import OpenAIChatCompletionBatch\nfrom langbatch import BatchDispatcher\nfrom langbatch.request_queues import InMemoryRequestQueue\n\nlogging.basicConfig(level=logging.INFO)\n\n# Function to process the successfully completed batch\ndef process_batch(batch: OpenAIChatCompletionBatch):\n    successful_results, unsuccessful_results = batch.get_results()\n    for successful_result in successful_results:\n        print(successful_result[\"custom_id\"])\n        print(successful_result[\"choices\"][0][\"message\"][\"content\"])\n\n        # TODO: process the successful result\n\n# Initialize Batch Handler and Batch Dispatcher\nbatch_queue = FileBatchQueue(\"batch_queue.json\")\nbatch_storage = FileBatchStorage()\nhandler = BatchHandler(\n    batch_process_func = process_batch, \n    batch_type = OpenAIChatCompletionBatch, \n    batch_queue = batch_queue,\n    batch_storage = batch_storage,\n    wait_time = 3600 # check batches every 1 hour\n)\n\nrequest_kwargs = {\n    \"model\": \"gpt-4o-mini\",\n    \"max_tokens\": 1000,\n    \"temperature\": 0.2\n}\nqueue = InMemoryRequestQueue()\ndispatcher = BatchDispatcher(\n    batch_handler = handler, \n    queue = queue, \n    queue_threshold = 50000, # dispatch batches when the queue size &gt;= queue_threshold\n    time_threshold = 3600, # dispatch batch when the seconds since the last dispatch &gt;= time_threshold\n    # even if the queue size is less than the queue threshold\n    time_interval = 600, # check the conditions every 600 seconds to dispatch batches\n    requests_type = 'partial', # partial requests (only messages)\n    request_kwargs = request_kwargs\n)\n\n# start the dispatcher and processor in the background\ndef run_dispatcher_in_background():\n    loop = asyncio.get_event_loop()\n    loop.create_task(dispatcher.run())\n\ndef run_processor_in_background():\n    loop = asyncio.get_event_loop()\n    loop.create_task(handler.run())\n\nrun_dispatcher_in_background()\nrun_processor_in_background()\n\n# FastAPI API Service setup\napp = FastAPI()\n\nclass MessagesList(BaseModel):\n    data: List\n\nasync def handle_requests(messages_list: MessagesList):\n    # add requests to the queue\n    await asyncio.to_thread(dispatcher.queue.add_requests, list(messages_list.data))\n\n@app.post(\"/requests\")\nasync def handle_requests_api(messages_list: MessagesList):\n    try:\n        await handle_requests(messages_list)\n        return {\"status\": \"success\"}\n    except Exception as e:\n        logging.error(f\"Error handling requests: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre> <p>Now run command <pre><code>uvicorn main:app --reload\n</code></pre></p> <p>Use <code>curl</code> to send a POST request to the API service <pre><code>curl -X 'POST' \\\n  'http://127.0.0.1:8000/requests' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"data\": [\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"How can I learn Python?\"\n                }\n            ],\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Who is the first president of the United States?\"\n                }\n            ]\n        ]\n   }'\n</code></pre></p>"},{"location":"howtos/batch_dispatcher_service_redis/","title":"Stream to Batch pipeline with RedisRequestQueue","text":"<p>The previous guide uses InMemoryRequestQueue to store the incoming requests. In production, you may want to use a persistent queue to store the incoming requests.</p> <p>Redis is a popular in-memory data store with persistent storage, and it's easy to set up and use with LangBatch. This guide will show you how to set up a Redis-based RedisRequestQueue.</p> <p>Replace <pre><code>request_queue = InMemoryRequestQueue()\n</code></pre></p> <p>with <pre><code>import os\nimport redis\n\nREDIS_URL = os.environ.get('REDIS_URL')\nredis_client = redis.from_url(REDIS_URL)\n\nrequest_queue = RedisRequestQueue(redis_client, queue_name='stream_requests')\n</code></pre></p>"},{"location":"references/","title":"References","text":"<p>Reference documents for the <code>langbatch</code> package.</p>"},{"location":"references/Batch/","title":"Batch","text":""},{"location":"references/Batch/#langbatch.Batch.Batch","title":"langbatch.Batch.Batch","text":"<p>               Bases: <code>ABC</code></p> <p>Batch class is the base class for all batch classes.</p> <p>Implementations of this class will be platform specific (OpenAI, Vertex AI, etc.)</p> Source code in <code>langbatch\\Batch.py</code> <pre><code>class Batch(ABC):\n    \"\"\"\n    Batch class is the base class for all batch classes.\n\n    Implementations of this class will be platform specific (OpenAI, Vertex AI, etc.)\n    \"\"\"\n    _url: str = \"\"\n    platform_batch_id: str | None = None\n\n    def __init__(self, file: str):\n        \"\"\"\n        Initialize the Batch class.\n\n        Args:\n            file (str): The path to the batch file. File should be in OpenAI compatible batch file in jsonl format.\n        \"\"\"\n        self._file = file\n        self.id = str(uuid.uuid4())\n\n        self._validate_requests() # Validate the requests in the batch file\n\n    @classmethod\n    def _create_batch_file_from_requests(cls, requests) -&gt; Path:\n        try:\n            batches_dir = Path(DATA_PATH) / \"created_batches\"\n            batches_dir.mkdir(exist_ok=True, parents=True)\n\n            id = str(uuid.uuid4())\n            file_path = batches_dir / f\"{id}.jsonl\"\n            with jsonlines.open(file_path, mode='w') as writer:\n                writer.write_all(requests)\n        except:\n            logging.error(f\"Error creating batch file\", exc_info=True)\n            return None\n\n        return file_path\n\n    @classmethod\n    def _create_batch_file(cls, key: str, data: List[Any], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; Path | None:\n        \"\"\"\n        Create the batch file when given a list of items.\n        For Chat Completions, this would be a list of messages.\n        For Embeddings, this would be a list of texts.\n        \"\"\"\n        requests = []\n        try:\n            for item in data:\n                try:\n                    body = request_kwargs.copy()  # Copy kwargs to avoid mutation\n                    custom_id = str(uuid.uuid4())\n\n                    body[key] = item\n\n                    request = {\n                        \"custom_id\": custom_id,\n                        \"method\": \"POST\",\n                        \"url\": cls._url,\n                        \"body\": body\n                    }\n                    requests.append(request)\n                except:\n                    logging.warning(f\"Error processing item {item}\", exc_info= True)\n                    continue\n        except:\n            logging.error(f\"Error creating requests from data to create batch file\", exc_info=True)\n            return None\n\n        file_path = cls._create_batch_file_from_requests(requests)\n\n        if file_path is None:\n            raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n        return cls(file_path, **batch_kwargs)\n\n    @classmethod\n    def create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n        \"\"\"\n        Creates a batch when given a list of requests. \n        These requests should be in correct Batch API request format as per the Batch type.\n        Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n        Args:\n            requests: A list of requests.\n            batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n        Returns:\n            An instance of the Batch class.\n\n        Raises:\n            BatchInitializationError: If the input data is invalid.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch.create_from_requests([\n            {   \"custom_id\": \"request-1\",\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": \"gpt-4o-mini\",\n                    \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                    \"max_tokens\": 1000\n                }\n            },\n            {\n                \"custom_id\": \"request-2\",\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": \"gpt-4o-mini\",\n                    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                    \"max_tokens\": 1000\n                }\n            }\n        ]\n        ``` \n        \"\"\"\n\n        file_path = cls._create_batch_file_from_requests(requests)\n\n        if file_path is None:\n            raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n        return cls(file_path, **batch_kwargs)\n\n    @classmethod\n    @abstractmethod\n    def _get_init_args(cls, meta_data) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the init arguments from meta data json file when loading a batch from storage.\n        \"\"\"\n        pass\n\n    @classmethod\n    def load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n        \"\"\"\n        Load a batch from the storage and return a Batch object.\n\n        Args:\n            id (str): The id of the batch.\n            storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n            batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n        Returns:\n            Batch: The batch object.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n        ```\n        \"\"\"\n        data_file, meta_file = storage.load(id)\n\n        # Load metadata based on file extension\n        if meta_file.suffix == '.json':\n            with open(meta_file, 'r') as f:\n                meta_data = json.load(f)\n        else:  # .pkl\n            with open(meta_file, 'rb') as f:\n                meta_data = pickle.load(f)\n\n        init_args = cls._get_init_args(meta_data)\n\n        for key, value in batch_kwargs.items():\n            if key not in init_args:\n                init_args[key] = value\n\n        batch = cls(str(data_file), **init_args)\n        batch.platform_batch_id = meta_data['platform_batch_id']\n        batch.id = id\n\n        return batch\n\n    @abstractmethod\n    def _create_meta_data(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Create the meta data for the batch to be saved in the storage.\n        \"\"\"\n        pass\n\n    def save(self, storage: BatchStorage = FileBatchStorage()):\n        \"\"\"\n        Save the batch to the storage.\n\n        Args:\n            storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch(file)\n        batch.save()\n\n        # save the batch to file storage\n        batch.save(storage=FileBatchStorage(\"./data\"))\n        ```\n        \"\"\"\n        meta_data = self._create_meta_data()\n        meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n        storage.save(self.id, Path(self._file), meta_data)\n\n    @abstractmethod\n    def _upload_batch_file(self):\n        pass\n\n    @abstractmethod\n    def start(self):\n        \"\"\"\n        Usage:\n        ```python\n        # create a batch\n        batch = OpenAIChatCompletionBatch(file)\n\n        # start the batch process\n        batch.start()\n        ```\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_status(self):\n        \"\"\"\n        Usage:\n        ```python\n        # create a batch and start batch process\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        # get the status of the batch process\n        status = batch.get_status()\n        print(status)\n        ```\n        \"\"\"\n        pass\n\n    def _get_requests(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get all the requests from the jsonl batch file.\n        \"\"\"\n        requests = []\n        try:\n            with jsonlines.open(self._file) as reader:\n                for obj in reader:\n                    requests.append(obj)\n        except:\n            logging.error(f\"Error reading requests from batch file\", exc_info=True)\n            raise BatchError(\"Error reading requests from batch file\")\n\n        return requests\n\n    @abstractmethod\n    def _validate_request(self, request):\n        pass\n\n    def _validate_requests(self) -&gt; None:\n        \"\"\"\n        Validate all the requests in the batch file before starting the batch process.\n\n        Depends on the implementation of the _validate_request method in the subclass.\n        \"\"\"\n        invalid_requests = []\n        for request in self._get_requests():\n            valid = True\n            try:\n                self._validate_request(request['body'])\n            except:\n                logging.info(f\"Invalid request: {request}\", exc_info=True)\n                valid = False\n\n            if not valid:\n                invalid_requests.append(request['custom_id'])\n\n        if len(invalid_requests) &gt; 0:\n            raise BatchValidationError(f\"Invalid requests: {invalid_requests}\")\n\n        if len(self._get_requests()) == 0:\n            raise BatchValidationError(\"No requests found in the batch file\")\n\n    def _create_results_file_path(self):\n        results_dir = Path(DATA_PATH) / \"results\"\n        results_dir.mkdir(exist_ok=True)\n\n        return results_dir / f\"{self.id}.jsonl\"\n\n    @abstractmethod\n    def _download_results_file(self):\n        pass\n\n    # return results file in OpenAI compatible format\n    def get_results_file(self):\n        \"\"\"\n        Usage:\n        ```python\n        import jsonlines\n\n        # create a batch and start batch process\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        if batch.get_status() == \"completed\":\n            # get the results file\n            results_file = batch.get_results_file()\n\n            with jsonlines.open(results_file) as reader:\n                for obj in reader:\n                    print(obj)\n        ```\n        \"\"\"\n        file_path = self._download_results_file()\n        return file_path\n\n    def _prepare_results(\n        self, process_func\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n        \"\"\"\n        Prepare the results file by processing the results,\n        and separating them into successful and unsuccessful results\n        based on the status code of the response.\n\n        Depends on the implementation of the process_func method in the subclass.\n        \"\"\"\n\n        file_id = self._download_results_file()\n\n        if file_id is None:\n            return None, None\n\n        try:\n            results = []\n            with jsonlines.open(file_id) as reader:\n                for obj in reader:\n                    results.append(obj)\n\n            successful_results = []\n            unsuccessful_results = []\n            for result in results:\n                if result['response'] is None:\n                    if result['error'] is not None:\n                        error = {\n                            \"custom_id\": result['custom_id'],\n                            \"error\": result['error']\n                        }\n                    else:\n                        error = {\n                            \"custom_id\": result['custom_id'],\n                            \"error\": \"No response from the API\"\n                        }\n                    unsuccessful_results.append(error)\n                    continue\n\n                if result['response']['status_code'] == 200:\n                    choices = {\n                        \"custom_id\": result['custom_id'],\n                        **process_func(result)\n                    }\n                    successful_results.append(choices)\n                else:\n                    error = {\n                        \"custom_id\": result['custom_id'],\n                        \"error\": result['error']\n                    }\n                    unsuccessful_results.append(error)\n\n            return successful_results, unsuccessful_results\n        except:\n            logging.error(f\"Error preparing results file\", exc_info=True)\n            return None, None\n\n    # return results list\n    @abstractmethod\n    def get_results(self):\n        pass\n\n    @abstractmethod\n    def is_retryable_failure(self) -&gt; bool:\n        pass\n\n    # Retry on rate limit fail cases\n    @abstractmethod\n    def retry(self):\n        pass\n\n    def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Retrieve the unsuccessful requests from the batch.\n\n        Returns:\n            A list of requests that failed.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        if batch.get_status() == \"completed\":\n            # get the unsuccessful requests\n            unsuccessful_requests = batch.get_unsuccessful_requests()\n\n            for request in unsuccessful_requests:\n                print(request[\"custom_id\"])\n        ```\n        \"\"\"\n        custom_ids = []\n        _, unsuccessful_results = self.get_results()\n        for result in unsuccessful_results:\n            custom_ids.append(result[\"custom_id\"])\n\n        return self.get_requests_by_custom_ids(custom_ids)\n\n    def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Retrieve the requests from the batch file by custom ids.\n\n        Args:\n            custom_ids (List[str]): A list of custom ids.\n\n        Returns:\n            A list of requests.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        if batch.get_status() == \"completed\":\n            # get the requests by custom ids\n            requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n            for request in requests:\n                print(request[\"custom_id\"])\n        ```\n        \"\"\"\n        requests = []\n        with jsonlines.open(self._file) as reader:\n            for request in reader:\n                if request[\"custom_id\"] in custom_ids:\n                    requests.append(request)\n        return requests\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.__init__","title":"__init__","text":"<pre><code>__init__(file: str)\n</code></pre> <p>Initialize the Batch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the batch file. File should be in OpenAI compatible batch file in jsonl format.</p> </li> </ul> Source code in <code>langbatch\\Batch.py</code> <pre><code>def __init__(self, file: str):\n    \"\"\"\n    Initialize the Batch class.\n\n    Args:\n        file (str): The path to the batch file. File should be in OpenAI compatible batch file in jsonl format.\n    \"\"\"\n    self._file = file\n    self.id = str(uuid.uuid4())\n\n    self._validate_requests() # Validate the requests in the batch file\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start()\n</code></pre> <p>Usage: <pre><code># create a batch\nbatch = OpenAIChatCompletionBatch(file)\n\n# start the batch process\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@abstractmethod\ndef start(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch\n    batch = OpenAIChatCompletionBatch(file)\n\n    # start the batch process\n    batch.start()\n    ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_status","title":"get_status  <code>abstractmethod</code>","text":"<pre><code>get_status()\n</code></pre> <p>Usage: <pre><code># create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\n# get the status of the batch process\nstatus = batch.get_status()\nprint(status)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@abstractmethod\ndef get_status(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    # get the status of the batch process\n    status = batch.get_status()\n    print(status)\n    ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/ChatCompletion/","title":"ChatCompletionBatch","text":""},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch","title":"langbatch.ChatCompletionBatch.ChatCompletionBatch","text":"<p>               Bases: <code>Batch</code></p> <p>ChatCompletionBatch is a base class for chat completion batch classes. Utilizes OpenAI Chat Completion API format as the standard request format.</p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>class ChatCompletionBatch(Batch):\n    \"\"\"\n    ChatCompletionBatch is a base class for chat completion batch classes.\n    Utilizes OpenAI Chat Completion API format as the standard request format.\n    \"\"\"\n    _url: str = \"/v1/chat/completions\"\n\n    def __init__(self, file) -&gt; None:\n        \"\"\"\n        Initialize the ChatCompletionBatch class.\n        \"\"\"\n        super().__init__(file)\n\n    @classmethod\n    def create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n        \"\"\"\n        Create a chat completion batch when given a list of messages.\n\n        Args:\n            data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n            request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n            batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n        Returns:\n            An instance of the ChatCompletionBatch class.\n\n        Raises:\n            BatchInitializationError: If the input data is invalid.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch.create([\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n            ],\n            request_kwargs={\"model\": \"gpt-4o\"})\n\n        # For Vertex AI\n        batch = VertexAIChatCompletionBatch.create([\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n            ],\n            request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n            batch_kwargs={\n                \"gcp_project\": \"your-gcp-project\", \n                \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n                \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n            })\n        ```\n        \"\"\"\n        return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n\n    def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n        \"\"\"\n        Retrieve the results of the chat completion batch.\n\n        Returns:\n            A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n        Usage:\n        ```python\n        successful_results, unsuccessful_results = batch.get_results()\n        for result in successful_results:\n            print(result[\"choices\"])\n        ```\n        \"\"\"\n        process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n        return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file) -&gt; None\n</code></pre> <p>Initialize the ChatCompletionBatch class.</p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def __init__(self, file) -&gt; None:\n    \"\"\"\n    Initialize the ChatCompletionBatch class.\n    \"\"\"\n    super().__init__(file)\n</code></pre>"},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/Embedding/","title":"EmbeddingBatch","text":""},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch","title":"langbatch.EmbeddingBatch.EmbeddingBatch","text":"<p>               Bases: <code>Batch</code></p> <p>EmbeddingBatch is a base class for embedding batch classes. Utilizes OpenAI Embedding API format as the standard request format.</p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>class EmbeddingBatch(Batch):\n    \"\"\"\n    EmbeddingBatch is a base class for embedding batch classes.\n    Utilizes OpenAI Embedding API format as the standard request format.\n    \"\"\"\n    _url: str = \"/v1/embeddings\" \n\n    def __init__(self, file) -&gt; None:\n        \"\"\"\n        Initialize the EmbeddingBatch class.\n        \"\"\"\n        super().__init__(file)\n\n    @classmethod\n    def create(cls, data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"EmbeddingBatch\":\n        \"\"\"\n        Create an embedding batch when given a list of texts.\n\n        Args:\n            data (List[str]): A list of texts to be embedded.\n            request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, encoding_format, etc.\n            batch_kwargs (Dict): Additional keyword arguments for the batch class.\n\n        Returns:\n            An instance of the EmbeddingBatch class.\n\n        Raises:\n            BatchInitializationError: If the input data is invalid.\n\n        Usage:\n        ```python\n        batch = OpenAIEmbeddingBatch.create([\n            \"Hello world\", \n            \"Hello LangBatch\"\n        ], \n            request_kwargs={\"model\": \"text-embedding-3-small\"})\n        ```\n        \"\"\"\n        return cls._create_batch_file(\"input\", data, request_kwargs, batch_kwargs)\n\n    def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n        \"\"\"\n        Retrieve the results of the embedding batch.\n\n        Returns:\n            A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n        Usage:\n        ```python\n        successful_results, unsuccessful_results = batch.get_results()\n        for result in successful_results:\n            print(result[\"embedding\"])\n        ```\n        \"\"\"\n        process_func = lambda result: {\"embedding\": result['response']['body']['data'][0]['embedding']}\n        return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch.__init__","title":"__init__","text":"<pre><code>__init__(file) -&gt; None\n</code></pre> <p>Initialize the EmbeddingBatch class.</p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>def __init__(self, file) -&gt; None:\n    \"\"\"\n    Initialize the EmbeddingBatch class.\n    \"\"\"\n    super().__init__(file)\n</code></pre>"},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; EmbeddingBatch\n</code></pre> <p>Create an embedding batch when given a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[str]</code>)           \u2013            <p>A list of texts to be embedded.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, encoding_format, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EmbeddingBatch</code>           \u2013            <p>An instance of the EmbeddingBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIEmbeddingBatch.create([\n    \"Hello world\", \n    \"Hello LangBatch\"\n], \n    request_kwargs={\"model\": \"text-embedding-3-small\"})\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"EmbeddingBatch\":\n    \"\"\"\n    Create an embedding batch when given a list of texts.\n\n    Args:\n        data (List[str]): A list of texts to be embedded.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, encoding_format, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class.\n\n    Returns:\n        An instance of the EmbeddingBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIEmbeddingBatch.create([\n        \"Hello world\", \n        \"Hello LangBatch\"\n    ], \n        request_kwargs={\"model\": \"text-embedding-3-small\"})\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"input\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the embedding batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"embedding\"])\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the embedding batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"embedding\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"embedding\": result['response']['body']['data'][0]['embedding']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/","title":"AnthropicChatCompletionBatch","text":""},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch","title":"langbatch.anthropic.AnthropicChatCompletionBatch","text":"<p>               Bases: <code>AnthropicBatch</code>, <code>ChatCompletionBatch</code></p> <p>AnthropicChatCompletionBatch is a class for Anthropic chat completion batches.</p> <p>Usage: <pre><code>batch = AnthropicChatCompletionBatch(\"path/to/file.jsonl\", client)\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\anthropic.py</code> <pre><code>class AnthropicChatCompletionBatch(AnthropicBatch, ChatCompletionBatch):\n    \"\"\"\n    AnthropicChatCompletionBatch is a class for Anthropic chat completion batches.\n\n    Usage:\n    ```python\n    batch = AnthropicChatCompletionBatch(\"path/to/file.jsonl\", client)\n    batch.start()\n    ```\n    \"\"\"\n    def _convert_request(self, req: dict) -&gt; Request:\n        custom_id = req[\"custom_id\"]\n        request = convert_request(req)\n\n        anthropic_request = Request(\n            custom_id=custom_id,\n            params=MessageCreateParamsNonStreaming(**request)\n        )\n        return anthropic_request\n\n    def _convert_response(self, response) -&gt; dict:\n        return convert_response(response)\n\n    def _validate_request(self, request):\n        AnthropicChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, client: Optional[Anthropic] = None) -&gt; None\n</code></pre> <p>Initialize the AnthropicBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batch format.</p> </li> <li> <code>client</code>               (<code>Anthropic</code>, default:                   <code>None</code> )           \u2013            <p>The Anthropic client.</p> </li> </ul> <p>Usage: <pre><code>batch = AnthropicChatCompletionBatch(\n    \"path/to/file.jsonl\"\n)\n</code></pre></p> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def __init__(self, file: str, client: Optional[Anthropic] = None) -&gt; None:\n    \"\"\"\n    Initialize the AnthropicBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batch format.\n        client (Anthropic): The Anthropic client.\n\n    Usage:\n    ```python\n    batch = AnthropicChatCompletionBatch(\n        \"path/to/file.jsonl\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = client or Anthropic()\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    response = self._client.beta.messages.batches.retrieve(\n        self.platform_batch_id\n    )\n    return anthropic_state_map[response.processing_status]\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    status = self.get_status()\n    if status == \"errored\" or status == \"expired\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/","title":"BedrockClaudeChatCompletionBatch","text":""},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch","title":"langbatch.bedrock.BedrockClaudeChatCompletionBatch","text":"<p>               Bases: <code>BedrockBatch</code>, <code>ChatCompletionBatch</code></p> <p>BedrockClaudeChatCompletionBatch is a class for Bedrock chat completion batches with Claude models.</p> <p>Usage: <pre><code>batch = BedrockClaudeChatCompletionBatch(\n    \"path/to/file.jsonl\", \n    \"model\",\n    \"input_bucket\",\n    \"output_bucket\",\n    \"region\",\n    \"service_role\"\n)\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\bedrock.py</code> <pre><code>class BedrockClaudeChatCompletionBatch(BedrockBatch, ChatCompletionBatch):\n    \"\"\"\n    BedrockClaudeChatCompletionBatch is a class for Bedrock chat completion batches with Claude models.\n\n    Usage:\n    ```python\n    batch = BedrockClaudeChatCompletionBatch(\n        \"path/to/file.jsonl\", \n        \"model\",\n        \"input_bucket\",\n        \"output_bucket\",\n        \"region\",\n        \"service_role\"\n    )\n    batch.start()\n    ```\n    \"\"\"\n    def _convert_request(self, req: dict):\n        custom_id = req[\"custom_id\"]\n        request = convert_request(req)\n        request[\"anthropic_version\"] = \"bedrock-2023-05-31\"\n        del request[\"model\"]\n\n        return {\"recordId\": custom_id, \"modelInput\": request}\n\n    def _convert_response(self, response) -&gt; dict:\n        res= convert_message(response['modelOutput'], response[\"recordId\"])\n        error = None\n\n        output = {\n            \"id\": f'{response[\"recordId\"]}',\n            \"custom_id\": response[\"recordId\"],\n            \"response\": res,\n            \"error\": error\n        }\n        return output\n\n    def _validate_request(self, request):\n        AnthropicChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.input_bucket","title":"input_bucket  <code>instance-attribute</code>","text":"<pre><code>input_bucket = input_bucket\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.output_bucket","title":"output_bucket  <code>instance-attribute</code>","text":"<pre><code>output_bucket = output_bucket\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.service_role","title":"service_role  <code>instance-attribute</code>","text":"<pre><code>service_role = service_role\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.region","title":"region  <code>instance-attribute</code>","text":"<pre><code>region = region\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, model: str, input_bucket: str, output_bucket: str, region: str, service_role: str) -&gt; None\n</code></pre> <p>Initialize the BedrockBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batch format.</p> </li> </ul> <p>Usage: <pre><code>batch = BedrockChatCompletionBatch(\n    \"path/to/file.jsonl\",\n    \"model\",\n    \"input_bucket\",\n    \"output_bucket\",\n    \"region\"\n)\n</code></pre></p> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def __init__(self, file: str, model: str, input_bucket: str, output_bucket: str, region: str, service_role: str) -&gt; None:\n    \"\"\"\n    Initialize the BedrockBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batch format.\n\n    Usage:\n    ```python\n    batch = BedrockChatCompletionBatch(\n        \"path/to/file.jsonl\",\n        \"model\",\n        \"input_bucket\",\n        \"output_bucket\",\n        \"region\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = boto3.client(service_name='bedrock', region_name=region)\n    self._s3_client = boto3.resource('s3')\n\n    self.model = model\n    self.input_bucket = input_bucket\n    self.output_bucket = output_bucket\n    self.service_role = service_role\n    self.region = region\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = self._client.get_model_invocation_job(\n        jobIdentifier=self.platform_batch_id\n    )\n    return bedrock_state_map[job['status']]\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    error = self._get_errors()\n    if error:\n        return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockClaudeChatCompletionBatch/#langbatch.bedrock.BedrockClaudeChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/","title":"BedrockNovaChatCompletionBatch","text":""},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch","title":"langbatch.bedrock.BedrockNovaChatCompletionBatch","text":"<p>               Bases: <code>BedrockBatch</code>, <code>ChatCompletionBatch</code></p> <p>BedrockNovaChatCompletionBatch is a class for Bedrock chat completion batches with Nova models.</p> <p>Usage: <pre><code>batch = BedrockNovaChatCompletionBatch(\n    \"path/to/file.jsonl\", \n    \"model\",\n    \"input_bucket\",\n    \"output_bucket\",\n    \"region\",\n    \"service_role\"\n)\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\bedrock.py</code> <pre><code>class BedrockNovaChatCompletionBatch(BedrockBatch, ChatCompletionBatch):\n    \"\"\"\n    BedrockNovaChatCompletionBatch is a class for Bedrock chat completion batches with Nova models.\n\n    Usage:\n    ```python\n    batch = BedrockNovaChatCompletionBatch(\n        \"path/to/file.jsonl\", \n        \"model\",\n        \"input_bucket\",\n        \"output_bucket\",\n        \"region\",\n        \"service_role\"\n    )\n    batch.start()\n    ```\n    \"\"\"\n    def _convert_request(self, req: dict):\n        custom_id = req[\"custom_id\"]\n        request = convert_request_nova(req)\n\n        return {\"recordId\": custom_id, \"modelInput\": request}\n\n    def _convert_response(self, response) -&gt; dict:\n        return convert_response_nova(response, self.model)\n\n    def _validate_request(self, request):\n        AnthropicChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.input_bucket","title":"input_bucket  <code>instance-attribute</code>","text":"<pre><code>input_bucket = input_bucket\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.output_bucket","title":"output_bucket  <code>instance-attribute</code>","text":"<pre><code>output_bucket = output_bucket\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.service_role","title":"service_role  <code>instance-attribute</code>","text":"<pre><code>service_role = service_role\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.region","title":"region  <code>instance-attribute</code>","text":"<pre><code>region = region\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, model: str, input_bucket: str, output_bucket: str, region: str, service_role: str) -&gt; None\n</code></pre> <p>Initialize the BedrockBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batch format.</p> </li> </ul> <p>Usage: <pre><code>batch = BedrockChatCompletionBatch(\n    \"path/to/file.jsonl\",\n    \"model\",\n    \"input_bucket\",\n    \"output_bucket\",\n    \"region\"\n)\n</code></pre></p> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def __init__(self, file: str, model: str, input_bucket: str, output_bucket: str, region: str, service_role: str) -&gt; None:\n    \"\"\"\n    Initialize the BedrockBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batch format.\n\n    Usage:\n    ```python\n    batch = BedrockChatCompletionBatch(\n        \"path/to/file.jsonl\",\n        \"model\",\n        \"input_bucket\",\n        \"output_bucket\",\n        \"region\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = boto3.client(service_name='bedrock', region_name=region)\n    self._s3_client = boto3.resource('s3')\n\n    self.model = model\n    self.input_bucket = input_bucket\n    self.output_bucket = output_bucket\n    self.service_role = service_role\n    self.region = region\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = self._client.get_model_invocation_job(\n        jobIdentifier=self.platform_batch_id\n    )\n    return bedrock_state_map[job['status']]\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    error = self._get_errors()\n    if error:\n        return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\bedrock.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/Bedrock/BedrockNovaChatCompletionBatch/#langbatch.bedrock.BedrockNovaChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/","title":"OpenAIChatCompletionBatch","text":""},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch","title":"langbatch.openai.OpenAIChatCompletionBatch","text":"<p>               Bases: <code>OpenAIBatch</code>, <code>ChatCompletionBatch</code></p> <p>OpenAIChatCompletionBatch is a class for OpenAI chat completion batches. Can be used for batch processing with gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4 models</p> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>class OpenAIChatCompletionBatch(OpenAIBatch, ChatCompletionBatch):\n    \"\"\"\n    OpenAIChatCompletionBatch is a class for OpenAI chat completion batches.\n    Can be used for batch processing with gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4 models\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n    ```\n    \"\"\"\n    _url: str = \"/v1/chat/completions\"\n\n    def _validate_request(self, request):\n        OpenAIChatCompletionRequest(**request)\n\n    # Override the upload batch file method to fix requests for Azure OpenAI\n    def _upload_batch_file(self):\n        if isinstance(self._client, AzureOpenAI):\n            requests = self._get_requests()\n\n            modified_requests = []\n            for request in requests:\n                modified_requests.append(self._fix_request_for_azure(request))\n\n            with jsonlines.open(self._file, mode=\"w\") as writer:\n                writer.write_all(modified_requests)\n\n        # Upload the batch file to OpenAI\n        with open(self._file, \"rb\") as file:\n            batch_input_file  = self._client.files.create(file=file, purpose=\"batch\")\n            return batch_input_file.id\n\n    def _fix_request_for_azure(self, request):\n        \"\"\"\n        Azure OpenAI does not support passing None for content field in messages.\n        This function fixes this issue by adding an empty string to the message content.\n        \"\"\"\n        modified_request = request.copy()\n        for message in modified_request[\"body\"][\"messages\"]:\n            if message[\"content\"] is None:\n                message[\"content\"] = \"\"\n        return modified_request\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, client: Optional[OpenAI | AzureOpenAI] = None) -&gt; None\n</code></pre> <p>Initialize the OpenAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batchformat.</p> </li> <li> <code>client</code>               (<code>OpenAI</code>, default:                   <code>None</code> )           \u2013            <p>The OpenAI client to use. Defaults to OpenAI().</p> </li> </ul> <p>Usage: <pre><code>batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n# With custom OpenAI client\nclient = OpenAI(\n    api_key=\"sk-proj-...\",\n    base_url=\"https://api.provider.com/v1\"\n)\nbatch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def __init__(self, file: str, client: Optional[OpenAI | AzureOpenAI] = None) -&gt; None:\n    \"\"\"\n    Initialize the OpenAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batchformat.\n        client (OpenAI, optional): The OpenAI client to use. Defaults to OpenAI().\n\n    Usage:\n    ```python\n    batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n    # With custom OpenAI client\n    client = OpenAI(\n        api_key=\"sk-proj-...\",\n        base_url=\"https://api.provider.com/v1\"\n    )\n    batch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = client or OpenAI()\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    batch_input_file_id = self._upload_batch_file()\n    self._create_batch(batch_input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n    return batch.status\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    errors = self._get_errors()\n    if errors:\n        error = errors.data[0]['code']\n\n        if error == \"token_limit_exceeded\":\n            return True\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n\n    self._create_batch(batch.input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.cancel","title":"cancel","text":"<pre><code>cancel()\n</code></pre> <p>Usage: <pre><code># create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\n# cancel the batch process\nbatch.cancel()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def cancel(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    # cancel the batch process\n    batch.cancel()\n    ```\n    \"\"\"\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    batch = self._client.batches.cancel(self.platform_batch_id)\n    if batch.status == \"cancelling\" or batch.status == \"cancelled\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/","title":"OpenAIEmbeddingBatch","text":""},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch","title":"langbatch.openai.OpenAIEmbeddingBatch","text":"<p>               Bases: <code>OpenAIBatch</code>, <code>EmbeddingBatch</code></p> <p>OpenAIEmbeddingBatch is a class for OpenAI embedding batches. Can be used for batch processing with text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002 models</p> <p>Usage: <pre><code>batch = OpenAIEmbeddingBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>class OpenAIEmbeddingBatch(OpenAIBatch, EmbeddingBatch):\n    \"\"\"\n    OpenAIEmbeddingBatch is a class for OpenAI embedding batches.\n    Can be used for batch processing with text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002 models\n\n    Usage:\n    ```python\n    batch = OpenAIEmbeddingBatch(\"path/to/file.jsonl\")\n    batch.start()\n    ```\n    \"\"\"\n    _url: str = \"/v1/embeddings\"\n\n    def _validate_request(self, request):\n        OpenAIEmbeddingRequest(**request)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, client: Optional[OpenAI | AzureOpenAI] = None) -&gt; None\n</code></pre> <p>Initialize the OpenAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batchformat.</p> </li> <li> <code>client</code>               (<code>OpenAI</code>, default:                   <code>None</code> )           \u2013            <p>The OpenAI client to use. Defaults to OpenAI().</p> </li> </ul> <p>Usage: <pre><code>batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n# With custom OpenAI client\nclient = OpenAI(\n    api_key=\"sk-proj-...\",\n    base_url=\"https://api.provider.com/v1\"\n)\nbatch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def __init__(self, file: str, client: Optional[OpenAI | AzureOpenAI] = None) -&gt; None:\n    \"\"\"\n    Initialize the OpenAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batchformat.\n        client (OpenAI, optional): The OpenAI client to use. Defaults to OpenAI().\n\n    Usage:\n    ```python\n    batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n    # With custom OpenAI client\n    client = OpenAI(\n        api_key=\"sk-proj-...\",\n        base_url=\"https://api.provider.com/v1\"\n    )\n    batch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = client or OpenAI()\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    batch_input_file_id = self._upload_batch_file()\n    self._create_batch(batch_input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n    return batch.status\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the embedding batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"embedding\"])\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the embedding batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"embedding\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"embedding\": result['response']['body']['data'][0]['embedding']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    errors = self._get_errors()\n    if errors:\n        error = errors.data[0]['code']\n\n        if error == \"token_limit_exceeded\":\n            return True\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n\n    self._create_batch(batch.input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; EmbeddingBatch\n</code></pre> <p>Create an embedding batch when given a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[str]</code>)           \u2013            <p>A list of texts to be embedded.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, encoding_format, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EmbeddingBatch</code>           \u2013            <p>An instance of the EmbeddingBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIEmbeddingBatch.create([\n    \"Hello world\", \n    \"Hello LangBatch\"\n], \n    request_kwargs={\"model\": \"text-embedding-3-small\"})\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"EmbeddingBatch\":\n    \"\"\"\n    Create an embedding batch when given a list of texts.\n\n    Args:\n        data (List[str]): A list of texts to be embedded.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, encoding_format, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class.\n\n    Returns:\n        An instance of the EmbeddingBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIEmbeddingBatch.create([\n        \"Hello world\", \n        \"Hello LangBatch\"\n    ], \n        request_kwargs={\"model\": \"text-embedding-3-small\"})\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"input\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.cancel","title":"cancel","text":"<pre><code>cancel()\n</code></pre> <p>Usage: <pre><code># create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\n# cancel the batch process\nbatch.cancel()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def cancel(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    # cancel the batch process\n    batch.cancel()\n    ```\n    \"\"\"\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    batch = self._client.batches.cancel(self.platform_batch_id)\n    if batch.status == \"cancelling\" or batch.status == \"cancelled\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/","title":"VertexAIChatCompletionBatch","text":""},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch","title":"langbatch.vertexai.VertexAIChatCompletionBatch","text":"<p>               Bases: <code>VertexAIBatch</code>, <code>ChatCompletionBatch</code></p> <p>VertexAIChatCompletionBatch is a class for Vertex AI chat completion batches. Can be used for batch processing with Gemini 1.5 Flash and Gemini 1.5 Pro models</p> <p>Usage: <pre><code>batch = VertexAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>class VertexAIChatCompletionBatch(VertexAIBatch, ChatCompletionBatch):\n    \"\"\"\n    VertexAIChatCompletionBatch is a class for Vertex AI chat completion batches.\n    Can be used for batch processing with Gemini 1.5 Flash and Gemini 1.5 Pro models\n\n    Usage:\n    ```python\n    batch = VertexAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n    ```\n    \"\"\"\n    _url: str = \"/v1/chat/completions\"\n\n    def _convert_request(self, req: dict) -&gt; str:\n        custom_schema = {\n            \"contents\": [],\n            \"systemInstruction\": None,\n            \"tools\": [],\n            \"generationConfig\": {}\n        }\n        request = json.loads(VertexAIChatCompletionRequest(**req[\"body\"]).model_dump_json())\n\n        # Track tool responses to match with tool calls\n        tool_responses = {}\n\n        # First pass - collect tool responses\n        for message in request[\"messages\"]:\n            if message[\"role\"] == \"tool\":\n                tool_call_id = message[\"tool_call_id\"]\n                if tool_call_id:\n                    tool_responses[tool_call_id] = {\n                        \"response\": json.loads(message[\"content\"])\n                    }\n\n        # Second pass - process messages\n        for message in request[\"messages\"]:\n            role = message[\"role\"]\n            content = message[\"content\"]\n\n            function_calls = []\n            tool_responses_cache = []\n            if message.get(\"tool_calls\"):\n                for tool_call in message[\"tool_calls\"]:\n                    function_calls.append({\n                        \"functionCall\": {\n                            \"name\": tool_call[\"function\"][\"name\"],\n                            \"args\": json.loads(tool_call[\"function\"][\"arguments\"])\n                        }\n                    })\n                    # If we have a response for this tool call, add it in the next message\n                    if tool_call[\"id\"] in tool_responses:\n                        response = tool_responses[tool_call[\"id\"]]\n                        response[\"name\"] = tool_call[\"function\"][\"name\"]\n                        tool_responses_cache.append({\n                            \"functionResponse\": response\n                        })\n\n            if role == \"system\":\n                custom_schema[\"systemInstruction\"] = {\n                    \"role\": \"system\",\n                    \"parts\": {\"text\": content}\n                }\n            elif role != \"tool\":  # Skip tool messages as we handle them separately\n                if len(function_calls) &gt; 0:\n                    custom_schema[\"contents\"].append({\n                        \"role\": role,\n                        \"parts\": function_calls\n                    })\n                    custom_schema[\"contents\"].append({\n                        \"role\": \"model\",\n                        \"parts\": tool_responses_cache\n                    })\n                elif content is not None:\n                    custom_schema[\"contents\"].append({\n                        \"role\": role,\n                        \"parts\": {\"text\": content}\n                    })\n\n        # Convert tools\n        if request[\"tools\"]:\n            for tool in request[\"tools\"]:\n                function = tool.get(\"function\", {})\n\n                custom_schema[\"tools\"].append({\n                    \"functionDeclarations\": [{\n                        \"name\": function.get(\"name\"),\n                        \"description\": function.get(\"description\", \"\"),\n                        \"parameters\": function.get(\"parameters\", {})\n                    }]\n                })\n\n        # Convert generation config\n        gen_config = custom_schema[\"generationConfig\"]\n        if request.get(\"temperature\"):\n            gen_config[\"temperature\"] = request[\"temperature\"]\n        if request.get(\"top_p\"):\n            gen_config[\"topP\"] = request[\"top_p\"]\n        if request.get(\"max_tokens\"):\n            gen_config[\"maxOutputTokens\"] = request[\"max_tokens\"]\n        if request.get(\"n\"):\n            gen_config[\"candidateCount\"] = request[\"n\"]\n        if request.get(\"presence_penalty\"):\n            gen_config[\"presencePenalty\"] = request[\"presence_penalty\"]\n        if request.get(\"frequency_penalty\"):\n            gen_config[\"frequencyPenalty\"] = request[\"frequency_penalty\"]\n        if request.get(\"stop\"):\n            gen_config[\"stopSequences\"] = request[\"stop\"] if isinstance(request[\"stop\"], list) else [request[\"stop\"]] if request[\"stop\"] else None\n        if request.get(\"seed\"):\n            gen_config[\"seed\"] = request[\"seed\"]\n\n        if request.get(\"response_format\"):\n            mime_type_map = {\n                \"json_object\": \"application/json\",\n                \"text\": \"text/plain\",\n                \"json_schema\": \"application/json\"\n            }\n\n            gen_config[\"responseMimeType\"] = mime_type_map[request[\"response_format\"][\"type\"]]\n\n            if request[\"response_format\"][\"type\"] == \"json_schema\" and request[\"response_format\"][\"json_schema\"]:\n                gen_config[\"responseSchema\"] = request[\"response_format\"][\"json_schema\"]\n\n                # Check for single enum property to use text/x.enum mime type\n                data = json.loads(request[\"response_format\"][\"json_schema\"][\"schema\"])\n                concrete_types = [\"string\", \"number\", \"integer\", \"boolean\"]\n                if data.get(\"type\") in concrete_types and len(data.get(\"enum\", [])) == 0:\n                    gen_config[\"responseMimeType\"] = \"text/x.enum\"\n\n\n        gen_config = {k: v for k, v in gen_config.items() if v is not None}\n        custom_schema[\"generationConfig\"] = gen_config\n\n        return { \n            \"custom_id\": req[\"custom_id\"],\n            \"request\": json.dumps(custom_schema, indent=2)\n        }\n\n    def _convert_response(self, response):\n        # Parse the input JSON\n        response_data = json.loads(response[\"response\"])\n\n        status = response[\"status\"]\n        if status != \"\":\n            if \"Bad Request: \" in status:\n                error_data = json.loads(status.split(\"Bad Request: \")[1])\n            else:\n                error_data = {\n                    \"message\": status,\n                    \"code\": \"server_error\"\n                }\n\n            error = {\n                \"message\": error_data[\"error\"][\"message\"],\n                \"code\": error_data[\"error\"][\"code\"]\n            }\n\n            res = None\n        else:\n            # Extract relevant information\n            candidates = response_data[\"candidates\"]\n            tokens = response_data[\"usageMetadata\"]\n\n            # Create the choices array\n            choices = []\n            for index, candidate in enumerate(candidates):\n                choice = {\n                    \"index\": index,\n                    \"logprobs\": None,\n                    \"finish_reason\": candidate[\"finishReason\"].lower()\n                }\n\n                tool_calls = []\n                text_part = None\n                for part in candidate[\"content\"][\"parts\"]:\n                    if part.get(\"functionCall\", None):\n                        tool_call = {\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": part[\"functionCall\"].get(\"name\"),\n                                \"arguments\": json.dumps(part[\"functionCall\"].get(\"args\", {}))\n                            }\n                        }\n                        tool_calls.append(tool_call)\n                    else:\n                        text_part = part[\"text\"]\n\n                message = {\n                    \"role\": \"assistant\",\n                    \"content\": text_part\n                }\n                if len(tool_calls) &gt; 0:\n                    message[\"tool_calls\"] = tool_calls\n\n                choice[\"message\"] = message\n                choices.append(choice)\n\n            usage = {\n                \"prompt_tokens\": tokens.get(\"promptTokenCount\", 0),\n                \"completion_tokens\": tokens.get(\"candidatesTokenCount\", 0),\n                \"total_tokens\": tokens.get(\"totalTokenCount\", 0)\n            }\n\n            # Create the body\n            body = {\n                \"id\": f'{response[\"custom_id\"]}',\n                \"object\": \"chat.completion\",\n                \"created\": int(response[\"processed_time\"].timestamp()),\n                \"model\": self.model,\n                \"system_fingerprint\": None,\n                \"choices\": choices,\n                \"usage\": usage\n            }\n\n            res = {\n                \"request_id\": response[\"custom_id\"],\n                \"status_code\": 200,\n                \"body\": body,\n            }\n\n            error = None\n\n        # create output\n        output = {\n            \"id\": f'{response[\"custom_id\"]}',\n            \"custom_id\": response[\"custom_id\"],\n            \"response\": res,\n            \"error\": error\n        }\n\n        return output\n\n    def _validate_request(self, request):\n        VertexAIChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.gcp_project","title":"gcp_project  <code>instance-attribute</code>","text":"<pre><code>gcp_project = gcp_project\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.bigquery_input_dataset","title":"bigquery_input_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_input_dataset = bigquery_input_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.bigquery_output_dataset","title":"bigquery_output_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, model: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None\n</code></pre> <p>Initialize the VertexAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in Vertex AI batch format.</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use for the batch prediction.</p> </li> <li> <code>gcp_project</code>               (<code>str</code>)           \u2013            <p>The GCP project to use for the batch prediction.</p> </li> <li> <code>bigquery_input_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction input.</p> </li> <li> <code>bigquery_output_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction output.</p> </li> </ul> <p>Usage: <pre><code>batch = VertexAIBatch(\n    \"path/to/file.jsonl\",\n    \"model\",\n    \"gcp_project\",\n    \"bigquery_input_dataset\",\n    \"bigquery_output_dataset\"\n)\n</code></pre></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def __init__(self, file: str, model: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None:\n    \"\"\"\n    Initialize the VertexAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in Vertex AI batch format.\n        model (str): The name of the model to use for the batch prediction.\n        gcp_project (str): The GCP project to use for the batch prediction.\n        bigquery_input_dataset (str): The BigQuery dataset to use for the batch prediction input.\n        bigquery_output_dataset (str): The BigQuery dataset to use for the batch prediction output.\n\n    Usage:\n    ```python\n    batch = VertexAIBatch(\n        \"path/to/file.jsonl\",\n        \"model\",\n        \"gcp_project\",\n        \"bigquery_input_dataset\",\n        \"bigquery_output_dataset\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n\n    self.model = model\n    self.gcp_project = gcp_project\n    self.bigquery_input_dataset = bigquery_input_dataset\n    self.bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    input_dataset = self._upload_batch_file()\n    output_dataset_id = self._create_table(self.bigquery_output_dataset)\n    output_dataset = f\"bq://{self.gcp_project}.{self.bigquery_output_dataset}.{output_dataset_id}\"\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    return vertexai_state_map[str(job.state.name)]\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    # TODO: implement retry logic for Vertex AI API\n    error = self._get_errors()\n    if error:\n        logging.error(f\"Error in VertexAI Batch: {error}\")\n        if \"Failed to import data. Not found: Dataset\" in error:\n            return False\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    input_dataset = job._gca_resource.input_config.bigquery_source.input_uri\n    output_dataset = job._gca_resource.output_config.bigquery_destination.output_uri\n\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/","title":"VertexAIClaudeChatCompletionBatch","text":""},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch","title":"langbatch.vertexai.VertexAIClaudeChatCompletionBatch","text":"<p>               Bases: <code>VertexAIBatch</code>, <code>ChatCompletionBatch</code></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>class VertexAIClaudeChatCompletionBatch(VertexAIBatch, ChatCompletionBatch):\n    _url: str = \"/v1/chat/completions\"\n    _publisher: str = \"anthropic\"\n    _field_name: str = \"request\"\n\n    def _convert_request(self, req: dict) -&gt; str:\n        request = convert_request(req)\n        request[\"anthropic_version\"] = \"vertex-2023-10-16\"\n        del request[\"model\"]\n\n        return {\n            \"custom_id\": req[\"custom_id\"],\n            \"request\": json.dumps(request, indent=2)\n        }\n\n    def _convert_response(self, response):\n        response_data = json.loads(response[\"response\"])\n\n        status = response[\"status\"]\n        if status != \"\":\n            if \"Bad Request: \" in status:\n                error_data = json.loads(status.split(\"Bad Request: \")[1])\n            else:\n                error_data = {\n                    \"message\": status,\n                    \"code\": \"server_error\"\n                }\n\n            error = {\n                \"message\": error_data[\"error\"][\"message\"],\n                \"code\": error_data[\"error\"][\"code\"]\n            }\n\n            res = None\n        else:\n            res= convert_message(response_data, response[\"custom_id\"])\n            error = None\n\n        output = {\n            \"id\": f'{response[\"custom_id\"]}',\n            \"custom_id\": response[\"custom_id\"],\n            \"response\": res,\n            \"error\": error\n        }\n\n        return output\n\n    def _validate_request(self, request):\n        AnthropicChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.gcp_project","title":"gcp_project  <code>instance-attribute</code>","text":"<pre><code>gcp_project = gcp_project\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.bigquery_input_dataset","title":"bigquery_input_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_input_dataset = bigquery_input_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.bigquery_output_dataset","title":"bigquery_output_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, model: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None\n</code></pre> <p>Initialize the VertexAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in Vertex AI batch format.</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use for the batch prediction.</p> </li> <li> <code>gcp_project</code>               (<code>str</code>)           \u2013            <p>The GCP project to use for the batch prediction.</p> </li> <li> <code>bigquery_input_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction input.</p> </li> <li> <code>bigquery_output_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction output.</p> </li> </ul> <p>Usage: <pre><code>batch = VertexAIBatch(\n    \"path/to/file.jsonl\",\n    \"model\",\n    \"gcp_project\",\n    \"bigquery_input_dataset\",\n    \"bigquery_output_dataset\"\n)\n</code></pre></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def __init__(self, file: str, model: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None:\n    \"\"\"\n    Initialize the VertexAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in Vertex AI batch format.\n        model (str): The name of the model to use for the batch prediction.\n        gcp_project (str): The GCP project to use for the batch prediction.\n        bigquery_input_dataset (str): The BigQuery dataset to use for the batch prediction input.\n        bigquery_output_dataset (str): The BigQuery dataset to use for the batch prediction output.\n\n    Usage:\n    ```python\n    batch = VertexAIBatch(\n        \"path/to/file.jsonl\",\n        \"model\",\n        \"gcp_project\",\n        \"bigquery_input_dataset\",\n        \"bigquery_output_dataset\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n\n    self.model = model\n    self.gcp_project = gcp_project\n    self.bigquery_input_dataset = bigquery_input_dataset\n    self.bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    input_dataset = self._upload_batch_file()\n    output_dataset_id = self._create_table(self.bigquery_output_dataset)\n    output_dataset = f\"bq://{self.gcp_project}.{self.bigquery_output_dataset}.{output_dataset_id}\"\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    return vertexai_state_map[str(job.state.name)]\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    # TODO: implement retry logic for Vertex AI API\n    error = self._get_errors()\n    if error:\n        logging.error(f\"Error in VertexAI Batch: {error}\")\n        if \"Failed to import data. Not found: Dataset\" in error:\n            return False\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    input_dataset = job._gca_resource.input_config.bigquery_source.input_uri\n    output_dataset = job._gca_resource.output_config.bigquery_destination.output_uri\n\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIClaudeChatCompletionBatch/#langbatch.vertexai.VertexAIClaudeChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/","title":"VertexAILlamaChatCompletionBatch","text":""},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch","title":"langbatch.vertexai.VertexAILlamaChatCompletionBatch","text":"<p>               Bases: <code>VertexAIBatch</code>, <code>ChatCompletionBatch</code></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>class VertexAILlamaChatCompletionBatch(VertexAIBatch, ChatCompletionBatch):\n    _url: str = \"/v1/chat/completions\"\n    _publisher: str = \"meta\"\n    _field_name: str = \"body\"\n\n    def _convert_request(self, req: dict) -&gt; str:\n        request = VertexAILlamaChatCompletionRequest(**req[\"body\"])\n\n        request.model = f\"meta/{self.model}\"\n\n        return {\n            \"custom_id\": req[\"custom_id\"],\n            \"body\": request.model_dump_json()\n        }\n\n    def _convert_response(self, response):\n        response_data = json.loads(response[\"response\"])\n        res = {\n            \"request_id\": response[\"custom_id\"],\n            \"status_code\": 200,\n            \"body\": response_data,\n\n        }\n        output = {\n            \"id\": f'{response[\"id\"]}',\n            \"custom_id\": response[\"custom_id\"],\n            \"response\": res,\n            \"error\": response[\"error\"]\n        }\n\n        return output\n\n    def _validate_request(self, request):\n        VertexAILlamaChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.gcp_project","title":"gcp_project  <code>instance-attribute</code>","text":"<pre><code>gcp_project = gcp_project\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.bigquery_input_dataset","title":"bigquery_input_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_input_dataset = bigquery_input_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.bigquery_output_dataset","title":"bigquery_output_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, model: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None\n</code></pre> <p>Initialize the VertexAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in Vertex AI batch format.</p> </li> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use for the batch prediction.</p> </li> <li> <code>gcp_project</code>               (<code>str</code>)           \u2013            <p>The GCP project to use for the batch prediction.</p> </li> <li> <code>bigquery_input_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction input.</p> </li> <li> <code>bigquery_output_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction output.</p> </li> </ul> <p>Usage: <pre><code>batch = VertexAIBatch(\n    \"path/to/file.jsonl\",\n    \"model\",\n    \"gcp_project\",\n    \"bigquery_input_dataset\",\n    \"bigquery_output_dataset\"\n)\n</code></pre></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def __init__(self, file: str, model: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None:\n    \"\"\"\n    Initialize the VertexAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in Vertex AI batch format.\n        model (str): The name of the model to use for the batch prediction.\n        gcp_project (str): The GCP project to use for the batch prediction.\n        bigquery_input_dataset (str): The BigQuery dataset to use for the batch prediction input.\n        bigquery_output_dataset (str): The BigQuery dataset to use for the batch prediction output.\n\n    Usage:\n    ```python\n    batch = VertexAIBatch(\n        \"path/to/file.jsonl\",\n        \"model\",\n        \"gcp_project\",\n        \"bigquery_input_dataset\",\n        \"bigquery_output_dataset\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n\n    self.model = model\n    self.gcp_project = gcp_project\n    self.bigquery_input_dataset = bigquery_input_dataset\n    self.bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise BatchInitializationError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {})\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage(), batch_kwargs: Dict = {}):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, meta_file = storage.load(id)\n\n    # Load metadata based on file extension\n    if meta_file.suffix == '.json':\n        with open(meta_file, 'r') as f:\n            meta_data = json.load(f)\n    else:  # .pkl\n        with open(meta_file, 'rb') as f:\n            meta_data = pickle.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    for key, value in batch_kwargs.items():\n        if key not in init_args:\n            init_args[key] = value\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise BatchStateError(\"Batch already started\")\n\n    input_dataset = self._upload_batch_file()\n    output_dataset_id = self._create_table(self.bigquery_output_dataset)\n    output_dataset = f\"bq://{self.gcp_project}.{self.bigquery_output_dataset}.{output_dataset_id}\"\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    return vertexai_state_map[str(job.state.name)]\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    # TODO: implement retry logic for Vertex AI API\n    error = self._get_errors()\n    if error:\n        logging.error(f\"Error in VertexAI Batch: {error}\")\n        if \"Failed to import data. Not found: Dataset\" in error:\n            return False\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise BatchStateError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    input_dataset = job._gca_resource.input_config.bigquery_source.input_uri\n    output_dataset = job._gca_resource.output_config.bigquery_destination.output_uri\n\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAILlamaChatCompletionBatch/#langbatch.vertexai.VertexAILlamaChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BatchInitializationError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        BatchInitializationError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-2.0-flash-001\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/utils/BatchDispatcher/","title":"BatchDispatcher","text":""},{"location":"references/utils/BatchDispatcher/#langbatch.BatchDispatcher","title":"langbatch.BatchDispatcher","text":""},{"location":"references/utils/BatchDispatcher/#langbatch.BatchDispatcher.BatchDispatcher","title":"BatchDispatcher","text":"<p>Batch dispatcher creates batches from requests in the queue and dispatches them to the batch handler. It periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.</p> <p>Usage: <pre><code># Create a batch dispatcher\nbatch_dispatcher = BatchDispatcher(\n    batch_handler=batch_handler,\n    queue=request_queue,\n    queue_threshold=50000,\n    time_threshold=3600 * 2,\n    requests_type=\"partial\",\n    request_kwargs=request_kwargs\n)\n\nasyncio.create_task(batch_dispatcher.run())\n</code></pre></p> Source code in <code>langbatch\\BatchDispatcher.py</code> <pre><code>class BatchDispatcher:\n    \"\"\"\n    Batch dispatcher creates batches from requests in the queue and dispatches them to the batch handler.\n    It periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.\n\n    Usage:\n    ```python\n    # Create a batch dispatcher\n    batch_dispatcher = BatchDispatcher(\n        batch_handler=batch_handler,\n        queue=request_queue,\n        queue_threshold=50000,\n        time_threshold=3600 * 2,\n        requests_type=\"partial\",\n        request_kwargs=request_kwargs\n    )\n\n    asyncio.create_task(batch_dispatcher.run())\n    ```\n    \"\"\"\n\n    def __init__(\n            self, \n            batch_handler: BatchHandler, \n            queue: RequestQueue, \n            queue_threshold: int = 50000, \n            time_threshold: int = 3600 * 2, \n            time_interval: int = 600, \n            requests_type: Literal[\"partial\", \"full\"] = \"partial\", \n            request_kwargs: Dict = {}\n        ):\n        self.batch_handler = batch_handler\n        self.queue = queue\n        self.queue_threshold = queue_threshold\n        self.time_threshold = time_threshold\n        self.time_interval = time_interval\n        self.last_batch_time = time.time()\n        self.requests_type = requests_type\n        self.request_kwargs = request_kwargs\n\n    async def run(self):\n        \"\"\"\n        Start the batch dispatcher as a asynchronous background task.\n        Periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.\n\n        Examples:\n            ```python\n            asyncio.create_task(batch_dispatcher.run())\n            ```\n        \"\"\"\n        while True:\n            logger.info(\"Running batch dispatcher\")\n            await self._check_batch_conditions()\n            await asyncio.sleep(self.time_interval)\n\n    async def _check_batch_conditions(self):\n        logger.info(\"Checking queue for batch creation\")\n        while True:\n            current_time = time.time()\n            queue_size = len(self.queue)\n            has_threshold_requests = queue_size &gt;= self.queue_threshold\n            reached_time_threshold = (current_time - self.last_batch_time) &gt;= self.time_threshold\n            if has_threshold_requests or (reached_time_threshold and queue_size &gt; 0):\n                logger.info(\"Creating and dispatching batch\")\n                await self._create_and_dispatch_batch()\n            else:\n                logger.info(\"No batch conditions met, waiting for next check\")\n                break\n\n    async def _create_and_dispatch_batch(self):\n        try:\n            logger.info(\"Creating batch\")\n            requests = await asyncio.to_thread(self.queue.get_requests, self.queue_threshold)\n            batch_class = self.batch_handler.batch_type\n            batch_kwargs = self.batch_handler.batch_kwargs\n            if self.requests_type == \"partial\":\n                batch = await asyncio.to_thread(batch_class.create, requests, self.request_kwargs, batch_kwargs)\n            else:\n                batch = await asyncio.to_thread(batch_class.create_from_requests, requests, batch_kwargs)\n            self.last_batch_time = time.time()\n            await self._dispatch_batch(batch)\n        except BatchInitializationError as e:\n            logger.warning(f\"Failed to create batch: {str(e)}\")\n\n    async def _dispatch_batch(self, batch: Batch):\n        logger.info(f\"Dispatching batch {batch.id}\")\n        await asyncio.to_thread(batch.save, self.batch_handler.batch_storage)\n\n        await self.batch_handler.add_batch(batch.id)\n        logger.info(f\"Batch {batch.id} dispatched successfully\")\n</code></pre>"},{"location":"references/utils/BatchDispatcher/#langbatch.BatchDispatcher.BatchDispatcher.run","title":"run  <code>async</code>","text":"<pre><code>run()\n</code></pre> <p>Start the batch dispatcher as a asynchronous background task. Periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.</p> <p>Examples:</p> <pre><code>asyncio.create_task(batch_dispatcher.run())\n</code></pre> Source code in <code>langbatch\\BatchDispatcher.py</code> <pre><code>async def run(self):\n    \"\"\"\n    Start the batch dispatcher as a asynchronous background task.\n    Periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.\n\n    Examples:\n        ```python\n        asyncio.create_task(batch_dispatcher.run())\n        ```\n    \"\"\"\n    while True:\n        logger.info(\"Running batch dispatcher\")\n        await self._check_batch_conditions()\n        await asyncio.sleep(self.time_interval)\n</code></pre>"},{"location":"references/utils/BatchHandler/","title":"BatchHandler","text":""},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler","title":"langbatch.BatchHandler","text":""},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler.BatchHandler","title":"BatchHandler","text":"<p>Batch handler that handles the batches in a queue manner. It handles: <pre><code>* starting batches\n* checking the status of batches\n* processing completed batches\n* retrying failed batches\n* cancelling non retryable failed batches\n</code></pre></p> <p>Examples:</p> <pre><code># Create a batch handler process\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch\n)\nasyncio.create_task(batch_handler.run())\n\n# Add batches to the queue\nawait batch_handler.add_batch(\"123\")\nawait batch_handler.add_batch(\"456\")\n\n# With custom batch queue and batch storage\ncustom_batch_queue = MyCustomBatchQueue()\ncustom_batch_storage = MyCustomBatchStorage()\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=custom_batch_queue,\n    batch_storage=custom_batch_storage\n)\nasyncio.create_task(batch_handler.run())\n</code></pre> Source code in <code>langbatch\\BatchHandler.py</code> <pre><code>class BatchHandler:\n    \"\"\"\n    Batch handler that handles the batches in a queue manner. It handles:\n    ```\n    * starting batches\n    * checking the status of batches\n    * processing completed batches\n    * retrying failed batches\n    * cancelling non retryable failed batches\n    ```\n\n    Examples:\n        ```python\n        # Create a batch handler process\n        batch_handler = BatchHandler(\n            batch_process_func=process_batch,\n            batch_type=OpenAIChatCompletionBatch\n        )\n        asyncio.create_task(batch_handler.run())\n\n        # Add batches to the queue\n        await batch_handler.add_batch(\"123\")\n        await batch_handler.add_batch(\"456\")\n\n        # With custom batch queue and batch storage\n        custom_batch_queue = MyCustomBatchQueue()\n        custom_batch_storage = MyCustomBatchStorage()\n        batch_handler = BatchHandler(\n            batch_process_func=process_batch,\n            batch_type=OpenAIChatCompletionBatch,\n            batch_queue=custom_batch_queue,\n            batch_storage=custom_batch_storage\n        )\n        asyncio.create_task(batch_handler.run())\n        ```\n    \"\"\"\n    def __init__(\n            self, \n            batch_process_func: Callable, \n            batch_type: Type[Batch], \n            batch_queue: BatchQueue = None,\n            batch_storage: BatchStorage = None,\n            wait_time: int = 3600,\n            batch_kwargs: Dict = {}\n        ):\n        self.batch_process_func = batch_process_func\n        self.batch_type = batch_type\n        self.batch_queue = batch_queue or FileBatchQueue(\"batch_queue.json\")\n        self.queues = self.batch_queue.load()\n        self.wait_time = wait_time\n        self.batch_kwargs = batch_kwargs\n        self.batch_storage = batch_storage or FileBatchStorage()\n\n    async def add_batch(self, batch_id: str):\n        \"\"\"\n        Add a batch to the queue.\n\n        Parameters:\n            batch_id: The ID of the batch to add.\n\n        Examples:\n            ```python\n            await batch_handler.add_batch(\"123\")\n            ```\n        \"\"\"\n        self.queues[\"pending\"].append(batch_id)\n        self._save_queues()\n        logger.info(f\"Added batch {batch_id} to pending queue\")\n\n    async def start_batch(self, batch: Batch):\n        if batch.id in self.queues[\"pending\"]:\n            try:\n                await asyncio.to_thread(batch.start)\n                await asyncio.to_thread(batch.save, self.batch_storage)\n                self.queues[\"processing\"].append(batch.id)\n                logger.info(f\"Moved batch {batch.id} from pending to processing queue\")\n            except:\n                logger.error(f\"Error starting batch {batch.id}\", exc_info=True)\n            finally:\n                self.queues[\"pending\"].remove(batch.id)\n\n            self._save_queues() \n        else:\n            logger.warning(f\"Batch {batch.id} not found in pending queue\")\n\n    async def process_completed_batch(self, batch: Batch):\n        try:\n            logger.info(f\"Processing completed batch {batch.id}\")\n            if batch.id in self.queues[\"processing\"]:\n                try:\n                    await asyncio.to_thread(self.batch_process_func, batch)\n                    logger.info(f\"Processed batch {batch.id}\")\n                except:\n                    logger.error(f\"Error processing completed batch {batch.id}\", exc_info=True)\n                self.queues[\"processing\"].remove(batch.id)\n                self._save_queues()\n                logger.info(f\"Removed completed batch {batch.id} from processing queue\")\n            else:\n                logger.warning(f\"Completed batch {batch.id} not found in processing queue\")\n        except:\n            logger.error(f\"Error processing completed batch {batch.id}\", exc_info=True)\n\n    async def retry_batch(self, batch: Batch):\n        if batch.id in self.queues[\"processing\"]:\n            try:\n                logger.info(f\"Retrying batch {batch.id}\")\n                await asyncio.to_thread(batch.retry)\n            except:\n                logger.error(f\"Error retrying batch {batch.id}\", exc_info=True)\n                await self.cancel_batch(batch.id)\n        else:\n            logger.warning(f\"Batch {batch.id} not found in processing queue for retry\")\n\n    async def cancel_batch(self, batch_id: str):\n        for queue in self.queues.values():\n            if batch_id in queue:\n                queue.remove(batch_id)\n                self._save_queues()\n                logger.info(f\"Cancelled and removed batch {batch_id} from queue\")\n                return\n        logger.warning(f\"Batch {batch_id} not found in any queue for cancellation\")\n\n    def _save_queues(self):\n        self.batch_queue.save(self.queues)\n\n    async def run(self):\n        \"\"\"\n        Start the batch handler as a asynchronous background task.\n        Periodically checks the status of batches in the queue and processes them accordingly.\n\n        Usage:\n        ```python\n        asyncio.create_task(batch_handler.run())\n        ```\n        \"\"\"\n        while True:\n            logger.info(\"Handling batches\")\n            retried_batches = 0\n            for batch_id in self.queues[\"processing\"]:\n                if self.batch_storage:\n                    batch = self.batch_type.load(\n                        batch_id, \n                        storage = self.batch_storage,\n                        batch_kwargs = self.batch_kwargs\n                    )\n                else:\n                    batch = self.batch_type.load(batch_id, batch_kwargs = self.batch_kwargs)\n                status = BatchStatus(await asyncio.to_thread(batch.get_status))\n\n                if status == BatchStatus.COMPLETED:\n                    await self.process_completed_batch(batch)\n                elif status in [BatchStatus.FAILED, BatchStatus.EXPIRED]:\n                    if retried_batches &lt; 4:\n                        retried = await self._handle_failed_or_expired_batch(batch, status)\n                        if retried:\n                            retried_batches += 1\n                elif status in [BatchStatus.CANCELLING, BatchStatus.CANCELLED]:\n                    await self.cancel_batch(batch_id)\n                elif status not in [BatchStatus.VALIDATING, BatchStatus.IN_PROGRESS, BatchStatus.FINALIZING]:\n                    logger.error(f\"Unknown status {status.value} for batch {batch_id}\")\n                    await self.cancel_batch(batch_id)\n\n            if retried_batches &lt; 4:\n                started_batches = 0\n                for batch_id in self.queues[\"pending\"]:\n                    if self.batch_storage:\n                        batch = self.batch_type.load(\n                            batch_id, \n                            storage=self.batch_storage, \n                            batch_kwargs=self.batch_kwargs\n                        )\n                    else:\n                        batch = self.batch_type.load(batch_id, batch_kwargs=self.batch_kwargs)\n                    await self.start_batch(batch)\n                    started_batches += 1\n\n                    if (started_batches + retried_batches) == 4:\n                        break\n\n            await asyncio.sleep(self.wait_time)\n\n    async def _handle_failed_or_expired_batch(self, batch: 'Batch', status: BatchStatus):\n        try:\n            if status == BatchStatus.FAILED:\n                retryable = await batch.is_retryable_failure()\n                if retryable:\n                    await asyncio.to_thread(self.retry_batch, batch)\n                    return True\n                else:\n                    logger.warning(f\"Batch {batch.id} failed due to non token-limit error\")\n                    await asyncio.to_thread(self.cancel_batch, batch.id)\n                    return False\n            elif status == BatchStatus.EXPIRED:\n                await asyncio.to_thread(self.retry_batch, batch)\n                return True\n        except Exception as e:\n            logger.error(f\"Error handling {status.value} batch {batch.id}: {e}\")\n            return False\n</code></pre>"},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler.BatchHandler.add_batch","title":"add_batch  <code>async</code>","text":"<pre><code>add_batch(batch_id: str)\n</code></pre> <p>Add a batch to the queue.</p> <p>Parameters:</p> <ul> <li> <code>batch_id</code>               (<code>str</code>)           \u2013            <p>The ID of the batch to add.</p> </li> </ul> <p>Examples:</p> <pre><code>await batch_handler.add_batch(\"123\")\n</code></pre> Source code in <code>langbatch\\BatchHandler.py</code> <pre><code>async def add_batch(self, batch_id: str):\n    \"\"\"\n    Add a batch to the queue.\n\n    Parameters:\n        batch_id: The ID of the batch to add.\n\n    Examples:\n        ```python\n        await batch_handler.add_batch(\"123\")\n        ```\n    \"\"\"\n    self.queues[\"pending\"].append(batch_id)\n    self._save_queues()\n    logger.info(f\"Added batch {batch_id} to pending queue\")\n</code></pre>"},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler.BatchHandler.run","title":"run  <code>async</code>","text":"<pre><code>run()\n</code></pre> <p>Start the batch handler as a asynchronous background task. Periodically checks the status of batches in the queue and processes them accordingly.</p> <p>Usage: <pre><code>asyncio.create_task(batch_handler.run())\n</code></pre></p> Source code in <code>langbatch\\BatchHandler.py</code> <pre><code>async def run(self):\n    \"\"\"\n    Start the batch handler as a asynchronous background task.\n    Periodically checks the status of batches in the queue and processes them accordingly.\n\n    Usage:\n    ```python\n    asyncio.create_task(batch_handler.run())\n    ```\n    \"\"\"\n    while True:\n        logger.info(\"Handling batches\")\n        retried_batches = 0\n        for batch_id in self.queues[\"processing\"]:\n            if self.batch_storage:\n                batch = self.batch_type.load(\n                    batch_id, \n                    storage = self.batch_storage,\n                    batch_kwargs = self.batch_kwargs\n                )\n            else:\n                batch = self.batch_type.load(batch_id, batch_kwargs = self.batch_kwargs)\n            status = BatchStatus(await asyncio.to_thread(batch.get_status))\n\n            if status == BatchStatus.COMPLETED:\n                await self.process_completed_batch(batch)\n            elif status in [BatchStatus.FAILED, BatchStatus.EXPIRED]:\n                if retried_batches &lt; 4:\n                    retried = await self._handle_failed_or_expired_batch(batch, status)\n                    if retried:\n                        retried_batches += 1\n            elif status in [BatchStatus.CANCELLING, BatchStatus.CANCELLED]:\n                await self.cancel_batch(batch_id)\n            elif status not in [BatchStatus.VALIDATING, BatchStatus.IN_PROGRESS, BatchStatus.FINALIZING]:\n                logger.error(f\"Unknown status {status.value} for batch {batch_id}\")\n                await self.cancel_batch(batch_id)\n\n        if retried_batches &lt; 4:\n            started_batches = 0\n            for batch_id in self.queues[\"pending\"]:\n                if self.batch_storage:\n                    batch = self.batch_type.load(\n                        batch_id, \n                        storage=self.batch_storage, \n                        batch_kwargs=self.batch_kwargs\n                    )\n                else:\n                    batch = self.batch_type.load(batch_id, batch_kwargs=self.batch_kwargs)\n                await self.start_batch(batch)\n                started_batches += 1\n\n                if (started_batches + retried_batches) == 4:\n                    break\n\n        await asyncio.sleep(self.wait_time)\n</code></pre>"},{"location":"references/utils/BatchQueue/","title":"BatchQueue Classes","text":""},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.BatchQueue","title":"langbatch.batch_queues.BatchQueue","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for batch queue. Implementations should provide a way to save and load batch queues.</p> <p>Used in BatchHandler to save and load the batch queues.</p> <p>Usage: <pre><code>import asyncio\n\n# Using default FileBatchQueue\nfile_batch_queue = FileBatchQueue(\"batch_queue.json\")\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=file_batch_queue\n)\nasyncio.create_task(batch_handler.run())\n\n# With custom batch queue\nclass MyCustomBatchQueue(BatchQueue):\n    def save(self, queue: Dict[str, List[str]]):\n        # Custom save logic\n\n    def load(self) -&gt; Dict[str, List[str]]:\n        # Custom load logic\n\ncustom_batch_queue = MyCustomBatchQueue()\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=custom_batch_queue\n)\nasyncio.create_task(batch_handler.run())\n</code></pre></p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>class BatchQueue(ABC):\n    \"\"\"\n    Abstract class for batch queue.\n    Implementations should provide a way to save and load batch queues.\n\n    Used in BatchHandler to save and load the batch queues.\n\n    Usage:\n    ```python\n    import asyncio\n\n    # Using default FileBatchQueue\n    file_batch_queue = FileBatchQueue(\"batch_queue.json\")\n    batch_handler = BatchHandler(\n        batch_process_func=process_batch,\n        batch_type=OpenAIChatCompletionBatch,\n        batch_queue=file_batch_queue\n    )\n    asyncio.create_task(batch_handler.run())\n\n    # With custom batch queue\n    class MyCustomBatchQueue(BatchQueue):\n        def save(self, queue: Dict[str, List[str]]):\n            # Custom save logic\n\n        def load(self) -&gt; Dict[str, List[str]]:\n            # Custom load logic\n\n    custom_batch_queue = MyCustomBatchQueue()\n    batch_handler = BatchHandler(\n        batch_process_func=process_batch,\n        batch_type=OpenAIChatCompletionBatch,\n        batch_queue=custom_batch_queue\n    )\n    asyncio.create_task(batch_handler.run())\n    ```\n    \"\"\"\n    @abstractmethod\n    def save(self, queue: Dict[str, List[str]]):\n        \"\"\"\n        Save the batch queue.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        Load the batch queue.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.BatchQueue.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(queue: Dict[str, List[str]])\n</code></pre> <p>Save the batch queue.</p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>@abstractmethod\ndef save(self, queue: Dict[str, List[str]]):\n    \"\"\"\n    Save the batch queue.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.BatchQueue.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; Dict[str, List[str]]\n</code></pre> <p>Load the batch queue.</p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    Load the batch queue.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue","title":"langbatch.batch_queues.FileBatchQueue","text":"<p>               Bases: <code>BatchQueue</code></p> <p>Batch queue that saves the queue to a file.</p> <p>Usage: <pre><code>queue = FileBatchQueue(\"batch_queue.json\")\n\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=queue\n)\n\nasyncio.create_task(batch_handler.run())\n</code></pre></p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>class FileBatchQueue(BatchQueue):\n    \"\"\"\n    Batch queue that saves the queue to a file.\n\n    Usage:\n    ```python\n    queue = FileBatchQueue(\"batch_queue.json\")\n\n    batch_handler = BatchHandler(\n        batch_process_func=process_batch,\n        batch_type=OpenAIChatCompletionBatch,\n        batch_queue=queue\n    )\n\n    asyncio.create_task(batch_handler.run())\n    ```\n    \"\"\"\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n\n    def save(self, queue: Dict[str, List[str]]):\n        try:\n            with open(self.file_path, 'w') as f:\n                json.dump(queue, f)\n        except IOError as e:\n            logger.error(f\"Error saving queue to file: {e}\")\n            raise\n\n    def load(self) -&gt; Dict[str, List[str]]:\n        try:\n            if os.path.exists(self.file_path):\n                with open(self.file_path, 'r') as f:\n                    return json.load(f)\n            return {\"pending\": [], \"processing\": []}\n        except IOError as e:\n            logger.error(f\"Error loading queue from file: {e}\")\n            raise\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path = file_path\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.__init__","title":"__init__","text":"<pre><code>__init__(file_path: str)\n</code></pre> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>def __init__(self, file_path: str):\n    self.file_path = file_path\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.save","title":"save","text":"<pre><code>save(queue: Dict[str, List[str]])\n</code></pre> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>def save(self, queue: Dict[str, List[str]]):\n    try:\n        with open(self.file_path, 'w') as f:\n            json.dump(queue, f)\n    except IOError as e:\n        logger.error(f\"Error saving queue to file: {e}\")\n        raise\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.load","title":"load","text":"<pre><code>load() -&gt; Dict[str, List[str]]\n</code></pre> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>def load(self) -&gt; Dict[str, List[str]]:\n    try:\n        if os.path.exists(self.file_path):\n            with open(self.file_path, 'r') as f:\n                return json.load(f)\n        return {\"pending\": [], \"processing\": []}\n    except IOError as e:\n        logger.error(f\"Error loading queue from file: {e}\")\n        raise\n</code></pre>"},{"location":"references/utils/BatchStorage/","title":"BatchStorage Classes","text":""},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.BatchStorage","title":"langbatch.batch_storages.BatchStorage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for batch storage. Implementations should provide a way to save and load batches.</p> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n\n# Using default FileBatchStorage\nbatch.save(storage=FileBatchStorage()) # same as batch.save()\nbatch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage()) # same as OpenAIChatCompletionBatch.load(\"1ff73c3f\")\n\n# With custom storage\nclass MyCustomBatchStorage(BatchStorage):\n    def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n        # Custom save logic\n\n    def load(self, id: str) -&gt; Tuple[Path, Path]:\n        # Custom load logic\n\ncustom_storage = MyCustomBatchStorage()\n\n# Using custom storage\nbatch.save(storage=custom_storage)\nbatch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=custom_storage)\n</code></pre></p> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>class BatchStorage(ABC):\n    \"\"\"\n    Abstract class for batch storage.\n    Implementations should provide a way to save and load batches.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n\n    # Using default FileBatchStorage\n    batch.save(storage=FileBatchStorage()) # same as batch.save()\n    batch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage()) # same as OpenAIChatCompletionBatch.load(\"1ff73c3f\")\n\n    # With custom storage\n    class MyCustomBatchStorage(BatchStorage):\n        def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n            # Custom save logic\n\n        def load(self, id: str) -&gt; Tuple[Path, Path]:\n            # Custom load logic\n\n    custom_storage = MyCustomBatchStorage()\n\n    # Using custom storage\n    batch.save(storage=custom_storage)\n    batch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=custom_storage)\n    ```\n    \"\"\"\n\n    @abstractmethod\n    def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n        \"\"\"\n        Save the batch data and metadata to the storage.\n\n        Args:\n            id (str): The id of the batch.\n            data_file (Path): The path to the batch data file.\n            meta_data (Dict[str, Any]): The metadata of the batch.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load(self, id: str) -&gt; Tuple[Path, Path]:\n        \"\"\"\n        Load the batch data and metadata from the storage.\n\n        Args:\n            id (str): The id of the batch.\n\n        Returns:\n            Tuple[Path, Path]: The path to the batch data jsonlfile and the path to the metadata jsonfile.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.BatchStorage.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(id: str, data_file: Path, meta_data: Dict[str, Any])\n</code></pre> <p>Save the batch data and metadata to the storage.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>data_file</code>               (<code>Path</code>)           \u2013            <p>The path to the batch data file.</p> </li> <li> <code>meta_data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>The metadata of the batch.</p> </li> </ul> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>@abstractmethod\ndef save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n    \"\"\"\n    Save the batch data and metadata to the storage.\n\n    Args:\n        id (str): The id of the batch.\n        data_file (Path): The path to the batch data file.\n        meta_data (Dict[str, Any]): The metadata of the batch.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.BatchStorage.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(id: str) -&gt; Tuple[Path, Path]\n</code></pre> <p>Load the batch data and metadata from the storage.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[Path, Path]</code>           \u2013            <p>Tuple[Path, Path]: The path to the batch data jsonlfile and the path to the metadata jsonfile.</p> </li> </ul> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>@abstractmethod\ndef load(self, id: str) -&gt; Tuple[Path, Path]:\n    \"\"\"\n    Load the batch data and metadata from the storage.\n\n    Args:\n        id (str): The id of the batch.\n\n    Returns:\n        Tuple[Path, Path]: The path to the batch data jsonlfile and the path to the metadata jsonfile.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage","title":"langbatch.batch_storages.FileBatchStorage","text":"<p>               Bases: <code>BatchStorage</code></p> <p>Batch storage that saves the batch data and metadata to the file system. Automatically chooses between JSON and pickle serialization based on content: - Uses JSON for simple metadata (human-readable, portable) - Uses pickle for complex objects (like API clients)</p> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n\n# Save the batch\nbatch.save(storage=FileBatchStorage())\n\n# Load the batch\nbatch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage())\n</code></pre></p> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>class FileBatchStorage(BatchStorage):\n    \"\"\"\n    Batch storage that saves the batch data and metadata to the file system.\n    Automatically chooses between JSON and pickle serialization based on content:\n    - Uses JSON for simple metadata (human-readable, portable)\n    - Uses pickle for complex objects (like API clients)\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n\n    # Save the batch\n    batch.save(storage=FileBatchStorage())\n\n    # Load the batch\n    batch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage())\n    ```\n    \"\"\"\n\n    def __init__(self, directory: str = DATA_PATH):\n        \"\"\"\n        Initialize the FileBatchStorage. Will create or use a directory named 'saved_batches' in the given directory to save the batches.\n\n        Args:\n            directory (str): The directory to save the batches. Defaults to the DATA_PATH.\n        \"\"\"\n        self.saved_batches_directory = Path(directory) / \"saved_batches\"\n        self.saved_batches_directory.mkdir(exist_ok=True, parents=True)\n\n    def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n        # Check if metadata can be JSON serialized\n        use_json = _is_json_serializable(meta_data)\n        json_meta_file = self.saved_batches_directory / f\"{id}.json\"\n        pkl_meta_file = self.saved_batches_directory / f\"{id}.pkl\"\n        if use_json:\n            with open(json_meta_file, 'w') as f:\n                json.dump(meta_data, f)\n\n            if pkl_meta_file.exists():\n                pkl_meta_file.unlink(missing_ok=True)\n        else:\n            with open(pkl_meta_file, 'wb') as f:\n                pickle.dump(meta_data, f)\n\n            if json_meta_file.exists():\n                json_meta_file.unlink(missing_ok=True)\n\n        destination = self.saved_batches_directory / f\"{id}.jsonl\"\n        if not destination.exists(): \n            # if the file does not exist, copy the file from the data_file\n            shutil.copy(data_file, destination)\n\n    def load(self, id: str) -&gt; Tuple[Path, Path]:\n        data_file = self.saved_batches_directory / f\"{id}.jsonl\"\n\n        # Try JSON first, then pickle\n        json_file = self.saved_batches_directory / f\"{id}.json\"\n        pkl_file = self.saved_batches_directory / f\"{id}.pkl\"\n\n        if json_file.is_file():\n            meta_file = json_file\n        elif pkl_file.is_file():\n            meta_file = pkl_file\n        else:\n            raise BatchStorageError(f\"Batch with id {id} not found\")\n\n        if not data_file.is_file():\n            raise BatchStorageError(f\"Batch with id {id} not found\")\n\n        return data_file, meta_file\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.saved_batches_directory","title":"saved_batches_directory  <code>instance-attribute</code>","text":"<pre><code>saved_batches_directory = Path(directory) / 'saved_batches'\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.__init__","title":"__init__","text":"<pre><code>__init__(directory: str = DATA_PATH)\n</code></pre> <p>Initialize the FileBatchStorage. Will create or use a directory named 'saved_batches' in the given directory to save the batches.</p> <p>Parameters:</p> <ul> <li> <code>directory</code>               (<code>str</code>, default:                   <code>DATA_PATH</code> )           \u2013            <p>The directory to save the batches. Defaults to the DATA_PATH.</p> </li> </ul> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>def __init__(self, directory: str = DATA_PATH):\n    \"\"\"\n    Initialize the FileBatchStorage. Will create or use a directory named 'saved_batches' in the given directory to save the batches.\n\n    Args:\n        directory (str): The directory to save the batches. Defaults to the DATA_PATH.\n    \"\"\"\n    self.saved_batches_directory = Path(directory) / \"saved_batches\"\n    self.saved_batches_directory.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.save","title":"save","text":"<pre><code>save(id: str, data_file: Path, meta_data: Dict[str, Any])\n</code></pre> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n    # Check if metadata can be JSON serialized\n    use_json = _is_json_serializable(meta_data)\n    json_meta_file = self.saved_batches_directory / f\"{id}.json\"\n    pkl_meta_file = self.saved_batches_directory / f\"{id}.pkl\"\n    if use_json:\n        with open(json_meta_file, 'w') as f:\n            json.dump(meta_data, f)\n\n        if pkl_meta_file.exists():\n            pkl_meta_file.unlink(missing_ok=True)\n    else:\n        with open(pkl_meta_file, 'wb') as f:\n            pickle.dump(meta_data, f)\n\n        if json_meta_file.exists():\n            json_meta_file.unlink(missing_ok=True)\n\n    destination = self.saved_batches_directory / f\"{id}.jsonl\"\n    if not destination.exists(): \n        # if the file does not exist, copy the file from the data_file\n        shutil.copy(data_file, destination)\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.load","title":"load","text":"<pre><code>load(id: str) -&gt; Tuple[Path, Path]\n</code></pre> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>def load(self, id: str) -&gt; Tuple[Path, Path]:\n    data_file = self.saved_batches_directory / f\"{id}.jsonl\"\n\n    # Try JSON first, then pickle\n    json_file = self.saved_batches_directory / f\"{id}.json\"\n    pkl_file = self.saved_batches_directory / f\"{id}.pkl\"\n\n    if json_file.is_file():\n        meta_file = json_file\n    elif pkl_file.is_file():\n        meta_file = pkl_file\n    else:\n        raise BatchStorageError(f\"Batch with id {id} not found\")\n\n    if not data_file.is_file():\n        raise BatchStorageError(f\"Batch with id {id} not found\")\n\n    return data_file, meta_file\n</code></pre>"},{"location":"references/utils/RequestQueue/","title":"RequestQueue Classes","text":"<p>Info</p> <p>Please make sure to pass the the correct type of requests to the queue as per the type of Batch you are going to use. For Example, if you are using ChatCompletionBatch as your batch type, then you should pass the requests in the same format as the  ChatCompletionBatch.create() expects. For EmbeddingBatch, refer EmbeddingBatch.create()</p>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RequestQueue","title":"langbatch.request_queues.RequestQueue","text":"<p>               Bases: <code>ABC</code></p> <p>RequestQueue is an abstract class for request queues. Implementations should provide a way to add and retrieve requests.</p> <p>Used in <code>BatchDispatcher</code> to get requests.</p> <p>Usage: <pre><code>request_queue = InMemoryRequestQueue()\nrequest_queue.add_requests([\n    [\n        {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n    ],\n    [\n        {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n        {\"role\": \"assistant\", \"content\": \"George Washington\"},\n        {\"role\": \"user\", \"content\": \"Second?\"}\n    ]\n])\n\nbatch_dispatcher = BatchDispatcher(\n    batch_handler=batch_handler,\n    queue=request_queue\n)\n\nasyncio.create_task(batch_dispatcher.run())\n</code></pre></p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>class RequestQueue(ABC):\n    \"\"\"\n    RequestQueue is an abstract class for request queues.\n    Implementations should provide a way to add and retrieve requests.\n\n    Used in `BatchDispatcher` to get requests.\n\n    Usage:\n    ```python\n    request_queue = InMemoryRequestQueue()\n    request_queue.add_requests([\n        [\n            {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n        ],\n        [\n            {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n            {\"role\": \"assistant\", \"content\": \"George Washington\"},\n            {\"role\": \"user\", \"content\": \"Second?\"}\n        ]\n    ])\n\n    batch_dispatcher = BatchDispatcher(\n        batch_handler=batch_handler,\n        queue=request_queue\n    )\n\n    asyncio.create_task(batch_dispatcher.run())\n    ```\n    \"\"\"\n    @abstractmethod\n    def add_requests(self, requests: List[Any]):\n        \"\"\"\n        Add requests to the queue\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_requests(self, count: int) -&gt; List[Any]:\n        \"\"\"\n        Get requests from the queue\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __len__(self):\n        pass\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RequestQueue.add_requests","title":"add_requests  <code>abstractmethod</code>","text":"<pre><code>add_requests(requests: List[Any])\n</code></pre> <p>Add requests to the queue</p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>@abstractmethod\ndef add_requests(self, requests: List[Any]):\n    \"\"\"\n    Add requests to the queue\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RequestQueue.get_requests","title":"get_requests  <code>abstractmethod</code>","text":"<pre><code>get_requests(count: int) -&gt; List[Any]\n</code></pre> <p>Get requests from the queue</p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>@abstractmethod\ndef get_requests(self, count: int) -&gt; List[Any]:\n    \"\"\"\n    Get requests from the queue\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.InMemoryRequestQueue","title":"langbatch.request_queues.InMemoryRequestQueue","text":"<p>               Bases: <code>RequestQueue</code></p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>class InMemoryRequestQueue(RequestQueue):\n    def __init__(self):\n        self.queue = deque()\n\n    def add_requests(self, requests: List[Any]):\n        self.queue.extend(requests)\n        logging.info(f\"Added {len(requests)} requests to queue\")\n\n    def get_requests(self, count: int) -&gt; List[Any]:\n        if count &gt; len(self.queue):\n            count = len(self.queue)\n        return [self.queue.popleft() for _ in range(count)]\n\n    def __len__(self):\n        return len(self.queue)\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.InMemoryRequestQueue.add_requests","title":"add_requests","text":"<pre><code>add_requests(requests: List[Any])\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def add_requests(self, requests: List[Any]):\n    self.queue.extend(requests)\n    logging.info(f\"Added {len(requests)} requests to queue\")\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.InMemoryRequestQueue.get_requests","title":"get_requests","text":"<pre><code>get_requests(count: int) -&gt; List[Any]\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def get_requests(self, count: int) -&gt; List[Any]:\n    if count &gt; len(self.queue):\n        count = len(self.queue)\n    return [self.queue.popleft() for _ in range(count)]\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue","title":"langbatch.request_queues.RedisRequestQueue","text":"<p>               Bases: <code>RequestQueue</code></p> <p>RedisRequestQueue is a request queue that uses a Redis list to store requests.</p> <p>Usage: <pre><code>import os\nimport redis\n\nREDIS_URL = os.environ.get('REDIS_URL')\nredis_client = redis.from_url(REDIS_URL)\n\nrequest_queue = RedisRequestQueue(redis_client, queue_name='turbo_requests')\nrequest_queue.add_requests([\n    [\n        {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n    ],\n    [\n        {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n        {\"role\": \"assistant\", \"content\": \"George Washington\"},\n        {\"role\": \"user\", \"content\": \"Second?\"}\n    ]\n])\n</code></pre></p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>class RedisRequestQueue(RequestQueue):\n    \"\"\"\n    RedisRequestQueue is a request queue that uses a Redis list to store requests.\n\n    Usage:\n    ```python\n    import os\n    import redis\n\n    REDIS_URL = os.environ.get('REDIS_URL')\n    redis_client = redis.from_url(REDIS_URL)\n\n    request_queue = RedisRequestQueue(redis_client, queue_name='turbo_requests')\n    request_queue.add_requests([\n        [\n            {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n        ],\n        [\n            {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n            {\"role\": \"assistant\", \"content\": \"George Washington\"},\n            {\"role\": \"user\", \"content\": \"Second?\"}\n        ]\n    ])\n    ```\n    \"\"\"\n    def __init__(self, redis_client: Any, queue_name: str = 'request_queue'):\n        try:\n            import redis\n            if not isinstance(redis_client, redis.Redis):\n                raise TypeError(\"redis_client must be an instance of redis.Redis\")\n        except ImportError:\n            raise ImportError(\"redis package is required for RedisRequestQueue. Run: pip install langbatch[redis]\")\n\n        self.redis_client = redis_client\n        self.queue_name = queue_name\n\n    def add_requests(self, requests: List[Any]):\n        count = len(requests)\n        for request in requests:\n            self.redis_client.rpush(self.queue_name, str(json.dumps(request)))\n        logging.debug(f\"Added {count} requests to queue.\")\n\n    def get_requests(self, count: int) -&gt; List[Any]:\n        size = len(self)\n        if count &gt; size:\n            count = size\n\n        if count == 0:\n            return []\n\n        items = self.redis_client.lpop(self.queue_name, count=count)\n        if items is None:\n            return []\n\n        results = [json.loads(item.decode('utf-8')) for item in items]\n\n        logging.debug(f\"Retrieved {len(results)} requests from queue.\")\n        return results\n\n    def __len__(self):\n        length = self.redis_client.llen(self.queue_name)\n        return length\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue.__init__","title":"__init__","text":"<pre><code>__init__(redis_client: Any, queue_name: str = 'request_queue')\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def __init__(self, redis_client: Any, queue_name: str = 'request_queue'):\n    try:\n        import redis\n        if not isinstance(redis_client, redis.Redis):\n            raise TypeError(\"redis_client must be an instance of redis.Redis\")\n    except ImportError:\n        raise ImportError(\"redis package is required for RedisRequestQueue. Run: pip install langbatch[redis]\")\n\n    self.redis_client = redis_client\n    self.queue_name = queue_name\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue.add_requests","title":"add_requests","text":"<pre><code>add_requests(requests: List[Any])\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def add_requests(self, requests: List[Any]):\n    count = len(requests)\n    for request in requests:\n        self.redis_client.rpush(self.queue_name, str(json.dumps(request)))\n    logging.debug(f\"Added {count} requests to queue.\")\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue.get_requests","title":"get_requests","text":"<pre><code>get_requests(count: int) -&gt; List[Any]\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def get_requests(self, count: int) -&gt; List[Any]:\n    size = len(self)\n    if count &gt; size:\n        count = size\n\n    if count == 0:\n        return []\n\n    items = self.redis_client.lpop(self.queue_name, count=count)\n    if items is None:\n        return []\n\n    results = [json.loads(item.decode('utf-8')) for item in items]\n\n    logging.debug(f\"Retrieved {len(results)} requests from queue.\")\n    return results\n</code></pre>"}]}