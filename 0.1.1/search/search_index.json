{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>LangBatch is a Python library for large scale AI generation using batch APIs from providers like OpenAI, Anthropic, Azure OpenAI, GCP Vertex AI, etc.  </p> <ul> <li> <p>Utlize Batch APIs for:</p> <ul> <li>Requests that don't require immediate responses.</li> <li>Low cost (usually 50% discount)</li> <li>Higher rate limits</li> <li>Example use cases: Data processing pipelines, Evaluations, Classifying huge data, Creating embeddings for large text contents</li> </ul> </li> <li> <p>Features:</p> <ul> <li>Unified API to access Batch API from different AI providers.</li> <li>Standarized OpenAI format for requests and responses</li> <li>Utilities for handling the complete lifecycle of a batch job: Creating, Starting, Monitoring, Retrying and Processing Completed</li> <li>Convert incoming requests into batch jobs</li> </ul> </li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>\ud83d\ude80 Get Started</p> <p>Learn the basics and become familiar with the LangBatch and how to utilize it.</p> <p> Get Started</p> </li> <li> <p>\ud83d\udcda Core Concepts</p> <p>The high-level explanations for building a better understand about the important topics such as how to utlize Batch classes for batch generations.</p> <p> Core Concepts</p> </li> <li> <p>\ud83d\udee0\ufe0f How-to Guides</p> <p>Practical guides to help you achieve a specific goals. Take a look at these guides to learn how to use LangBatch to solve real-world problems.</p> <p> How-to Guides</p> </li> <li> <p>\ud83d\udcd6 References</p> <p>Technical descriptions of how LangBatch classes and methods work.</p> <p> References</p> </li> </ul>"},{"location":"community/","title":"\u2764\ufe0f Community","text":"<p>\"Individually, we are one drop. Together, we are an ocean.\" - Ryunosuke Satoro</p>"},{"location":"community/#contributing","title":"Contributing","text":"<p>We welcome contributions to LangBatch!</p> <p>If you're looking to contribute to LangBatch, please raise an issue or submit a PR.</p>"},{"location":"community/#issues","title":"Issues","text":"<p>If you have any issues or suggestions, please open an issue on our Issues.</p>"},{"location":"community/#roadmap","title":"Roadmap","text":"<p>Please check out our Roadmap to see what we're working on.</p>"},{"location":"community/#shoutouts","title":"Shoutouts","text":"<p>We'd like to thank the following projects that inspired us to build LangBatch.</p> <ul> <li>LangChain</li> <li>Ragas - This documentation is heavily inspired by the Ragas documentation.</li> </ul>"},{"location":"concepts/","title":"\ud83d\udcda Core Concepts","text":"<p>LangBatch gives you building blocks to create pipelines for AI inference. The core concepts are:</p> <ul> <li> <p>Batch Types</p> <p>What is a batch?</p> </li> <li> <p>Pipeline</p> <p>How to create a batch AI inference pipeline?</p> </li> <li> <p>Providers</p> <p>Provider based implementations</p> </li> </ul>"},{"location":"concepts/pipeline/","title":"Pipeline","text":"<p>LangBatch provides building blocks to create pipelines for AI inference. You can create a pipeline by combining these components. BatchHandler and BatchDispatcher are the core components of the pipeline.</p> <ul> <li> <p>Batch Handler</p> <p>Handler for the batches</p> </li> <li> <p>Batch Dispatcher</p> <p>Batching incoming requests</p> </li> </ul>"},{"location":"concepts/pipeline/batch_dispatcher/","title":"Batch Dispatcher","text":""},{"location":"concepts/pipeline/batch_dispatcher/#why-do-we-need-batch-dispatcher","title":"Why do we need Batch Dispatcher?","text":"<p>Batch Dispatcher is responsible for creating batches from incoming stream of requests and dispatching them to the batch handler in a balanced manner. This is useful in situations where you need to setup a API service and listen to incoming requests.</p> <p>Also, we need to keep track of all the incoming requests and maintain a queue.</p>"},{"location":"concepts/pipeline/batch_dispatcher/#initialize-a-batch-dispatcher","title":"Initialize a Batch Dispatcher","text":"<p>You can initialize a batch dispatcher by passing the batch handler, request queue, queue threshold, time threshold, time interval, and request kwargs.</p> <pre><code>from langbatch import BatchHandler\nfrom langbatch.request_queues import InMemoryRequestQueue\nfrom langbatch import BatchDispatcher\n\n# Create a batch handler\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch\n)\n\n# Create a request queue\nrequest_queue = InMemoryRequestQueue()\n\n# Create a batch dispatcher\nbatch_dispatcher = BatchDispatcher(\n    batch_handler=batch_handler,\n    queue=request_queue,\n    queue_threshold=50000, # 50000 requests\n    time_threshold=3600 * 2, # 2 hours\n    time_interval=600, # 10 minutes,\n    requests_type=\"partial\",\n    request_kwargs=request_kwargs\n)\n</code></pre> <p>Here, we are initializing a batch dispatcher with a InMemoryRequestQueue. </p> <p><code>queue_threshold</code> is the maximum number of requests that can be added to the queue. If the queue threshold is reached, then the requests will be converted into a batch and sent to the batch handler.</p> <p><code>time_threshold</code> is the maximum time interval for which a request can be waited in queue. Even if the queue threshold is not reached. Once the time threshold is reached, the requests in the queue will be converted into a batch and sent to the batch handler.</p> <p><code>time_interval</code> is the time interval for which the queue will be checked for the queue threshold and time threshold.</p> <p><code>requests_type</code> is the type of requests that will be added to the queue. It can be \"partial\" or \"full\". If it is 'partial', Batch.create method is used to create the batch, and if it is 'full', Batch.create_from_requests method is used.</p> <p><code>request_kwargs</code> is the kwargs that will be passed to the Batch.create method in Batch class to create a batch. Ex. temperature, max_tokens, etc. Used when <code>requests_type</code> is 'partial'.</p>"},{"location":"concepts/pipeline/batch_dispatcher/#run-the-batch-dispatcher","title":"Run the Batch Dispatcher","text":"<pre><code>asyncio.create_task(batch_dispatcher.run())\n</code></pre> <p>This will start a background task that will run indefinitely until the program is terminated.</p>"},{"location":"concepts/pipeline/batch_dispatcher/#add-requests-to-the-queue","title":"Add Requests to the Queue","text":"<pre><code># Add multiple requests to the queue\nawait request_queue.add_requests(requests)\n</code></pre>"},{"location":"concepts/pipeline/batch_dispatcher/#redis-request-queue","title":"Redis Request Queue","text":"<p>You can also use RedisRequestQueue to add requests to the queue. With RedisRequestQueue,  * you can add requests to the queue in a persisted, distributed manner * it can be shared across multiple processes and machines to add requests to the queue.</p> <pre><code>from langbatch.request_queues import RedisRequestQueue\nimport redis\n\nREDIS_URL = os.environ.get('REDIS_URL')\nredis_client = redis.from_url(REDIS_URL)\n\nrequest_queue = RedisRequestQueue(\n    redis_client=redis_client,\n    queue_name='gemini_requests'\n)\n</code></pre>"},{"location":"concepts/pipeline/batch_dispatcher/#custom-request-queue","title":"Custom Request Queue","text":"<p>You can also implement your own request queue by implementing the <code>RequestQueue</code>.</p> <pre><code>from langbatch.request_queues import RequestQueue\n\nclass CustomRequestQueue(RequestQueue):\n    pass\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/","title":"Batch Handler","text":""},{"location":"concepts/pipeline/batch_handler/#why-do-we-need-batch-handler","title":"Why do we need Batch Handler?","text":"<p>When we start a batch job in OpenAI or in other providers, it can fail due to various reasons like rate limits, quota limits, etc. We need a mechanism to check the status of the batch periodically and retry the failed batches. And we need to process the completed batches. Also we may need to keep track of the batch jobs and make sure of successful completion of all the batch jobs. </p> <p>And due to the Quota and Rate Limits, we need to handle the batches in a queue manner to avoid Rate Limit errors.</p> <p>BatchHandler is designed to handle all these things in a production environment.</p>"},{"location":"concepts/pipeline/batch_handler/#initialize-a-batch-handler","title":"Initialize a Batch Handler","text":"<p>You can initialize a batch handler by passing the batch process function and the batch type.</p> <pre><code>from langbatch import BatchHandler\n\n# Create a batch handler process\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch\n)\nasyncio.create_task(batch_handler.run())\n</code></pre> <p>batch_process_func is the function that will be called to process the batch. It should accept batch as the argument. You can put the logic to process the batch in this function.</p> <pre><code>def process_batch(batch: OpenAIChatCompletionBatch):\n    successful_results, _ = batch.get_results()\n    for result in successful_results:\n        # Process the result\n        pass\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#add-batches-to-the-queue","title":"Add Batches to the Queue","text":"<p>You can add batches to the queue by calling the <code>add_batch</code> method.</p> <pre><code>await batch_handler.add_batch(\"55d506ef-2a1f-4ca1-9c6c-3fd2415c83f7\")\nawait batch_handler.add_batch(\"10d71a17-6e29-4b1a-ba3d-245bd7cdf4f0\")\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#wait-time","title":"Wait Time","text":"<p>Wait time is the time in seconds to wait for processing the next set of batches in time intervals. It is used to avoid Rate Limit errors.</p> <pre><code>batch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    wait_time=1800 # 30 minutes\n)\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#with-custom-storage","title":"With Custom Storage","text":"<p>By default, BatchHandler uses <code>FileBatchQueue</code> to handle the batch queue. And <code>FileBatchStorage</code> to store the batches. You can implement and use custom implemetations of <code>BatchQueue</code> and <code>BatchStorage</code> for the batch handler by passing them to the <code>BatchHandler</code> constructor.</p> <pre><code>custom_batch_queue = MyCustomBatchQueue()\ncustom_batch_storage = MyCustomBatchStorage()\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=custom_batch_queue,\n    batch_storage=custom_batch_storage\n)\n</code></pre>"},{"location":"concepts/pipeline/batch_handler/#batch-kwargs","title":"Batch Kwargs","text":"<p>You can pass additional kwargs to the batch process function by passing the <code>batch_kwargs</code> parameter to the <code>BatchHandler</code> constructor. These kwargs are used to initialize the batch object.</p> <pre><code>batch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=VertexAIChatCompletionBatch,\n    batch_kwargs={\n        \"model_name\": \"gemini-1.5-flash-002\",\n        \"project\": \"my-project\",\n        \"location\": \"us-central1\",\n        \"bigquery_input_dataset\": \"input-dataset\",\n        \"bigquery_output_dataset\": \"output-dataset\"\n    }\n)\n</code></pre>"},{"location":"concepts/providers/","title":"\ud83d\udcda Providers","text":"<p>LangBatch currently supports the following providers:</p> <ul> <li> <p>OpenAI</p> </li> <li> <p>Anthropic</p> </li> <li> <p>Azure OpenAI</p> </li> <li> <p>VertexAI</p> </li> <li> <p>Providers with OpenAI Batch API Support</p> </li> </ul> <p>Note</p> <p>LangBatch is designed to support multiple providers. We will add support for more providers in the future. Please raise an Issue in GitHub, if you need a support for a new provider. </p>"},{"location":"concepts/providers/Anthropic/","title":"Anthropic","text":"<p>You can utilize Anthropic Batch API for running batch generations on Anthropic models like Claude 3.5 Sonnet via LangBatch.</p>"},{"location":"concepts/providers/Anthropic/#data-format","title":"Data Format","text":"<p>OpenAI batch data format can be used for Anthropic.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/Anthropic/#anthropic-client","title":"Anthropic Client","text":"<p>Anthropic client is used to make requests to the Anthropic service.</p> <pre><code>from anthropic import Anthropic\nfrom langbatch.anthropic import AnthropicChatCompletionBatch\n\nclient = Anthropic(\n    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n)\n\nbatch = AnthropicChatCompletionBatch(\n    file=\"data.jsonl\",\n    client=client\n)\n\nbatch.start()\n</code></pre> <p>Refer to Anthropic Batch API Documentation for more information.</p>"},{"location":"concepts/providers/AzureOpenAI/","title":"Azure OpenAI","text":"<p>You can utilize Azure OpenAI Batch API for running batch generations on Azure OpenAI models via LangBatch.</p>"},{"location":"concepts/providers/AzureOpenAI/#data-format","title":"Data Format","text":"<p>Our default OpenAI data format can be used for Azure OpenAI. But you need to replace the model name with the model deployment name.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"REPLACE-WITH-MODEL-DEPLOYMENT-NAME\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"REPLACE-WITH-MODEL-DEPLOYMENT-NAME\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/AzureOpenAI/#azure-openai-client","title":"Azure OpenAI Client","text":"<p>Azure OpenAI client is used to make requests to the Azure OpenAI service.</p> <pre><code>from openai import AzureOpenAI\nfrom langbatch.openai import OpenAIChatCompletionBatch\n\nclient = AzureOpenAI(\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n    api_version=\"2024-07-01-preview\",\n    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n    )\n\nbatch = OpenAIChatCompletionBatch(file=\"data.jsonl\", client=client)\n\nbatch.start()\n</code></pre> <p>Note</p> <p>Azure OpenAI does not support Text Embedding models and Finetuned LLMs yet. Refer to Azure OpenAI Batch APIDocumentation for more information.</p>"},{"location":"concepts/providers/OpenAI/","title":"OpenAI","text":"<p>You can utilize OpenAI Batch API for running batch generations on OpenAI models via LangBatch.</p>"},{"location":"concepts/providers/OpenAI/#data-format","title":"Data Format","text":"<p>OpenAI batch data format can be used for OpenAI.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"model\": \"gpt-4o\", \"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/OpenAI/#openai-client","title":"OpenAI Client","text":"<p>OpenAI client is used to make requests to the OpenAI service.</p> <pre><code>from openai import OpenAI\nfrom langbatch.openai import OpenAIChatCompletionBatch\n\nclient = OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n)\n\nbatch = OpenAIChatCompletionBatch(\n    file=\"data.jsonl\",\n    client=client\n)\n\nbatch.start()\n</code></pre> <p>Refer to OpenAI Batch API Documentation for more information.</p>"},{"location":"concepts/providers/OpenAI/#other-providers-with-openai-api-support","title":"Other Providers with OpenAI API support","text":"<p>If you are using other providers with OpenAI Batch API compatible API, you can just change the base_url to the provider's base_url. And change the model name too.</p> <pre><code>from openai import OpenAI\nfrom langbatch.openai import OpenAIChatCompletionBatch\n\nclient = OpenAI(\n    api_key=os.getenv(\"API_KEY\"),\n    base_url=\"https://api.provider.com/v1/\"\n)\n\nbatch = OpenAIChatCompletionBatch(\n    file=\"data.jsonl\",\n    client=client\n)\n</code></pre>"},{"location":"concepts/providers/VertexAI/","title":"Vertex AI","text":"<p>You can utilize Vertex AI Batch generations for running batch generations on Vertex AI models via LangBatch.</p>"},{"location":"concepts/providers/VertexAI/#data-format","title":"Data Format","text":"<p>OpenAI data format can be used in LangBatch for Vertex AI. But the model name can be skipped here. And all the requests should have same model name.</p> <pre><code>{\"custom_id\": \"task-0\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was Microsoft founded?\"}]}}\n{\"custom_id\": \"task-1\", \"method\": \"POST\", \"url\": \"/chat/completions\", \"body\": {\"messages\": [{\"role\": \"system\", \"content\": \"You are an AI assistant that helps people find information.\"}, {\"role\": \"user\", \"content\": \"When was the first XBOX released?\"}]}}\n</code></pre>"},{"location":"concepts/providers/VertexAI/#vertex-ai-initialization","title":"Vertex AI Initialization","text":"<p>Vertex AI should be initialized with the project id and location.</p> <pre><code>import os\nimport vertexai\n\nGCP_PROJECT = os.environ.get('GCP_PROJECT')\nGCP_LOCATION = os.environ.get('GCP_LOCATION')\n\nvertexai.init(project=GCP_PROJECT, location=GCP_LOCATION)\n</code></pre>"},{"location":"concepts/providers/VertexAI/#vertex-ai-batch","title":"Vertex AI Batch","text":"<p>Vertex AI Batch can be created with the model name, project id, location, bigquery input dataset and bigquery output dataset values.</p> <pre><code>from langbatch.vertexai import VertexAIChatCompletionBatch\n\nbatch = VertexAIChatCompletionBatch(\n    file=\"data.jsonl\",\n    model_name=\"gemini-1.5-flash-002\",\n    project=GCP_PROJECT,\n    location=GCP_LOCATION,\n    bigquery_input_dataset=\"batches\",\n    bigquery_output_dataset=\"gen_ai_batch_prediction\")\n\nbatch.start()\n</code></pre> <p>Note</p> <p>You need to make sure that the BigQuery datasets are created before running the batch.</p>"},{"location":"concepts/types/","title":"Batch and Batch Types","text":"<p>Batch is the core building block of LangBatch. A batch instance is created with a JSONL file containing lot of individual requests.  This collection of requests is sent to an AI provider as a single unit when we start a batch. </p> <p>There are different types of batches in LangBatch for different types of AI tasks. For Example, LangBatch has <code>ChatCompletionBatch</code> for Chat Completion tasks with Large Language Models and <code>EmbeddingBatch</code> for creating text embeddings. We will add more batch types in future to support more AI tasks.</p> <ul> <li> <p>Batch Class</p> <p>How to create batch?</p> </li> <li> <p>Chat Completion</p> <p>How to create a batch for Chat Completion?</p> </li> <li> <p>Embedding</p> <p>How to create a batch for Embedding?</p> </li> </ul>"},{"location":"concepts/types/batch/","title":"Batch","text":""},{"location":"concepts/types/batch/#initialize-a-batch","title":"Initialize a Batch","text":"<p>You can initialize a batch by passing the path to a JSONL file. File should be in OpenAI format.</p> <pre><code>from langbatch import OpenAIChatCompletionBatch\n\nbatch = OpenAIChatCompletionBatch(\"data.jsonl\")\nprint(batch.id)\n</code></pre> <p>You can also pass a list of requests to the <code>create</code> method to create a batch.</p> <pre><code>batch = OpenAIChatCompletionBatch.create([\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]},\n    {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n        ]\n    }\n])\n</code></pre> <p>Note</p> <p>When you initialize a batch, it is not started automatically. You need to call the <code>start</code> method to start the batch job.</p>"},{"location":"concepts/types/batch/#start-a-batch","title":"Start a Batch","text":"<p>You can start a batch by calling the <code>start</code> method.</p> <pre><code>batch.start()\n</code></pre>"},{"location":"concepts/types/batch/#get-batch-status","title":"Get Batch Status","text":"<p>You can get the status of a batch by calling the <code>get_status</code> method.</p> <pre><code>batch.get_status()\n</code></pre>"},{"location":"concepts/types/batch/#retry-batch","title":"Retry Batch","text":"<p>Incase of any failure with batch job due to rate limits, exceeded 24h wait time or other issues, you can retry the same batch by calling the <code>retry</code> method.</p> <pre><code>batch.retry()\n</code></pre> <p>Note</p> <p>A batch instance in LangBatch can be retried multiple times. It will internally create multiple batch jobs until the batch is successful.</p>"},{"location":"concepts/types/batch/#get-batch-results","title":"Get Batch Results","text":"<p>You can get the results of a completed batch by calling the <code>get_results</code> method.</p> <pre><code>successful_results, unsuccessful_results = batch.get_results()\n\nfor result in successful_results:\n    print(result)\n</code></pre>"},{"location":"concepts/types/batch/#get-batch-results-file","title":"Get Batch Results File","text":"<p>You can get the results file of a completed batch by calling the <code>get_results_file</code> method. This file will be in OpenAI Batch Result JSONL format.</p> <pre><code>file_path = batch.get_results_file()\n\nwith open(file_path, \"r\") as file:\n    results = file.readlines()\n\nfor result in results:\n    print(result)\n</code></pre>"},{"location":"concepts/types/batch/#get-unsuccessful-requests","title":"Get Unsuccessful Requests","text":"<p>You can get the unsuccessful requests of a batch by calling the <code>get_unsuccessful_requests</code> method. This will be useful to retry failed requests or to debug the issues with the requests.</p> <pre><code>unsuccessful_requests = batch.get_unsuccessful_requests()\n\nfor request in unsuccessful_requests:\n    print(request)\n</code></pre>"},{"location":"concepts/types/batch/#get-requests-by-custom-ids","title":"Get Requests by Custom IDs","text":"<p>You can get the requests of a batch by passing the custom ids to the <code>get_requests_by_custom_ids</code> method. This will be useful to get the requests to debug the issues with the requests.</p> <pre><code>custom_ids = [\"custom_id_1\", \"custom_id_2\"]\nrequests = batch.get_requests_by_custom_ids(custom_ids)\n\nfor request in requests:\n    print(request)\n</code></pre>"},{"location":"concepts/types/batch/#save-batch","title":"Save Batch","text":"<p>You can save a batch by calling the <code>save</code> method. This will be useful to keep track of created batches and resume the batch job later.</p> <pre><code># save batch\nbatch.save()\n\n# Save Batch to Custom Storage\nstorage = CustomStorage()\nbatch = OpenAIChatCompletionBatch(\"data.jsonl\", storage=storage)\nbatch.save()\n</code></pre> <p>Note</p> <p>By default, File based storage will be used to save the batch. You can also pass a custom storage instance when initializing the batch.</p>"},{"location":"concepts/types/batch/#load-batch","title":"Load Batch","text":"<p>You can load a batch by calling the <code>load</code> method. This will be useful to resume the batch job later.</p> <pre><code># load batch\nbatch = OpenAIChatCompletionBatch.load(\"batch_id\")\n\n# Load Batch from Custom Storage\nstorage = CustomStorage()\nbatch = OpenAIChatCompletionBatch.load(\"batch_id\", storage=storage)\n</code></pre>"},{"location":"concepts/types/chat_completion/","title":"Chat Completion Batch","text":"<p><code>ChatCompletionBatch</code> is the abstract batch class for processing chat completion requests. It's designed to utilize various Language Models (LLMs), using the OpenAI Chat Completion API format as its standard request structure.</p> <ol> <li> <p>Universal Compatibility: While based on the OpenAI format, <code>ChatCompletionBatch</code> can be used with LLMs from various providers, not just OpenAI.</p> </li> <li> <p>Extensibility: Serves as a base class for creating specialized chat completion batch classes.</p> </li> <li> <p>Standardized Input/Output: Uses OpenAI Batch File format and the OpenAI Chat Completion API format for input. Results are also consistently provided in the OpenAI format.</p> </li> </ol> <p>By standardizing on the OpenAI format, <code>ChatCompletionBatch</code> ensures consistency and interoperability across different LLM implementations within the LangBatch ecosystem.</p>"},{"location":"concepts/types/chat_completion/#initialize-a-chat-completion-batch","title":"Initialize a Chat Completion Batch","text":"<p>You can initialize a ChatCompletionBatch by passing the path to a JSONL file. File should be in OpenAI batch File format and requests should be in OpenAI Chat Completion format.</p> <pre><code>from langbatch import OpenAIChatCompletionBatch\n\nbatch = OpenAIChatCompletionBatch(\"data.jsonl\")\n</code></pre> <p>You can also pass a list of requests to the batch.</p> <pre><code>messages_list = [\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]},\n    {\"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of the moon?\"}\n    ]}\n]\n\n\nbatch = OpenAIChatCompletionBatch.create(messages_list)\n\n# Initializing with request kwargs\nbatch = OpenAIChatCompletionBatch.create(\n    messages_list, \n    request_kwargs={\"temperature\": 0.3, \"max_tokens\": 500}\n)\n</code></pre>"},{"location":"concepts/types/chat_completion/#get-results","title":"Get Results","text":"<pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(f\"Content: {result['choices'][0]['message']['content']}\")\n</code></pre>"},{"location":"concepts/types/embedding/","title":"Embedding Batch","text":"<p><code>EmbeddingBatch</code> is the abstract batch class for processing embedding requests. It utilizes the OpenAI Embedding API format as its standard request structure.</p> <ol> <li> <p>Universal Compatibility: While based on the OpenAI format, <code>EmbeddingBatch</code> can be used with embedding models from various providers, not just OpenAI.</p> </li> <li> <p>Extensibility: Serves as a base class for creating specialized embedding batch classes.</p> </li> <li> <p>Standardized Input/Output: Uses OpenAI Batch File format and the OpenAI Embedding API format for input. Results are also consistently provided in the OpenAI format.</p> </li> </ol> <p>By standardizing on the OpenAI format, <code>EmbeddingBatch</code> ensures consistency and interoperability across different embedding implementations within the LangBatch ecosystem.</p>"},{"location":"concepts/types/embedding/#initialize-an-embedding-batch","title":"Initialize an Embedding Batch","text":"<p>You can initialize an <code>EmbeddingBatch</code> by passing the path to a file. The file should be in OpenAI Batch File format.</p> <pre><code>from langbatch import OpenAIEmbeddingBatch\n\nbatch = OpenAIEmbeddingBatch(\"data.jsonl\")\n</code></pre> <p>You can also pass a list of texts to the batch.</p> <pre><code>batch = OpenAIEmbeddingBatch.create([\n    \"Lincoln was the 16th President of the United States. His face is on Mount Rushmore.\", \n    \"Steve Jobs was the co-founder of Apple Inc. He was considered a visionary and a pioneer in the personal computer revolution.\"\n])\n\n# Initializing with request kwargs\nbatch = OpenAIEmbeddingBatch.create([\n    \"Hello World\",\n    \"Hello LangBatch\"\n], request_kwargs={\"model\": \"text-embedding-3-large\"})\n</code></pre>"},{"location":"concepts/types/embedding/#get-results","title":"Get Results","text":"<pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"embedding\"])\n</code></pre>"},{"location":"getstarted/","title":"\ud83d\ude80 Get Started","text":"<p>Welcome to the LangBatch! If you're new to LangBatch, the Get Started guides will walk you through the fundamentals of working with LangBatch. These tutorials assume basic knowledge of Python.</p> <p>Let's get started!</p> <ul> <li> <p>Installation</p> <p>Learn how to install LangBatch.</p> </li> <li> <p>Running a Batch with LangBatch</p> <p>Learn how to successfully run a batch job using LangBatch.</p> </li> </ul> <p>Note</p> <p>The tutorials only provide an overview of what you can accomplish with LangBatch and the basic skills needed to utilize it effectively. For an in-depth explanation of the core concepts behind LangBatch, check out the Core Concepts page. You can also explore the How-to Guides for specific applications of LangBatch.</p>"},{"location":"getstarted/batch/","title":"Running a Batch with LangBatch","text":"<p>LangBatch provides a simple interface to run batch jobs.</p> <pre><code>from langbatch import OpenAIChatCompletionBatch\n\n# Create a batch object\nbatch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n\n# Start the batch job\nbatch.start()\n</code></pre> <p>To check the status of the batch job, use the <code>get_status</code> method:</p> <pre><code>status = batch.get_status()\nprint(status)\n</code></pre> <p>To get the results of the batch job, use the <code>get_results</code> method:</p> <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(f\"Custom ID: {result['custom_id']}\")\n    print(f\"Content: {result['choices'][0]['message']['content']}\")\n</code></pre> <p>Note</p> <p>You can perform the same actions with other providers and models.  For example, use the <code>AnthropicChatCompletionBatch</code> class to run batches with the Anthropic models. Check out the Providers section to learn more.</p> <pre><code>from langbatch import AnthropicChatCompletionBatch\n\nbatch = AnthropicChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre>"},{"location":"getstarted/install/","title":"Installation","text":"<p>To get started, install LangBatch using <code>pip</code> with the following command:</p> <pre><code>pip install langbatch\n</code></pre> <p>This will install the core LangBatch package. </p>"},{"location":"getstarted/install/#optional-provider-dependencies","title":"Optional Provider Dependencies","text":"<p>Core LangBatch package only supports OpenAI and OpenAI compatible providers. For Other providers, you need to install additional dependencies.</p> <ul> <li>Anthropic: <pre><code>pip install langbatch[Anthropic]\n</code></pre></li> </ul> <p>This will install the dependencies for using the Anthropic with LangBatch.</p> <ul> <li>Vertex AI: <pre><code>pip install langbatch[VertexAI]\n</code></pre></li> </ul> <p>This will install the dependencies for using the Vertex AI with LangBatch.</p>"},{"location":"getstarted/install/#utility-and-integration-dependencies","title":"Utility and Integration Dependencies","text":"<p>These are optional dependencies for using other utilities and integrations.</p> <ul> <li>Redis: <pre><code>pip install langbatch[redis]\n</code></pre></li> </ul> <p>This will install the dependencies for using RedisRequestQueue with LangBatch.</p>"},{"location":"getstarted/install/#install-all-dependencies","title":"Install all dependencies","text":"<pre><code>pip install langbatch[all]\n</code></pre> <p>This will install the dependencies for using all the utilities and integrations with LangBatch.</p>"},{"location":"getstarted/install/#install-from-main-branch","title":"Install from main branch","text":"<p>If you'd like to experiment with the latest features, install the most recent version from the main branch:</p> <pre><code>pip install git+https://github.com/EasyLLM/langbatch.git\n</code></pre>"},{"location":"getstarted/install/#install-from-source","title":"Install from source","text":"<p>If you're planning to contribute and make modifications to the code, ensure that you clone the repository and set it up as an editable install.</p> <pre><code>git clone https://github.com/EasyLLM/langbatch.git \ncd langbatch \npip install -e .\n</code></pre> <p>This will enable you to run LangBatch locally from the source code with immediate effect on changes without needing to reinstall.</p> <p>Next, let's create a OpenAIChatCompletionBatch object and perform batch generations.</p>"},{"location":"howtos/","title":"\ud83d\udee0\ufe0f How-to Guides","text":"<p>The how-to guides offer a more comprehensive overview of all the tools LangBatch offers and how to use them. This will help you tackle messier real-world usecases.</p> <p>The guides assume you are familiar and confortable with the LangBatch basics. If your not feel free to checkout the Get Started sections first.</p>"},{"location":"howtos/batch_dispatcher_service/","title":"A sample API service - Stream to Batch pipeline","text":"<p>In production, you may want to implement a REST API service that accepts invidual requests or list of requests and process them as batches later. </p> <p>Refer to Batch Dispatcher for more details.</p> <pre><code># main.py\nimport logging\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel\n\nfrom fastapi import FastAPI, HTTPException\nfrom langbatch import BatchHandler\nfrom langbatch.batch_queues import FileBatchQueue\nfrom langbatch.batch_storages import FileBatchStorage\nfrom langbatch.openai import OpenAIChatCompletionBatch\nfrom langbatch import BatchDispatcher\nfrom langbatch.request_queues import InMemoryRequestQueue\n\nlogging.basicConfig(level=logging.INFO)\n\n# Function to process the successfully completed batch\ndef process_batch(batch: OpenAIChatCompletionBatch):\n    successful_results, unsuccessful_results = batch.get_results()\n    for successful_result in successful_results:\n        print(successful_result[\"custom_id\"])\n        print(successful_result[\"choices\"][0][\"message\"][\"content\"])\n\n        # TODO: process the successful result\n\n# Initialize Batch Handler and Batch Dispatcher\nbatch_queue = FileBatchQueue(\"batch_queue.json\")\nbatch_storage = FileBatchStorage()\nhandler = BatchHandler(\n    batch_process_func = process_batch, \n    batch_type = OpenAIChatCompletionBatch, \n    batch_queue = batch_queue,\n    batch_storage = batch_storage,\n    wait_time = 3600 # check batches every 1 hour\n)\n\nrequest_kwargs = {\n    \"model\": \"gpt-4o-mini\",\n    \"max_tokens\": 1000,\n    \"temperature\": 0.2\n}\nqueue = InMemoryRequestQueue()\ndispatcher = BatchDispatcher(\n    batch_handler = handler, \n    queue = queue, \n    queue_threshold = 50000, # dispatch batches when the queue size &gt;= queue_threshold\n    time_threshold = 3600, # dispatch batch when the seconds since the last dispatch &gt;= time_threshold\n    # even if the queue size is less than the queue threshold\n    time_interval = 600, # check the conditions every 600 seconds to dispatch batches\n    requests_type = 'partial', # partial requests (only messages)\n    request_kwargs = request_kwargs\n)\n\n# start the dispatcher and processor in the background\ndef run_dispatcher_in_background():\n    loop = asyncio.get_event_loop()\n    loop.create_task(dispatcher.run())\n\ndef run_processor_in_background():\n    loop = asyncio.get_event_loop()\n    loop.create_task(handler.run())\n\nrun_dispatcher_in_background()\nrun_processor_in_background()\n\n# FastAPI API Service setup\napp = FastAPI()\n\nclass MessagesList(BaseModel):\n    data: List\n\nasync def handle_requests(messages_list: MessagesList):\n    # add requests to the queue\n    await asyncio.to_thread(dispatcher.queue.add_requests, list(messages_list.data))\n\n@app.post(\"/requests\")\nasync def handle_requests_api(messages_list: MessagesList):\n    try:\n        await handle_requests(messages_list)\n        return {\"status\": \"success\"}\n    except Exception as e:\n        logging.error(f\"Error handling requests: {e}\")\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre> <p>Now run command <pre><code>uvicorn main:app --reload\n</code></pre></p> <p>Use <code>curl</code> to send a POST request to the API service <pre><code>curl -X 'POST' \\\n  'http://127.0.0.1:8000/requests' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n        \"data\": [\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"How can I learn Python?\"\n                }\n            ],\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Who is the first president of the United States?\"\n                }\n            ]\n        ]\n   }'\n</code></pre></p>"},{"location":"howtos/batch_dispatcher_service_redis/","title":"Stream to Batch pipeline with RedisRequestQueue","text":"<p>The previous guide uses InMemoryRequestQueue to store the incoming requests. In production, you may want to use a persistent queue to store the incoming requests.</p> <p>Redis is a popular in-memory data store with persistent storage, and it's easy to set up and use with LangBatch. This guide will show you how to set up a Redis-based RedisRequestQueue.</p> <p>Replace <pre><code>request_queue = InMemoryRequestQueue()\n</code></pre></p> <p>with <pre><code>import os\nimport redis\n\nREDIS_URL = os.environ.get('REDIS_URL')\nredis_client = redis.from_url(REDIS_URL)\n\nrequest_queue = RedisRequestQueue(redis_client, queue_name='stream_requests')\n</code></pre></p>"},{"location":"references/","title":"References","text":"<p>Reference documents for the <code>langbatch</code> package.</p>"},{"location":"references/Batch/","title":"Batch","text":""},{"location":"references/Batch/#langbatch.Batch.Batch","title":"langbatch.Batch.Batch","text":"<p>               Bases: <code>ABC</code></p> <p>Batch class is the base class for all batch classes.</p> <p>Implementations of this class will be platform specific (OpenAI, Vertex AI, etc.)</p> Source code in <code>langbatch\\Batch.py</code> <pre><code>class Batch(ABC):\n    \"\"\"\n    Batch class is the base class for all batch classes.\n\n    Implementations of this class will be platform specific (OpenAI, Vertex AI, etc.)\n    \"\"\"\n    _url: str = \"\"\n    platform_batch_id: str | None = None\n\n    def __init__(self, file: str):\n        \"\"\"\n        Initialize the Batch class.\n\n        Args:\n            file (str): The path to the batch file. File should be in OpenAI compatible batch file in jsonl format.\n        \"\"\"\n        self._file = file\n        self.id = str(uuid.uuid4())\n\n        self._validate_requests() # Validate the requests in the batch file\n\n    @classmethod\n    def _create_batch_file_from_requests(cls, requests) -&gt; Path:\n        try:\n            batches_dir = Path(DATA_PATH) / \"created_batches\"\n            batches_dir.mkdir(exist_ok=True, parents=True)\n\n            id = str(uuid.uuid4())\n            file_path = batches_dir / f\"{id}.jsonl\"\n            with jsonlines.open(file_path, mode='w') as writer:\n                writer.write_all(requests)\n        except:\n            logging.error(f\"Error creating batch file\", exc_info=True)\n            return None\n\n        return file_path\n\n    @classmethod\n    def _create_batch_file(cls, key: str, data: List[Any], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; Path | None:\n        \"\"\"\n        Create the batch file when given a list of items.\n        For Chat Completions, this would be a list of messages.\n        For Embeddings, this would be a list of texts.\n        \"\"\"\n        requests = []\n        try:\n            for item in data:\n                try:\n                    body = request_kwargs.copy()  # Copy kwargs to avoid mutation\n                    custom_id = str(uuid.uuid4())\n\n                    body[key] = item\n\n                    request = {\n                        \"custom_id\": custom_id,\n                        \"method\": \"POST\",\n                        \"url\": cls._url,\n                        \"body\": body\n                    }\n                    requests.append(request)\n                except:\n                    logging.warning(f\"Error processing item {item}\", exc_info= True)\n                    continue\n        except:\n            logging.error(f\"Error creating requests from data to create batch file\", exc_info=True)\n            return None\n\n        file_path = cls._create_batch_file_from_requests(requests)\n\n        if file_path is None:\n            raise ValueError(\"Failed to create batch. Check the input data.\")\n\n        return cls(file_path, **batch_kwargs)\n\n    @classmethod\n    def create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n        \"\"\"\n        Creates a batch when given a list of requests. \n        These requests should be in correct Batch API request format as per the Batch type.\n        Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n        Args:\n            requests: A list of requests.\n            batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n        Returns:\n            An instance of the Batch class.\n\n        Raises:\n            ValueError: If the input data is invalid.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch.create_from_requests([\n            {   \"custom_id\": \"request-1\",\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": \"gpt-4o-mini\",\n                    \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                    \"max_tokens\": 1000\n                }\n            },\n            {\n                \"custom_id\": \"request-2\",\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": \"gpt-4o-mini\",\n                    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                    \"max_tokens\": 1000\n                }\n            }\n        ]\n        ``` \n        \"\"\"\n\n        file_path = cls._create_batch_file_from_requests(requests)\n\n        if file_path is None:\n            raise ValueError(\"Failed to create batch. Check the input data.\")\n\n        return cls(file_path, **batch_kwargs)\n\n    @classmethod\n    @abstractmethod\n    def _get_init_args(cls, meta_data) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the init arguments from meta data json file when loading a batch from storage.\n        \"\"\"\n        pass\n\n    @classmethod\n    def load(cls, id: str, storage: BatchStorage = FileBatchStorage()):\n        \"\"\"\n        Load a batch from the storage and return a Batch object.\n\n        Args:\n            id (str): The id of the batch.\n            storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n\n        Returns:\n            Batch: The batch object.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n        ```\n        \"\"\"\n        data_file, json_file = storage.load(id)\n\n        with open(json_file, 'r') as f:\n            meta_data = json.load(f)\n\n        init_args = cls._get_init_args(meta_data)\n\n        batch = cls(str(data_file), **init_args)\n        batch.platform_batch_id = meta_data['platform_batch_id']\n        batch.id = id\n\n        return batch\n\n    @abstractmethod\n    def _create_meta_data(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Create the meta data for the batch to be saved in the storage.\n        \"\"\"\n        pass\n\n    def save(self, storage: BatchStorage = FileBatchStorage()):\n        \"\"\"\n        Save the batch to the storage.\n\n        Args:\n            storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch(file)\n        batch.save()\n\n        # save the batch to file storage\n        batch.save(storage=FileBatchStorage(\"./data\"))\n        ```\n        \"\"\"\n        meta_data = self._create_meta_data()\n        meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n        storage.save(self.id, Path(self._file), meta_data)\n\n    @abstractmethod\n    def _upload_batch_file(self):\n        pass\n\n    @abstractmethod\n    def start(self):\n        \"\"\"\n        Usage:\n        ```python\n        # create a batch\n        batch = OpenAIChatCompletionBatch(file)\n\n        # start the batch process\n        batch.start()\n        ```\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_status(self):\n        \"\"\"\n        Usage:\n        ```python\n        # create a batch and start batch process\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        # get the status of the batch process\n        status = batch.get_status()\n        print(status)\n        ```\n        \"\"\"\n        pass\n\n    def _get_requests(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get all the requests from the jsonl batch file.\n        \"\"\"\n        requests = []\n        try:\n            with jsonlines.open(self._file) as reader:\n                for obj in reader:\n                    requests.append(obj)\n        except:\n            logging.error(f\"Error reading requests from batch file\", exc_info=True)\n            raise ValueError(\"Error reading requests from batch file\")\n\n        return requests\n\n    @abstractmethod\n    def _validate_request(self, request):\n        pass\n\n    def _validate_requests(self) -&gt; None:\n        \"\"\"\n        Validate all the requests in the batch file before starting the batch process.\n\n        Depends on the implementation of the _validate_request method in the subclass.\n        \"\"\"\n        invalid_requests = []\n        for request in self._get_requests():\n            valid = True\n            try:\n                self._validate_request(request['body'])\n            except:\n                logging.info(f\"Invalid request: {request}\", exc_info=True)\n                valid = False\n\n            if not valid:\n                invalid_requests.append(request['custom_id'])\n\n        if len(invalid_requests) &gt; 0:\n            raise ValueError(f\"Invalid requests: {invalid_requests}\")\n\n        if len(self._get_requests()) == 0:\n            raise ValueError(\"No requests found in the batch file\")\n\n    def _create_results_file_path(self):\n        results_dir = Path(DATA_PATH) / \"results\"\n        results_dir.mkdir(exist_ok=True)\n\n        return results_dir / f\"{self.id}.jsonl\"\n\n    @abstractmethod\n    def _download_results_file(self):\n        pass\n\n    # return results file in OpenAI compatible format\n    def get_results_file(self):\n        \"\"\"\n        Usage:\n        ```python\n        import jsonlines\n\n        # create a batch and start batch process\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        if batch.get_status() == \"completed\":\n            # get the results file\n            results_file = batch.get_results_file()\n\n            with jsonlines.open(results_file) as reader:\n                for obj in reader:\n                    print(obj)\n        ```\n        \"\"\"\n        file_path = self._download_results_file()\n        return file_path\n\n    def _prepare_results(\n        self, process_func\n    ) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n        \"\"\"\n        Prepare the results file by processing the results,\n        and separating them into successful and unsuccessful results\n        based on the status code of the response.\n\n        Depends on the implementation of the process_func method in the subclass.\n        \"\"\"\n\n        file_id = self._download_results_file()\n\n        if file_id is None:\n            return None, None\n\n        try:\n            results = []\n            with jsonlines.open(file_id) as reader:\n                for obj in reader:\n                    results.append(obj)\n\n            successful_results = []\n            unsuccessful_results = []\n            for result in results:\n                if result['response'] is None:\n                    if result['error'] is not None:\n                        error = {\n                            \"custom_id\": result['custom_id'],\n                            \"error\": result['error']\n                        }\n                    else:\n                        error = {\n                            \"custom_id\": result['custom_id'],\n                            \"error\": \"No response from the API\"\n                        }\n                    unsuccessful_results.append(error)\n                    continue\n\n                if result['response']['status_code'] == 200:\n                    choices = {\n                        \"custom_id\": result['custom_id'],\n                        **process_func(result)\n                    }\n                    successful_results.append(choices)\n                else:\n                    error = {\n                        \"custom_id\": result['custom_id'],\n                        \"error\": result['error']\n                    }\n                    unsuccessful_results.append(error)\n\n            return successful_results, unsuccessful_results\n        except:\n            logging.error(f\"Error preparing results file\", exc_info=True)\n            return None, None\n\n    # return results list\n    @abstractmethod\n    def get_results(self):\n        pass\n\n    @abstractmethod\n    def is_retryable_failure(self) -&gt; bool:\n        pass\n\n    # Retry on rate limit fail cases\n    @abstractmethod\n    def retry(self):\n        pass\n\n    def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Retrieve the unsuccessful requests from the batch.\n\n        Returns:\n            A list of requests that failed.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        if batch.get_status() == \"completed\":\n            # get the unsuccessful requests\n            unsuccessful_requests = batch.get_unsuccessful_requests()\n\n            for request in unsuccessful_requests:\n                print(request[\"custom_id\"])\n        ```\n        \"\"\"\n        custom_ids = []\n        _, unsuccessful_results = self.get_results()\n        for result in unsuccessful_results:\n            custom_ids.append(result[\"custom_id\"])\n\n        return self.get_requests_by_custom_ids(custom_ids)\n\n    def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Retrieve the requests from the batch file by custom ids.\n\n        Args:\n            custom_ids (List[str]): A list of custom ids.\n\n        Returns:\n            A list of requests.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch(file)\n        batch.start()\n\n        if batch.get_status() == \"completed\":\n            # get the requests by custom ids\n            requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n            for request in requests:\n                print(request[\"custom_id\"])\n        ```\n        \"\"\"\n        requests = []\n        with jsonlines.open(self._file) as reader:\n            for request in reader:\n                if request[\"custom_id\"] in custom_ids:\n                    requests.append(request)\n        return requests\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.__init__","title":"__init__","text":"<pre><code>__init__(file: str)\n</code></pre> <p>Initialize the Batch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the batch file. File should be in OpenAI compatible batch file in jsonl format.</p> </li> </ul> Source code in <code>langbatch\\Batch.py</code> <pre><code>def __init__(self, file: str):\n    \"\"\"\n    Initialize the Batch class.\n\n    Args:\n        file (str): The path to the batch file. File should be in OpenAI compatible batch file in jsonl format.\n    \"\"\"\n    self._file = file\n    self.id = str(uuid.uuid4())\n\n    self._validate_requests() # Validate the requests in the batch file\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise ValueError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, json_file = storage.load(id)\n\n    with open(json_file, 'r') as f:\n        meta_data = json.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.start","title":"start  <code>abstractmethod</code>","text":"<pre><code>start()\n</code></pre> <p>Usage: <pre><code># create a batch\nbatch = OpenAIChatCompletionBatch(file)\n\n# start the batch process\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@abstractmethod\ndef start(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch\n    batch = OpenAIChatCompletionBatch(file)\n\n    # start the batch process\n    batch.start()\n    ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_status","title":"get_status  <code>abstractmethod</code>","text":"<pre><code>get_status()\n</code></pre> <p>Usage: <pre><code># create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\n# get the status of the batch process\nstatus = batch.get_status()\nprint(status)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@abstractmethod\ndef get_status(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    # get the status of the batch process\n    status = batch.get_status()\n    print(status)\n    ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/Batch/#langbatch.Batch.Batch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/ChatCompletion/","title":"ChatCompletionBatch","text":""},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch","title":"langbatch.ChatCompletionBatch.ChatCompletionBatch","text":"<p>               Bases: <code>Batch</code></p> <p>ChatCompletionBatch is a base class for chat completion batch classes. Utilizes OpenAI Chat Completion API format as the standard request format.</p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>class ChatCompletionBatch(Batch):\n    \"\"\"\n    ChatCompletionBatch is a base class for chat completion batch classes.\n    Utilizes OpenAI Chat Completion API format as the standard request format.\n    \"\"\"\n    _url: str = \"/v1/chat/completions\"\n\n    def __init__(self, file) -&gt; None:\n        \"\"\"\n        Initialize the ChatCompletionBatch class.\n        \"\"\"\n        super().__init__(file)\n\n    @classmethod\n    def create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n        \"\"\"\n        Create a chat completion batch when given a list of messages.\n\n        Args:\n            data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n            request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n            batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n        Returns:\n            An instance of the ChatCompletionBatch class.\n\n        Raises:\n            ValueError: If the input data is invalid.\n\n        Usage:\n        ```python\n        batch = OpenAIChatCompletionBatch.create([\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n            ],\n            request_kwargs={\"model\": \"gpt-4o\"})\n\n        # For Vertex AI\n        batch = VertexAIChatCompletionBatch.create([\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n                [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n            ],\n            request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n            batch_kwargs={\n                \"gcp_project\": \"your-gcp-project\", \n                \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n                \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n            })\n        ```\n        \"\"\"\n        return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n\n    def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n        \"\"\"\n        Retrieve the results of the chat completion batch.\n\n        Returns:\n            A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n        Usage:\n        ```python\n        successful_results, unsuccessful_results = batch.get_results()\n        for result in successful_results:\n            print(result[\"choices\"])\n        ```\n        \"\"\"\n        process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n        return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file) -&gt; None\n</code></pre> <p>Initialize the ChatCompletionBatch class.</p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def __init__(self, file) -&gt; None:\n    \"\"\"\n    Initialize the ChatCompletionBatch class.\n    \"\"\"\n    super().__init__(file)\n</code></pre>"},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/ChatCompletion/#langbatch.ChatCompletionBatch.ChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/Embedding/","title":"EmbeddingBatch","text":""},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch","title":"langbatch.EmbeddingBatch.EmbeddingBatch","text":"<p>               Bases: <code>Batch</code></p> <p>EmbeddingBatch is a base class for embedding batch classes. Utilizes OpenAI Embedding API format as the standard request format.</p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>class EmbeddingBatch(Batch):\n    \"\"\"\n    EmbeddingBatch is a base class for embedding batch classes.\n    Utilizes OpenAI Embedding API format as the standard request format.\n    \"\"\"\n    _url: str = \"/v1/embeddings\" \n\n    def __init__(self, file) -&gt; None:\n        \"\"\"\n        Initialize the EmbeddingBatch class.\n        \"\"\"\n        super().__init__(file)\n\n    @classmethod\n    def create(cls, data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"EmbeddingBatch\":\n        \"\"\"\n        Create an embedding batch when given a list of texts.\n\n        Args:\n            data (List[str]): A list of texts to be embedded.\n            request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, encoding_format, etc.\n            batch_kwargs (Dict): Additional keyword arguments for the batch class.\n\n        Returns:\n            An instance of the EmbeddingBatch class.\n\n        Raises:\n            ValueError: If the input data is invalid.\n\n        Usage:\n        ```python\n        batch = OpenAIEmbeddingBatch.create([\n            \"Hello world\", \n            \"Hello LangBatch\"\n        ], \n            request_kwargs={\"model\": \"text-embedding-3-small\"})\n        ```\n        \"\"\"\n        return cls._create_batch_file(\"input\", data, request_kwargs, batch_kwargs)\n\n    def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n        \"\"\"\n        Retrieve the results of the embedding batch.\n\n        Returns:\n            A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n        Usage:\n        ```python\n        successful_results, unsuccessful_results = batch.get_results()\n        for result in successful_results:\n            print(result[\"embedding\"])\n        ```\n        \"\"\"\n        process_func = lambda result: {\"embedding\": result['response']['body']['data'][0]['embedding']}\n        return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch.__init__","title":"__init__","text":"<pre><code>__init__(file) -&gt; None\n</code></pre> <p>Initialize the EmbeddingBatch class.</p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>def __init__(self, file) -&gt; None:\n    \"\"\"\n    Initialize the EmbeddingBatch class.\n    \"\"\"\n    super().__init__(file)\n</code></pre>"},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; EmbeddingBatch\n</code></pre> <p>Create an embedding batch when given a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[str]</code>)           \u2013            <p>A list of texts to be embedded.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, encoding_format, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EmbeddingBatch</code>           \u2013            <p>An instance of the EmbeddingBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIEmbeddingBatch.create([\n    \"Hello world\", \n    \"Hello LangBatch\"\n], \n    request_kwargs={\"model\": \"text-embedding-3-small\"})\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"EmbeddingBatch\":\n    \"\"\"\n    Create an embedding batch when given a list of texts.\n\n    Args:\n        data (List[str]): A list of texts to be embedded.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, encoding_format, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class.\n\n    Returns:\n        An instance of the EmbeddingBatch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIEmbeddingBatch.create([\n        \"Hello world\", \n        \"Hello LangBatch\"\n    ], \n        request_kwargs={\"model\": \"text-embedding-3-small\"})\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"input\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/Embedding/#langbatch.EmbeddingBatch.EmbeddingBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the embedding batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"embedding\"])\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the embedding batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"embedding\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"embedding\": result['response']['body']['data'][0]['embedding']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/","title":"Anthropic ChatCompletionBatch","text":""},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch","title":"langbatch.anthropic.AnthropicChatCompletionBatch","text":"<p>               Bases: <code>AnthropicBatch</code>, <code>ChatCompletionBatch</code></p> <p>AnthropicChatCompletionBatch is a class for Anthropic chat completion batches.</p> <p>Usage: <pre><code>batch = AnthropicChatCompletionBatch(\"path/to/file.jsonl\", \"claude-3-sonnet-20240229\", \"your-api-key\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\anthropic.py</code> <pre><code>class AnthropicChatCompletionBatch(AnthropicBatch, ChatCompletionBatch):\n    \"\"\"\n    AnthropicChatCompletionBatch is a class for Anthropic chat completion batches.\n\n    Usage:\n    ```python\n    batch = AnthropicChatCompletionBatch(\"path/to/file.jsonl\", \"claude-3-sonnet-20240229\", \"your-api-key\")\n    batch.start()\n    ```\n    \"\"\"\n    def _convert_messages(self, messages: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        converted_messages = []\n        for message in messages:\n            if message[\"role\"] == \"assistant\" and message[\"tool_calls\"]:\n                converted_tool_calls = []\n                for tool_call in message[\"tool_calls\"]:\n                    converted_tool_call = {\n                        \"type\": \"tool_use\",\n                        \"id\": tool_call[\"id\"],\n                        \"name\": tool_call[\"function\"][\"name\"],\n                        \"input\": tool_call[\"function\"][\"arguments\"]\n                    }\n                    converted_tool_calls.append(converted_tool_call)\n                converted_message = {\"role\": \"assistant\", \"content\": [converted_tool_call]}\n            elif message[\"role\"] == \"tool\":\n                converted_message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"tool_result\",\n                            \"tool_use_id\": message[\"tool_call_id\"],\n                            \"content\": message[\"content\"]\n                        }\n                    ]\n                }\n            else:\n                converted_message = {\n                    \"role\": message[\"role\"],\n                    \"content\": self._convert_content(message[\"content\"])\n                }\n            converted_messages.append(converted_message)\n        return converted_messages\n\n    def _convert_content(self, content: Any) -&gt; List[Dict[str, Any]]:\n        if isinstance(content, str):\n            return [{\"type\": \"text\", \"text\": content}]\n        elif isinstance(content, list):\n            converted_content = []\n            for item in content:\n                if isinstance(item, str):\n                    converted_content.append({\"type\": \"text\", \"text\": item})\n                elif isinstance(item, dict):\n                    if item[\"type\"] == \"text\":\n                        converted_content.append(item)\n                    elif item[\"type\"] == \"image_url\":\n                        image_url = item[\"image_url\"][\"url\"]\n                        if image_url.startswith(\"data:\"):\n                            image_media_type = image_url.split(\";\")[0].split(\":\")[-1]\n                            image_data = image_url.split(\",\")[1]\n                        else:\n                            image_media_type, image_data = get_web_image(image_url)\n\n                        converted_content.append({\n                            \"type\": \"image\",\n                            \"source\": {\n                                \"type\": \"base64\",\n                                \"media_type\": image_media_type,\n                                \"data\": image_data\n                            }\n                        })\n\n            return converted_content\n        return []\n\n    def _convert_tools(self, tools: Optional[List[Dict[str, Any]]]):\n        if not tools:\n            return None\n\n        converted_tools = []\n        for tool in tools:\n            if tool[\"type\"] == \"function\":\n                converted_tool = {\n                    \"name\": tool[\"function\"][\"name\"],\n                    \"input_schema\": tool[\"function\"][\"parameters\"]\n                }\n                if tool[\"function\"][\"description\"]:\n                    converted_tool[\"description\"] = tool[\"function\"][\"description\"]\n                converted_tools.append(converted_tool)\n        return converted_tools\n\n    def _convert_tool_choice(self, tools_given: bool, tool_choice: Optional[Dict[str, Any]], parallel_tool_calls: Optional[bool]):\n        tool_choice_obj = None\n        if tool_choice is None and tools_given:\n            tool_choice_obj = {\"type\": \"auto\"}\n\n        if isinstance(tool_choice, str):\n            match tool_choice:\n                case \"auto\":\n                    tool_choice_obj = {\"type\": \"auto\"}\n                case \"required\":\n                    tool_choice_obj = {\"type\": \"any\"}\n                case \"none\":\n                    tool_choice_obj = {\"type\": \"auto\"} if tools_given else None\n        elif isinstance(tool_choice, dict):\n            if tool_choice[\"type\"] == \"function\":\n                return {\"type\": \"tool\", \"name\": tool_choice[\"function\"][\"name\"]}\n\n        # Handle parallel_tool_calls\n        if parallel_tool_calls and tool_choice_obj:\n            tool_choice_obj[\"disable_parallel_tool_use\"] = parallel_tool_calls\n\n        return tool_choice_obj\n\n    def _convert_request(self, req: dict) -&gt; Request:\n        custom_id = req[\"custom_id\"]\n        request = AnthropicChatCompletionRequest(**req[\"body\"])\n\n        messages = []\n        system = \"\"\n        for message in request.messages:\n            if message[\"role\"] == \"system\":\n                if isinstance(message[\"content\"], str):\n                    system = message[\"content\"]\n                elif isinstance(message[\"content\"], dict):\n                    try:\n                        system = message[\"content\"][\"text\"]\n                    except KeyError:\n                        pass\n            else:\n                messages.append(message)\n\n        messages = self._convert_messages(messages)\n\n        req = {\n            \"model\": request.model,\n            \"messages\": messages,\n            \"system\": system\n        }\n\n        if request.max_tokens:\n            req[\"max_tokens\"] = request.max_tokens\n        if request.temperature:\n            req[\"temperature\"] = request.temperature\n        if request.top_p:\n            req[\"top_p\"] = request.top_p\n        if request.stop:\n            req[\"stop_sequences\"] = request.stop\n        if request.tools:\n            tools = self._convert_tools(request.tools)\n            tool_choice = self._convert_tool_choice(tools is not None, request.tool_choice, request.parallel_tool_calls)\n            req[\"tools\"] = tools\n            req[\"tool_choice\"] = tool_choice\n\n        anthropic_request = Request(\n            custom_id=custom_id,\n            params=MessageCreateParamsNonStreaming(**req)\n        )\n        return anthropic_request\n\n    def _convert_response_message(self, message):\n        if isinstance(message.content, str):\n            return {\n                \"role\": message.role,\n                \"content\": message.content\n            }\n        elif isinstance(message.content, list):\n            tool_calls = []\n            content = []\n            for item in message.content:\n                if item.type == \"tool_use\":\n                    tool_calls.append(\n                        {\n                            \"type\": \"function\",\n                            \"id\": item.id,\n                            \"function\":{\n                                \"name\": item.name,\n                                \"arguments\": item.input\n                            }\n                        }\n                    )\n                else:\n                    content.append(item.to_dict())\n\n            return {\n                \"role\": message.role,\n                \"content\": content,\n                \"tool_calls\": tool_calls\n            }\n\n    def _convert_response(self, response) -&gt; dict:\n        if response.result.type == \"succeeded\":\n            message = response.result.message\n\n            choice = {\n                \"index\": 0,\n                \"logprobs\": None,\n                \"finish_reason\": message.stop_reason.lower(),\n                \"message\": self._convert_response_message(message)\n            }\n            choices = [choice]\n            usage = {\n                \"prompt_tokens\": message.usage.input_tokens,\n                \"completion_tokens\": message.usage.output_tokens,\n                \"total_tokens\": message.usage.input_tokens + message.usage.output_tokens\n            }\n            body = {\n                \"id\": f'{response.custom_id}',\n                \"object\": \"chat.completion\",\n                \"created\": int(time.time()),\n                \"model\": message.model,\n                \"system_fingerprint\": None,\n                \"choices\": choices,\n                \"usage\": usage\n            }\n            res = {\n                \"request_id\": response.custom_id,\n                \"status_code\": 200,\n                \"body\": body,\n            }\n\n            error = None\n        elif response.result.type == \"errored\":\n            error = {\n                \"message\": response.result.error.type,\n                \"code\": response.result.error.type\n            }\n            res = None\n        elif response.result.type == \"expired\":\n            error = {\n                \"message\": \"Request expired\",\n                \"code\": \"request_expired\"\n            }\n            res = None\n\n        # create output\n        output = {\n            \"id\": f'{response.custom_id}',\n            \"custom_id\": response.custom_id,\n            \"response\": res,\n            \"error\": error\n        }\n        return output\n\n    def _validate_request(self, request):\n        AnthropicChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = client\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, client: Anthropic = Anthropic()) -&gt; None\n</code></pre> <p>Initialize the AnthropicBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batch format.</p> </li> <li> <code>client</code>               (<code>Anthropic</code>, default:                   <code>Anthropic()</code> )           \u2013            <p>The Anthropic client.</p> </li> </ul> <p>Usage: <pre><code>batch = AnthropicChatCompletionBatch(\n    \"path/to/file.jsonl\"\n)\n</code></pre></p> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def __init__(self, file: str, client: Anthropic = Anthropic()) -&gt; None:\n    \"\"\"\n    Initialize the AnthropicBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batch format.\n        client (Anthropic): The Anthropic client.\n\n    Usage:\n    ```python\n    batch = AnthropicChatCompletionBatch(\n        \"path/to/file.jsonl\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n    self.client = client\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise ValueError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, json_file = storage.load(id)\n\n    with open(json_file, 'r') as f:\n        meta_data = json.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise ValueError(\"Batch already started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    response = self.client.beta.messages.batches.retrieve(\n        self.platform_batch_id\n    )\n    return anthropic_state_map[response.processing_status]\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    status = self.get_status()\n    if status == \"errored\" or status == \"expired\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\anthropic.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    self._create_batch()\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/Anthropic/AnthropicChatCompletionBatch/#langbatch.anthropic.AnthropicChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/","title":"OpenAIChatCompletionBatch","text":""},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch","title":"langbatch.openai.OpenAIChatCompletionBatch","text":"<p>               Bases: <code>OpenAIBatch</code>, <code>ChatCompletionBatch</code></p> <p>OpenAIChatCompletionBatch is a class for OpenAI chat completion batches. Can be used for batch processing with gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4 models</p> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>class OpenAIChatCompletionBatch(OpenAIBatch, ChatCompletionBatch):\n    \"\"\"\n    OpenAIChatCompletionBatch is a class for OpenAI chat completion batches.\n    Can be used for batch processing with gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-4 models\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n    ```\n    \"\"\"\n    _url: str = \"/v1/chat/completions\"\n\n    def _validate_request(self, request):\n        OpenAIChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, client: OpenAI = OpenAI()) -&gt; None\n</code></pre> <p>Initialize the OpenAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batchformat.</p> </li> <li> <code>client</code>               (<code>OpenAI</code>, default:                   <code>OpenAI()</code> )           \u2013            <p>The OpenAI client to use. Defaults to OpenAI().</p> </li> </ul> <p>Usage: <pre><code>batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n# With custom OpenAI client\nclient = OpenAI(\n    api_key=\"sk-proj-...\",\n    base_url=\"https://api.provider.com/v1\"\n)\nbatch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def __init__(self, file: str, client: OpenAI = OpenAI()) -&gt; None:\n    \"\"\"\n    Initialize the OpenAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batchformat.\n        client (OpenAI, optional): The OpenAI client to use. Defaults to OpenAI().\n\n    Usage:\n    ```python\n    batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n    # With custom OpenAI client\n    client = OpenAI(\n        api_key=\"sk-proj-...\",\n        base_url=\"https://api.provider.com/v1\"\n    )\n    batch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = client\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise ValueError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, json_file = storage.load(id)\n\n    with open(json_file, 'r') as f:\n        meta_data = json.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise ValueError(\"Batch already started\")\n\n    batch_input_file_id = self._upload_batch_file()\n    self._create_batch(batch_input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n    return batch.status\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    errors = self._get_errors()\n    if errors:\n        error = errors.data[0]['code']\n\n        if error == \"token_limit_exceeded\":\n            return True\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n\n    self._create_batch(batch.input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIChatCompletionBatch/#langbatch.openai.OpenAIChatCompletionBatch.cancel","title":"cancel","text":"<pre><code>cancel()\n</code></pre> <p>Usage: <pre><code># create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\n# cancel the batch process\nbatch.cancel()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def cancel(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    # cancel the batch process\n    batch.cancel()\n    ```\n    \"\"\"\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    batch = self._client.batches.cancel(self.platform_batch_id)\n    if batch.status == \"cancelling\" or batch.status == \"cancelled\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/","title":"OpenAIEmbeddingBatch","text":""},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch","title":"langbatch.openai.OpenAIEmbeddingBatch","text":"<p>               Bases: <code>OpenAIBatch</code>, <code>EmbeddingBatch</code></p> <p>OpenAIEmbeddingBatch is a class for OpenAI embedding batches. Can be used for batch processing with text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002 models</p> <p>Usage: <pre><code>batch = OpenAIEmbeddingBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>class OpenAIEmbeddingBatch(OpenAIBatch, EmbeddingBatch):\n    \"\"\"\n    OpenAIEmbeddingBatch is a class for OpenAI embedding batches.\n    Can be used for batch processing with text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002 models\n\n    Usage:\n    ```python\n    batch = OpenAIEmbeddingBatch(\"path/to/file.jsonl\")\n    batch.start()\n    ```\n    \"\"\"\n    _url: str = \"/v1/embeddings\"\n\n    def _validate_request(self, request):\n        OpenAIEmbeddingRequest(**request)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, client: OpenAI = OpenAI()) -&gt; None\n</code></pre> <p>Initialize the OpenAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in OpenAI batchformat.</p> </li> <li> <code>client</code>               (<code>OpenAI</code>, default:                   <code>OpenAI()</code> )           \u2013            <p>The OpenAI client to use. Defaults to OpenAI().</p> </li> </ul> <p>Usage: <pre><code>batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n# With custom OpenAI client\nclient = OpenAI(\n    api_key=\"sk-proj-...\",\n    base_url=\"https://api.provider.com/v1\"\n)\nbatch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def __init__(self, file: str, client: OpenAI = OpenAI()) -&gt; None:\n    \"\"\"\n    Initialize the OpenAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in OpenAI batchformat.\n        client (OpenAI, optional): The OpenAI client to use. Defaults to OpenAI().\n\n    Usage:\n    ```python\n    batch = ChatCompletionBatch(\"path/to/file.jsonl\")\n\n    # With custom OpenAI client\n    client = OpenAI(\n        api_key=\"sk-proj-...\",\n        base_url=\"https://api.provider.com/v1\"\n    )\n    batch = OpenAIBatch(\"path/to/file.jsonl\", client = client)\n    ```\n    \"\"\"\n    super().__init__(file)\n    self._client = client\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise ValueError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, json_file = storage.load(id)\n\n    with open(json_file, 'r') as f:\n        meta_data = json.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise ValueError(\"Batch already started\")\n\n    batch_input_file_id = self._upload_batch_file()\n    self._create_batch(batch_input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n    return batch.status\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the embedding batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"embedding\"])\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the embedding batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"embedding\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"embedding\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"embedding\": result['response']['body']['data'][0]['embedding']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    errors = self._get_errors()\n    if errors:\n        error = errors.data[0]['code']\n\n        if error == \"token_limit_exceeded\":\n            return True\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\openai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    batch = self._client.batches.retrieve(self.platform_batch_id)\n\n    self._create_batch(batch.input_file_id)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; EmbeddingBatch\n</code></pre> <p>Create an embedding batch when given a list of texts.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[str]</code>)           \u2013            <p>A list of texts to be embedded.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, encoding_format, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>EmbeddingBatch</code>           \u2013            <p>An instance of the EmbeddingBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIEmbeddingBatch.create([\n    \"Hello world\", \n    \"Hello LangBatch\"\n], \n    request_kwargs={\"model\": \"text-embedding-3-small\"})\n</code></pre></p> Source code in <code>langbatch\\EmbeddingBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[str], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"EmbeddingBatch\":\n    \"\"\"\n    Create an embedding batch when given a list of texts.\n\n    Args:\n        data (List[str]): A list of texts to be embedded.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, encoding_format, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class.\n\n    Returns:\n        An instance of the EmbeddingBatch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIEmbeddingBatch.create([\n        \"Hello world\", \n        \"Hello LangBatch\"\n    ], \n        request_kwargs={\"model\": \"text-embedding-3-small\"})\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"input\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/providers/OpenAI/OpenAIEmbeddingBatch/#langbatch.openai.OpenAIEmbeddingBatch.cancel","title":"cancel","text":"<pre><code>cancel()\n</code></pre> <p>Usage: <pre><code># create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\n# cancel the batch process\nbatch.cancel()\n</code></pre></p> Source code in <code>langbatch\\openai.py</code> <pre><code>def cancel(self):\n    \"\"\"\n    Usage:\n    ```python\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    # cancel the batch process\n    batch.cancel()\n    ```\n    \"\"\"\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    batch = self._client.batches.cancel(self.platform_batch_id)\n    if batch.status == \"cancelling\" or batch.status == \"cancelled\":\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/","title":"VertexAIChatCompletionBatch","text":""},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch","title":"langbatch.vertexai.VertexAIChatCompletionBatch","text":"<p>               Bases: <code>VertexAIBatch</code>, <code>ChatCompletionBatch</code></p> <p>VertexAIChatCompletionBatch is a class for Vertex AI chat completion batches. Can be used for batch processing with Gemini 1.5 Flash and Gemini 1.5 Pro models</p> <p>Usage: <pre><code>batch = VertexAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n</code></pre></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>class VertexAIChatCompletionBatch(VertexAIBatch, ChatCompletionBatch):\n    \"\"\"\n    VertexAIChatCompletionBatch is a class for Vertex AI chat completion batches.\n    Can be used for batch processing with Gemini 1.5 Flash and Gemini 1.5 Pro models\n\n    Usage:\n    ```python\n    batch = VertexAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n    ```\n    \"\"\"\n    _url: str = \"/v1/chat/completions\"\n\n    def _convert_request(self, req: dict) -&gt; str:\n        custom_schema = {\n            \"contents\": [],\n            \"systemInstruction\": None,\n            \"tools\": [],\n            \"generationConfig\": {}\n        }\n\n        request = VertexAIChatCompletionRequest(**req)\n\n        # Convert messages\n        for message in request.messages:\n            if isinstance(message, dict):\n                role = message.get(\"role\")\n                content = message.get(\"content\")\n            else:\n                role = message.role\n                content = message.content\n\n            if role == \"system\":\n                custom_schema[\"systemInstruction\"] = {\n                    \"role\": \"system\",\n                    \"parts\": {\"text\": content}\n                }\n            else:\n                custom_schema[\"contents\"].append({\n                    \"role\": role,\n                    \"parts\": {\"text\": content}\n                })\n\n        # Convert tools\n        if request.tools:\n            for tool in request.tools:\n                if isinstance(tool, dict):\n                    function = tool.get(\"function\", {})\n                else:\n                    function = tool.function\n\n                custom_schema[\"tools\"].append({\n                    \"functionDeclarations\": [{\n                        \"name\": function.get(\"name\"),\n                        \"description\": function.get(\"description\", \"\"),\n                        \"parameters\": function.get(\"parameters\", {})\n                    }]\n                })\n\n        # Convert generation config\n        gen_config = custom_schema[\"generationConfig\"]\n        if request.temperature:\n            gen_config[\"temperature\"] = request.temperature\n        if request.top_p:\n            gen_config[\"topP\"] = request.top_p\n        if request.max_tokens:\n            gen_config[\"maxOutputTokens\"] = request.max_tokens\n        if request.n:\n            gen_config[\"candidateCount\"] = request.n\n        if request.presence_penalty:\n            gen_config[\"presencePenalty\"] = request.presence_penalty\n        if request.frequency_penalty:\n            gen_config[\"frequencyPenalty\"] = request.frequency_penalty\n        if request.stop:\n            gen_config[\"stopSequences\"] = request.stop if isinstance(request.stop, list) else [request.stop] if request.stop else None\n        if request.seed:\n            gen_config[\"seed\"] = request.seed\n\n        if request.response_format:\n            mime_type_map = {\n                \"json_object\": \"application/json\",\n                \"text\": \"text/plain\",\n                \"json_schema\": \"application/json\"\n            }\n\n            gen_config[\"responseMimeType\"] = mime_type_map[request.response_format[\"type\"]]\n\n            if request.response_format[\"type\"] == \"json_schema\" and request.response_format[\"json_schema\"]:\n                gen_config[\"responseSchema\"] = request.response_format[\"json_schema\"]\n\n                # Check for single enum property to use text/x.enum mime type\n                data = json.loads(request.response_format.json_schema.schema)\n                concrete_types = [\"string\", \"number\", \"integer\", \"boolean\"]\n                if data.get(\"type\") in concrete_types and len(data.get(\"enum\", [])) == 0:\n                    gen_config[\"responseMimeType\"] = \"text/x.enum\"\n\n\n        gen_config = {k: v for k, v in gen_config.items() if v is not None}\n        custom_schema[\"generationConfig\"] = gen_config\n\n        return json.dumps(custom_schema, indent=2)\n\n    def _convert_response(self, response):\n        # Parse the input JSON\n        response_data = json.loads(response[\"response\"])\n\n        status = response[\"status\"]\n        if status != \"\":\n            if \"Bad Request: \" in status:\n                error_data = json.loads(status.split(\"Bad Request: \")[1])\n            else:\n                error_data = {\n                    \"message\": status,\n                    \"code\": \"server_error\"\n                }\n\n            error = {\n                \"message\": error_data[\"error\"][\"message\"],\n                \"code\": error_data[\"error\"][\"code\"]\n            }\n\n            res = None\n        else:\n            # Extract relevant information\n            candidates = response_data[\"candidates\"]\n            tokens = response_data[\"usageMetadata\"]\n\n            # Create the choices array\n            choices = []\n            for index, candidate in enumerate(candidates):\n                choice = {\n                    \"index\": index,\n                    \"logprobs\": None,\n                    \"finish_reason\": candidate[\"finishReason\"].lower()\n                }\n                if candidate.get(\"function_calls\", None):\n                    tool_calls = []\n                    for function_call in candidate[\"function_calls\"]:\n                        tool_call = {\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": function_call.get(\"name\"),\n                                \"arguments\": function_call.get(\"arguments\", \"{}\")\n                            }\n                        }\n                        tool_calls.append(tool_call)\n                    message = {\n                        \"role\": \"tool\",\n                        \"content\": \"\",\n                        \"tool_calls\": tool_calls\n                    }\n                else:\n                    try:\n                        content = candidate[\"content\"][\"parts\"][0][\"text\"]\n                        message = {\n                            \"role\": \"assistant\",\n                            \"content\": content\n                        }\n                    except KeyError:\n                        message = {\"role\": \"assistant\", \"content\": \"\"}\n\n                choice[\"message\"] = message\n\n                choices.append(choice)\n\n            usage = {\n                \"prompt_tokens\": tokens.get(\"promptTokenCount\", 0),\n                \"completion_tokens\": tokens.get(\"candidatesTokenCount\", 0),\n                \"total_tokens\": tokens.get(\"totalTokenCount\", 0)\n            }\n\n            # Create the body\n            body = {\n                \"id\": f'{response[\"custom_id\"]}',\n                \"object\": \"chat.completion\",\n                \"created\": int(response[\"processed_time\"].timestamp()),\n                \"model\": self.model_name,\n                \"system_fingerprint\": None,\n                \"choices\": choices,\n                \"usage\": usage\n            }\n\n            res = {\n                \"request_id\": response[\"custom_id\"],\n                \"status_code\": 200,\n                \"body\": body,\n            }\n\n            error = None\n\n        # create output\n        output = {\n            \"id\": f'{response[\"custom_id\"]}',\n            \"custom_id\": response[\"custom_id\"],\n            \"response\": res,\n            \"error\": error\n        }\n\n        return output\n\n    def _validate_request(self, request):\n        VertexAIChatCompletionRequest(**request)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.platform_batch_id","title":"platform_batch_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>platform_batch_id: str | None = None\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = str(uuid4())\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name = model_name\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.gcp_project","title":"gcp_project  <code>instance-attribute</code>","text":"<pre><code>gcp_project = gcp_project\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.bigquery_input_dataset","title":"bigquery_input_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_input_dataset = bigquery_input_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.bigquery_output_dataset","title":"bigquery_output_dataset  <code>instance-attribute</code>","text":"<pre><code>bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.__init__","title":"__init__","text":"<pre><code>__init__(file: str, model_name: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None\n</code></pre> <p>Initialize the VertexAIBatch class.</p> <p>Parameters:</p> <ul> <li> <code>file</code>               (<code>str</code>)           \u2013            <p>The path to the jsonl file in Vertex AI batch format.</p> </li> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>The name of the model to use for the batch prediction.</p> </li> <li> <code>gcp_project</code>               (<code>str</code>)           \u2013            <p>The GCP project to use for the batch prediction.</p> </li> <li> <code>bigquery_input_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction input.</p> </li> <li> <code>bigquery_output_dataset</code>               (<code>str</code>)           \u2013            <p>The BigQuery dataset to use for the batch prediction output.</p> </li> </ul> <p>Usage: <pre><code>batch = VertexAIBatch(\n    \"path/to/file.jsonl\",\n    \"model_name\",\n    \"gcp_project\",\n    \"bigquery_input_dataset\",\n    \"bigquery_output_dataset\"\n)\n</code></pre></p> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def __init__(self, file: str, model_name: str, gcp_project: str, bigquery_input_dataset: str, bigquery_output_dataset: str) -&gt; None:\n    \"\"\"\n    Initialize the VertexAIBatch class.\n\n    Args:\n        file (str): The path to the jsonl file in Vertex AI batch format.\n        model_name (str): The name of the model to use for the batch prediction.\n        gcp_project (str): The GCP project to use for the batch prediction.\n        bigquery_input_dataset (str): The BigQuery dataset to use for the batch prediction input.\n        bigquery_output_dataset (str): The BigQuery dataset to use for the batch prediction output.\n\n    Usage:\n    ```python\n    batch = VertexAIBatch(\n        \"path/to/file.jsonl\",\n        \"model_name\",\n        \"gcp_project\",\n        \"bigquery_input_dataset\",\n        \"bigquery_output_dataset\"\n    )\n    ```\n    \"\"\"\n    super().__init__(file)\n\n    self.model_name = model_name\n    self.gcp_project = gcp_project\n    self.bigquery_input_dataset = bigquery_input_dataset\n    self.bigquery_output_dataset = bigquery_output_dataset\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.create_from_requests","title":"create_from_requests  <code>classmethod</code>","text":"<pre><code>create_from_requests(requests, batch_kwargs: Dict = {})\n</code></pre> <p>Creates a batch when given a list of requests.  These requests should be in correct Batch API request format as per the Batch type. Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.</p> <p>Parameters:</p> <ul> <li> <code>requests</code>           \u2013            <p>A list of requests.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>An instance of the Batch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create_from_requests([\n    {   \"custom_id\": \"request-1\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n            \"max_tokens\": 1000\n        }\n    },\n    {\n        \"custom_id\": \"request-2\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n            \"max_tokens\": 1000\n        }\n    }\n]\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef create_from_requests(cls, requests, batch_kwargs: Dict = {}):\n    \"\"\"\n    Creates a batch when given a list of requests. \n    These requests should be in correct Batch API request format as per the Batch type.\n    Ex. for OpenAIChatCompletionBatch, requests should be a Chat Completion request with custom_id.\n\n    Args:\n        requests: A list of requests.\n        batch_kwargs (Dict, optional): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the Batch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create_from_requests([\n        {   \"custom_id\": \"request-1\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Biryani Receipe, pls.\"}],\n                \"max_tokens\": 1000\n            }\n        },\n        {\n            \"custom_id\": \"request-2\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": \"gpt-4o-mini\",\n                \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about AI\"}],\n                \"max_tokens\": 1000\n            }\n        }\n    ]\n    ``` \n    \"\"\"\n\n    file_path = cls._create_batch_file_from_requests(requests)\n\n    if file_path is None:\n        raise ValueError(\"Failed to create batch. Check the input data.\")\n\n    return cls(file_path, **batch_kwargs)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(id: str, storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Load a batch from the storage and return a Batch object.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to load the batch from. Defaults to FileBatchStorage().</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Batch</code>          \u2013            <p>The batch object.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>@classmethod\ndef load(cls, id: str, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Load a batch from the storage and return a Batch object.\n\n    Args:\n        id (str): The id of the batch.\n        storage (BatchStorage, optional): The storage to load the batch from. Defaults to FileBatchStorage().\n\n    Returns:\n        Batch: The batch object.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.load(\"123\", storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    data_file, json_file = storage.load(id)\n\n    with open(json_file, 'r') as f:\n        meta_data = json.load(f)\n\n    init_args = cls._get_init_args(meta_data)\n\n    batch = cls(str(data_file), **init_args)\n    batch.platform_batch_id = meta_data['platform_batch_id']\n    batch.id = id\n\n    return batch\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.save","title":"save","text":"<pre><code>save(storage: BatchStorage = FileBatchStorage())\n</code></pre> <p>Save the batch to the storage.</p> <p>Parameters:</p> <ul> <li> <code>storage</code>               (<code>BatchStorage</code>, default:                   <code>FileBatchStorage()</code> )           \u2013            <p>The storage to save the batch to. Defaults to FileBatchStorage().</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.save()\n\n# save the batch to file storage\nbatch.save(storage=FileBatchStorage(\"./data\"))\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def save(self, storage: BatchStorage = FileBatchStorage()):\n    \"\"\"\n    Save the batch to the storage.\n\n    Args:\n        storage (BatchStorage, optional): The storage to save the batch to. Defaults to FileBatchStorage().\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.save()\n\n    # save the batch to file storage\n    batch.save(storage=FileBatchStorage(\"./data\"))\n    ```\n    \"\"\"\n    meta_data = self._create_meta_data()\n    meta_data[\"platform_batch_id\"] = self.platform_batch_id\n\n    storage.save(self.id, Path(self._file), meta_data)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.start","title":"start","text":"<pre><code>start()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def start(self):\n    if self.platform_batch_id is not None:\n        raise ValueError(\"Batch already started\")\n\n    input_dataset = self._upload_batch_file()\n    output_dataset_id = self._create_table(self.bigquery_output_dataset)\n    output_dataset = f\"bq://{self.gcp_project}.{self.bigquery_output_dataset}.{output_dataset_id}\"\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_status","title":"get_status","text":"<pre><code>get_status()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def get_status(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    return vertexai_state_map[str(job.state.name)]\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_results_file","title":"get_results_file","text":"<pre><code>get_results_file()\n</code></pre> <p>Usage: <pre><code>import jsonlines\n\n# create a batch and start batch process\nbatch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the results file\n    results_file = batch.get_results_file()\n\n    with jsonlines.open(results_file) as reader:\n        for obj in reader:\n            print(obj)\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_results_file(self):\n    \"\"\"\n    Usage:\n    ```python\n    import jsonlines\n\n    # create a batch and start batch process\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the results file\n        results_file = batch.get_results_file()\n\n        with jsonlines.open(results_file) as reader:\n            for obj in reader:\n                print(obj)\n    ```\n    \"\"\"\n    file_path = self._download_results_file()\n    return file_path\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]\n</code></pre> <p>Retrieve the results of the chat completion batch.</p> <p>Returns:</p> <ul> <li> <code>Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]</code>           \u2013            <p>A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.</p> </li> </ul> <p>Usage: <pre><code>successful_results, unsuccessful_results = batch.get_results()\nfor result in successful_results:\n    print(result[\"choices\"])\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>def get_results(self) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]] | Tuple[None, None]:\n    \"\"\"\n    Retrieve the results of the chat completion batch.\n\n    Returns:\n        A tuple containing successful and unsuccessful results. Successful results: A list of dictionaries with \"choices\" and \"custom_id\" keys. Unsuccessful results: A list of dictionaries with \"error\" and \"custom_id\" keys.\n\n    Usage:\n    ```python\n    successful_results, unsuccessful_results = batch.get_results()\n    for result in successful_results:\n        print(result[\"choices\"])\n    ```\n    \"\"\"\n    process_func = lambda result: {\"choices\": result['response']['body']['choices']}\n    return self._prepare_results(process_func)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.is_retryable_failure","title":"is_retryable_failure","text":"<pre><code>is_retryable_failure() -&gt; bool\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def is_retryable_failure(self) -&gt; bool:\n    # TODO: implement retry logic for Vertex AI API\n    error = self._get_errors()\n    if error:\n        logging.error(f\"Error in VertexAI Batch: {error}\")\n        if \"Failed to import data. Not found: Dataset\" in error:\n            return False\n        else:\n            return False\n    else:\n        return False\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.retry","title":"retry","text":"<pre><code>retry()\n</code></pre> Source code in <code>langbatch\\vertexai.py</code> <pre><code>def retry(self):\n    if self.platform_batch_id is None:\n        raise ValueError(\"Batch not started\")\n\n    job = BatchPredictionJob(self.platform_batch_id)\n    input_dataset = job._gca_resource.input_config.bigquery_source.input_uri\n    output_dataset = job._gca_resource.output_config.bigquery_destination.output_uri\n\n    self._create_batch(input_dataset, output_dataset)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_unsuccessful_requests","title":"get_unsuccessful_requests","text":"<pre><code>get_unsuccessful_requests() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the unsuccessful requests from the batch.</p> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests that failed.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the unsuccessful requests\n    unsuccessful_requests = batch.get_unsuccessful_requests()\n\n    for request in unsuccessful_requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_unsuccessful_requests(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the unsuccessful requests from the batch.\n\n    Returns:\n        A list of requests that failed.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the unsuccessful requests\n        unsuccessful_requests = batch.get_unsuccessful_requests()\n\n        for request in unsuccessful_requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    custom_ids = []\n    _, unsuccessful_results = self.get_results()\n    for result in unsuccessful_results:\n        custom_ids.append(result[\"custom_id\"])\n\n    return self.get_requests_by_custom_ids(custom_ids)\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.get_requests_by_custom_ids","title":"get_requests_by_custom_ids","text":"<pre><code>get_requests_by_custom_ids(custom_ids: List[str]) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Retrieve the requests from the batch file by custom ids.</p> <p>Parameters:</p> <ul> <li> <code>custom_ids</code>               (<code>List[str]</code>)           \u2013            <p>A list of custom ids.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Dict[str, Any]]</code>           \u2013            <p>A list of requests.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(file)\nbatch.start()\n\nif batch.get_status() == \"completed\":\n    # get the requests by custom ids\n    requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n    for request in requests:\n        print(request[\"custom_id\"])\n</code></pre></p> Source code in <code>langbatch\\Batch.py</code> <pre><code>def get_requests_by_custom_ids(self, custom_ids: List[str]) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve the requests from the batch file by custom ids.\n\n    Args:\n        custom_ids (List[str]): A list of custom ids.\n\n    Returns:\n        A list of requests.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(file)\n    batch.start()\n\n    if batch.get_status() == \"completed\":\n        # get the requests by custom ids\n        requests = batch.get_requests_by_custom_ids([\"custom_id1\", \"custom_id2\"])\n\n        for request in requests:\n            print(request[\"custom_id\"])\n    ```\n    \"\"\"\n    requests = []\n    with jsonlines.open(self._file) as reader:\n        for request in reader:\n            if request[\"custom_id\"] in custom_ids:\n                requests.append(request)\n    return requests\n</code></pre>"},{"location":"references/providers/VertexAI/VertexAIChatCompletionBatch/#langbatch.vertexai.VertexAIChatCompletionBatch.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; ChatCompletionBatch\n</code></pre> <p>Create a chat completion batch when given a list of messages.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>List[Iterable[ChatCompletionMessageParam]]</code>)           \u2013            <p>A list of messages to be sent to the API.</p> </li> <li> <code>request_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the API call. Ex. model, messages, etc.</p> </li> <li> <code>batch_kwargs</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ChatCompletionBatch</code>           \u2013            <p>An instance of the ChatCompletionBatch class.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input data is invalid.</p> </li> </ul> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gpt-4o\"})\n\n# For Vertex AI\nbatch = VertexAIChatCompletionBatch.create([\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n        [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n    ],\n    request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n    batch_kwargs={\n        \"gcp_project\": \"your-gcp-project\", \n        \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n        \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n    })\n</code></pre></p> Source code in <code>langbatch\\ChatCompletionBatch.py</code> <pre><code>@classmethod\ndef create(cls, data: List[Iterable[ChatCompletionMessageParam]], request_kwargs: Dict = {}, batch_kwargs: Dict = {}) -&gt; \"ChatCompletionBatch\":\n    \"\"\"\n    Create a chat completion batch when given a list of messages.\n\n    Args:\n        data (List[Iterable[ChatCompletionMessageParam]]): A list of messages to be sent to the API.\n        request_kwargs (Dict): Additional keyword arguments for the API call. Ex. model, messages, etc.\n        batch_kwargs (Dict): Additional keyword arguments for the batch class. Ex. gcp_project, etc. for VertexAIChatCompletionBatch.\n\n    Returns:\n        An instance of the ChatCompletionBatch class.\n\n    Raises:\n        ValueError: If the input data is invalid.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gpt-4o\"})\n\n    # For Vertex AI\n    batch = VertexAIChatCompletionBatch.create([\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}]\n        ],\n        request_kwargs={\"model\": \"gemini-1.5-flash-002\"},\n        batch_kwargs={\n            \"gcp_project\": \"your-gcp-project\", \n            \"bigquery_input_dataset\": \"your-bigquery-input-dataset\", \n            \"bigquery_output_dataset\": \"your-bigquery-output-dataset\"\n        })\n    ```\n    \"\"\"\n    return cls._create_batch_file(\"messages\", data, request_kwargs, batch_kwargs)\n</code></pre>"},{"location":"references/utils/BatchDispatcher/","title":"BatchDispatcher","text":""},{"location":"references/utils/BatchDispatcher/#langbatch.BatchDispatcher","title":"langbatch.BatchDispatcher","text":""},{"location":"references/utils/BatchDispatcher/#langbatch.BatchDispatcher.BatchDispatcher","title":"BatchDispatcher","text":"<p>Batch dispatcher creates batches from requests in the queue and dispatches them to the batch handler. It periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.</p> <p>Usage: <pre><code># Create a batch dispatcher\nbatch_dispatcher = BatchDispatcher(\n    batch_handler=batch_handler,\n    queue=request_queue,\n    queue_threshold=50000,\n    time_threshold=3600 * 2,\n    requests_type=\"partial\",\n    request_kwargs=request_kwargs\n)\n\nasyncio.create_task(batch_dispatcher.run())\n</code></pre></p> Source code in <code>langbatch\\BatchDispatcher.py</code> <pre><code>class BatchDispatcher:\n    \"\"\"\n    Batch dispatcher creates batches from requests in the queue and dispatches them to the batch handler.\n    It periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.\n\n    Usage:\n    ```python\n    # Create a batch dispatcher\n    batch_dispatcher = BatchDispatcher(\n        batch_handler=batch_handler,\n        queue=request_queue,\n        queue_threshold=50000,\n        time_threshold=3600 * 2,\n        requests_type=\"partial\",\n        request_kwargs=request_kwargs\n    )\n\n    asyncio.create_task(batch_dispatcher.run())\n    ```\n    \"\"\"\n\n    def __init__(\n            self, \n            batch_handler: BatchHandler, \n            queue: RequestQueue, \n            queue_threshold: int = 50000, \n            time_threshold: int = 3600 * 2, \n            time_interval: int = 600, \n            requests_type: Literal[\"partial\", \"full\"] = \"partial\", \n            request_kwargs: Dict = {}\n        ):\n        self.batch_handler = batch_handler\n        self.queue = queue\n        self.queue_threshold = queue_threshold\n        self.time_threshold = time_threshold\n        self.time_interval = time_interval\n        self.last_batch_time = time.time()\n        self.requests_type = requests_type\n        self.request_kwargs = request_kwargs\n\n    async def run(self):\n        \"\"\"\n        Start the batch dispatcher as a asynchronous background task.\n        Periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.\n\n        Examples:\n            ```python\n            asyncio.create_task(batch_dispatcher.run())\n            ```\n        \"\"\"\n        while True:\n            logger.info(\"Running batch dispatcher\")\n            await self._check_batch_conditions()\n            await asyncio.sleep(self.time_interval)\n\n    async def _check_batch_conditions(self):\n        logger.info(\"Checking queue for batch creation\")\n        while True:\n            current_time = time.time()\n            queue_size = len(self.queue)\n            has_threshold_requests = queue_size &gt;= self.queue_threshold\n            reached_time_threshold = (current_time - self.last_batch_time) &gt;= self.time_threshold\n            if has_threshold_requests or (reached_time_threshold and queue_size &gt; 0):\n                logger.info(\"Creating and dispatching batch\")\n                await self._create_and_dispatch_batch()\n            else:\n                logger.info(\"No batch conditions met, waiting for next check\")\n                break\n\n    async def _create_and_dispatch_batch(self):\n        try:\n            logger.info(\"Creating batch\")\n            requests = await asyncio.to_thread(self.queue.get_requests, self.queue_threshold)\n            batch_class = self.batch_handler.batch_type\n            batch_kwargs = self.batch_handler.batch_kwargs\n            if self.requests_type == \"partial\":\n                batch = await asyncio.to_thread(batch_class.create, requests, self.request_kwargs, batch_kwargs)\n            else:\n                batch = await asyncio.to_thread(batch_class.create_from_requests, requests, batch_kwargs)\n            self.last_batch_time = time.time()\n            await self._dispatch_batch(batch)\n        except ValueError as e:\n            logger.warning(f\"Failed to create batch: {str(e)}\")\n\n    async def _dispatch_batch(self, batch: Batch):\n        logger.info(f\"Dispatching batch {batch.id}\")\n        await asyncio.to_thread(batch.save, self.batch_handler.batch_storage)\n\n        await self.batch_handler.add_batch(batch.id)\n        logger.info(f\"Batch {batch.id} dispatched successfully\")\n</code></pre>"},{"location":"references/utils/BatchDispatcher/#langbatch.BatchDispatcher.BatchDispatcher.run","title":"run  <code>async</code>","text":"<pre><code>run()\n</code></pre> <p>Start the batch dispatcher as a asynchronous background task. Periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.</p> <p>Examples:</p> <pre><code>asyncio.create_task(batch_dispatcher.run())\n</code></pre> Source code in <code>langbatch\\BatchDispatcher.py</code> <pre><code>async def run(self):\n    \"\"\"\n    Start the batch dispatcher as a asynchronous background task.\n    Periodically checks the queue size and time threshold, and creates a batch and dispatches it to the batch handler.\n\n    Examples:\n        ```python\n        asyncio.create_task(batch_dispatcher.run())\n        ```\n    \"\"\"\n    while True:\n        logger.info(\"Running batch dispatcher\")\n        await self._check_batch_conditions()\n        await asyncio.sleep(self.time_interval)\n</code></pre>"},{"location":"references/utils/BatchHandler/","title":"BatchHandler","text":""},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler","title":"langbatch.BatchHandler","text":""},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler.BatchHandler","title":"BatchHandler","text":"<p>Batch handler that handles the batches in a queue manner. It handles: <pre><code>* starting batches\n* checking the status of batches\n* processing completed batches\n* retrying failed batches\n* cancelling non retryable failed batches\n</code></pre></p> <p>Examples:</p> <pre><code># Create a batch handler process\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch\n)\nasyncio.create_task(batch_handler.run())\n\n# Add batches to the queue\nawait batch_handler.add_batch(\"123\")\nawait batch_handler.add_batch(\"456\")\n\n# With custom batch queue and batch storage\ncustom_batch_queue = MyCustomBatchQueue()\ncustom_batch_storage = MyCustomBatchStorage()\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=custom_batch_queue,\n    batch_storage=custom_batch_storage\n)\nasyncio.create_task(batch_handler.run())\n</code></pre> Source code in <code>langbatch\\BatchHandler.py</code> <pre><code>class BatchHandler:\n    \"\"\"\n    Batch handler that handles the batches in a queue manner. It handles:\n    ```\n    * starting batches\n    * checking the status of batches\n    * processing completed batches\n    * retrying failed batches\n    * cancelling non retryable failed batches\n    ```\n\n    Examples:\n        ```python\n        # Create a batch handler process\n        batch_handler = BatchHandler(\n            batch_process_func=process_batch,\n            batch_type=OpenAIChatCompletionBatch\n        )\n        asyncio.create_task(batch_handler.run())\n\n        # Add batches to the queue\n        await batch_handler.add_batch(\"123\")\n        await batch_handler.add_batch(\"456\")\n\n        # With custom batch queue and batch storage\n        custom_batch_queue = MyCustomBatchQueue()\n        custom_batch_storage = MyCustomBatchStorage()\n        batch_handler = BatchHandler(\n            batch_process_func=process_batch,\n            batch_type=OpenAIChatCompletionBatch,\n            batch_queue=custom_batch_queue,\n            batch_storage=custom_batch_storage\n        )\n        asyncio.create_task(batch_handler.run())\n        ```\n    \"\"\"\n    def __init__(\n            self, \n            batch_process_func: Callable, \n            batch_type: Type[Batch], \n            batch_queue: BatchQueue = None,\n            batch_storage: BatchStorage = None,\n            wait_time: int = 3600,\n            batch_kwargs: Dict = {}\n        ):\n        self.batch_process_func = batch_process_func\n        self.batch_type = batch_type\n        self.batch_queue = batch_queue or FileBatchQueue(\"batch_queue.json\")\n        self.queues = self.batch_queue.load()\n        self.wait_time = wait_time\n        self.batch_kwargs = batch_kwargs\n        self.batch_storage = batch_storage or FileBatchStorage()\n\n    async def add_batch(self, batch_id: str):\n        \"\"\"\n        Add a batch to the queue.\n\n        Parameters:\n            batch_id: The ID of the batch to add.\n\n        Examples:\n            ```python\n            await batch_handler.add_batch(\"123\")\n            ```\n        \"\"\"\n        self.queues[\"pending\"].append(batch_id)\n        self._save_queues()\n        logger.info(f\"Added batch {batch_id} to pending queue\")\n\n    async def start_batch(self, batch: Batch):\n        if batch.id in self.queues[\"pending\"]:\n            try:\n                await asyncio.to_thread(batch.start)\n                self.queues[\"processing\"].append(batch.id)\n                logger.info(f\"Moved batch {batch.id} from pending to processing queue\")\n            except:\n                logger.error(f\"Error starting batch {batch.id}\", exc_info=True)\n            finally:\n                self.queues[\"pending\"].remove(batch.id)\n\n            self._save_queues() \n        else:\n            logger.warning(f\"Batch {batch.id} not found in pending queue\")\n\n    async def process_completed_batch(self, batch: Batch):\n        try:\n            logger.info(f\"Processing completed batch {batch.id}\")\n            if batch.id in self.queues[\"processing\"]:\n                try:\n                    await asyncio.to_thread(self.batch_process_func, batch)\n                    logger.info(f\"Processed batch {batch.id}\")\n                except:\n                    logger.error(f\"Error processing completed batch {batch.id}\", exc_info=True)\n                self.queues[\"processing\"].remove(batch.id)\n                self._save_queues()\n                logger.info(f\"Removed completed batch {batch.id} from processing queue\")\n            else:\n                logger.warning(f\"Completed batch {batch.id} not found in processing queue\")\n        except:\n            logger.error(f\"Error processing completed batch {batch.id}\", exc_info=True)\n\n    async def retry_batch(self, batch: Batch):\n        if batch.id in self.queues[\"processing\"]:\n            try:\n                logger.info(f\"Retrying batch {batch.id}\")\n                await asyncio.to_thread(batch.retry)\n            except:\n                logger.error(f\"Error retrying batch {batch.id}\", exc_info=True)\n                await self.cancel_batch(batch.id)\n        else:\n            logger.warning(f\"Batch {batch.id} not found in processing queue for retry\")\n\n    async def cancel_batch(self, batch_id: str):\n        for queue in self.queues.values():\n            if batch_id in queue:\n                queue.remove(batch_id)\n                self._save_queues()\n                logger.info(f\"Cancelled and removed batch {batch_id} from queue\")\n                return\n        logger.warning(f\"Batch {batch_id} not found in any queue for cancellation\")\n\n    def _save_queues(self):\n        self.batch_queue.save(self.queues)\n\n    async def run(self):\n        \"\"\"\n        Start the batch handler as a asynchronous background task.\n        Periodically checks the status of batches in the queue and processes them accordingly.\n\n        Usage:\n        ```python\n        asyncio.create_task(batch_handler.run())\n        ```\n        \"\"\"\n        while True:\n            logger.info(\"Handling batches\")\n            retried_batches = 0\n            for batch_id in self.queues[\"processing\"]:\n                if self.batch_storage:\n                    batch = self.batch_type.load(batch_id, storage = self.batch_storage)\n                else:\n                    batch = self.batch_type.load(batch_id)\n                status = BatchStatus(await asyncio.to_thread(batch.get_status))\n\n                if status == BatchStatus.COMPLETED:\n                    await self.process_completed_batch(batch)\n                elif status in [BatchStatus.FAILED, BatchStatus.EXPIRED]:\n                    if retried_batches &lt; 4:\n                        retried = await self._handle_failed_or_expired_batch(batch, status)\n                        if retried:\n                            retried_batches += 1\n                elif status in [BatchStatus.CANCELLING, BatchStatus.CANCELLED]:\n                    await self.cancel_batch(batch_id)\n                elif status not in [BatchStatus.VALIDATING, BatchStatus.IN_PROGRESS, BatchStatus.FINALIZING]:\n                    logger.error(f\"Unknown status {status.value} for batch {batch_id}\")\n                    await self.cancel_batch(batch_id)\n\n            if retried_batches &lt; 4:\n                started_batches = 0\n                for batch_id in self.queues[\"pending\"]:\n                    if self.batch_storage:\n                        batch = self.batch_type.load(batch_id, storage=self.batch_storage)\n                    else:\n                        batch = self.batch_type.load(batch_id)\n                    await self.start_batch(batch)\n                    started_batches += 1\n\n                    if (started_batches + retried_batches) == 4:\n                        break\n\n            await asyncio.sleep(self.wait_time)\n\n    async def _handle_failed_or_expired_batch(self, batch: 'Batch', status: BatchStatus):\n        try:\n            if status == BatchStatus.FAILED:\n                retryable = await batch.is_retryable_failure()\n                if retryable:\n                    await asyncio.to_thread(self.retry_batch, batch)\n                    return True\n                else:\n                    logger.warning(f\"Batch {batch.id} failed due to non token-limit error\")\n                    await asyncio.to_thread(self.cancel_batch, batch.id)\n                    return False\n            elif status == BatchStatus.EXPIRED:\n                await asyncio.to_thread(self.retry_batch, batch)\n                return True\n        except Exception as e:\n            logger.error(f\"Error handling {status.value} batch {batch.id}: {e}\")\n            return False\n</code></pre>"},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler.BatchHandler.add_batch","title":"add_batch  <code>async</code>","text":"<pre><code>add_batch(batch_id: str)\n</code></pre> <p>Add a batch to the queue.</p> <p>Parameters:</p> <ul> <li> <code>batch_id</code>               (<code>str</code>)           \u2013            <p>The ID of the batch to add.</p> </li> </ul> <p>Examples:</p> <pre><code>await batch_handler.add_batch(\"123\")\n</code></pre> Source code in <code>langbatch\\BatchHandler.py</code> <pre><code>async def add_batch(self, batch_id: str):\n    \"\"\"\n    Add a batch to the queue.\n\n    Parameters:\n        batch_id: The ID of the batch to add.\n\n    Examples:\n        ```python\n        await batch_handler.add_batch(\"123\")\n        ```\n    \"\"\"\n    self.queues[\"pending\"].append(batch_id)\n    self._save_queues()\n    logger.info(f\"Added batch {batch_id} to pending queue\")\n</code></pre>"},{"location":"references/utils/BatchHandler/#langbatch.BatchHandler.BatchHandler.run","title":"run  <code>async</code>","text":"<pre><code>run()\n</code></pre> <p>Start the batch handler as a asynchronous background task. Periodically checks the status of batches in the queue and processes them accordingly.</p> <p>Usage: <pre><code>asyncio.create_task(batch_handler.run())\n</code></pre></p> Source code in <code>langbatch\\BatchHandler.py</code> <pre><code>async def run(self):\n    \"\"\"\n    Start the batch handler as a asynchronous background task.\n    Periodically checks the status of batches in the queue and processes them accordingly.\n\n    Usage:\n    ```python\n    asyncio.create_task(batch_handler.run())\n    ```\n    \"\"\"\n    while True:\n        logger.info(\"Handling batches\")\n        retried_batches = 0\n        for batch_id in self.queues[\"processing\"]:\n            if self.batch_storage:\n                batch = self.batch_type.load(batch_id, storage = self.batch_storage)\n            else:\n                batch = self.batch_type.load(batch_id)\n            status = BatchStatus(await asyncio.to_thread(batch.get_status))\n\n            if status == BatchStatus.COMPLETED:\n                await self.process_completed_batch(batch)\n            elif status in [BatchStatus.FAILED, BatchStatus.EXPIRED]:\n                if retried_batches &lt; 4:\n                    retried = await self._handle_failed_or_expired_batch(batch, status)\n                    if retried:\n                        retried_batches += 1\n            elif status in [BatchStatus.CANCELLING, BatchStatus.CANCELLED]:\n                await self.cancel_batch(batch_id)\n            elif status not in [BatchStatus.VALIDATING, BatchStatus.IN_PROGRESS, BatchStatus.FINALIZING]:\n                logger.error(f\"Unknown status {status.value} for batch {batch_id}\")\n                await self.cancel_batch(batch_id)\n\n        if retried_batches &lt; 4:\n            started_batches = 0\n            for batch_id in self.queues[\"pending\"]:\n                if self.batch_storage:\n                    batch = self.batch_type.load(batch_id, storage=self.batch_storage)\n                else:\n                    batch = self.batch_type.load(batch_id)\n                await self.start_batch(batch)\n                started_batches += 1\n\n                if (started_batches + retried_batches) == 4:\n                    break\n\n        await asyncio.sleep(self.wait_time)\n</code></pre>"},{"location":"references/utils/BatchQueue/","title":"BatchQueue Classes","text":""},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.BatchQueue","title":"langbatch.batch_queues.BatchQueue","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for batch queue. Implementations should provide a way to save and load batch queues.</p> <p>Used in BatchHandler to save and load the batch queues.</p> <p>Usage: <pre><code>import asyncio\n\n# Using default FileBatchQueue\nfile_batch_queue = FileBatchQueue(\"batch_queue.json\")\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=file_batch_queue\n)\nasyncio.create_task(batch_handler.run())\n\n# With custom batch queue\nclass MyCustomBatchQueue(BatchQueue):\n    def save(self, queue: Dict[str, List[str]]):\n        # Custom save logic\n\n    def load(self) -&gt; Dict[str, List[str]]:\n        # Custom load logic\n\ncustom_batch_queue = MyCustomBatchQueue()\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=custom_batch_queue\n)\nasyncio.create_task(batch_handler.run())\n</code></pre></p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>class BatchQueue(ABC):\n    \"\"\"\n    Abstract class for batch queue.\n    Implementations should provide a way to save and load batch queues.\n\n    Used in BatchHandler to save and load the batch queues.\n\n    Usage:\n    ```python\n    import asyncio\n\n    # Using default FileBatchQueue\n    file_batch_queue = FileBatchQueue(\"batch_queue.json\")\n    batch_handler = BatchHandler(\n        batch_process_func=process_batch,\n        batch_type=OpenAIChatCompletionBatch,\n        batch_queue=file_batch_queue\n    )\n    asyncio.create_task(batch_handler.run())\n\n    # With custom batch queue\n    class MyCustomBatchQueue(BatchQueue):\n        def save(self, queue: Dict[str, List[str]]):\n            # Custom save logic\n\n        def load(self) -&gt; Dict[str, List[str]]:\n            # Custom load logic\n\n    custom_batch_queue = MyCustomBatchQueue()\n    batch_handler = BatchHandler(\n        batch_process_func=process_batch,\n        batch_type=OpenAIChatCompletionBatch,\n        batch_queue=custom_batch_queue\n    )\n    asyncio.create_task(batch_handler.run())\n    ```\n    \"\"\"\n    @abstractmethod\n    def save(self, queue: Dict[str, List[str]]):\n        \"\"\"\n        Save the batch queue.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load(self) -&gt; Dict[str, List[str]]:\n        \"\"\"\n        Load the batch queue.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.BatchQueue.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(queue: Dict[str, List[str]])\n</code></pre> <p>Save the batch queue.</p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>@abstractmethod\ndef save(self, queue: Dict[str, List[str]]):\n    \"\"\"\n    Save the batch queue.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.BatchQueue.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load() -&gt; Dict[str, List[str]]\n</code></pre> <p>Load the batch queue.</p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>@abstractmethod\ndef load(self) -&gt; Dict[str, List[str]]:\n    \"\"\"\n    Load the batch queue.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue","title":"langbatch.batch_queues.FileBatchQueue","text":"<p>               Bases: <code>BatchQueue</code></p> <p>Batch queue that saves the queue to a file.</p> <p>Usage: <pre><code>queue = FileBatchQueue(\"batch_queue.json\")\n\nbatch_handler = BatchHandler(\n    batch_process_func=process_batch,\n    batch_type=OpenAIChatCompletionBatch,\n    batch_queue=queue\n)\n\nasyncio.create_task(batch_handler.run())\n</code></pre></p> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>class FileBatchQueue(BatchQueue):\n    \"\"\"\n    Batch queue that saves the queue to a file.\n\n    Usage:\n    ```python\n    queue = FileBatchQueue(\"batch_queue.json\")\n\n    batch_handler = BatchHandler(\n        batch_process_func=process_batch,\n        batch_type=OpenAIChatCompletionBatch,\n        batch_queue=queue\n    )\n\n    asyncio.create_task(batch_handler.run())\n    ```\n    \"\"\"\n    def __init__(self, file_path: str):\n        self.file_path = file_path\n\n    def save(self, queue: Dict[str, List[str]]):\n        try:\n            with open(self.file_path, 'w') as f:\n                json.dump(queue, f)\n        except IOError as e:\n            logger.error(f\"Error saving queue to file: {e}\")\n            raise\n\n    def load(self) -&gt; Dict[str, List[str]]:\n        try:\n            if os.path.exists(self.file_path):\n                with open(self.file_path, 'r') as f:\n                    return json.load(f)\n            return {\"pending\": [], \"processing\": []}\n        except IOError as e:\n            logger.error(f\"Error loading queue from file: {e}\")\n            raise\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.file_path","title":"file_path  <code>instance-attribute</code>","text":"<pre><code>file_path = file_path\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.__init__","title":"__init__","text":"<pre><code>__init__(file_path: str)\n</code></pre> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>def __init__(self, file_path: str):\n    self.file_path = file_path\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.save","title":"save","text":"<pre><code>save(queue: Dict[str, List[str]])\n</code></pre> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>def save(self, queue: Dict[str, List[str]]):\n    try:\n        with open(self.file_path, 'w') as f:\n            json.dump(queue, f)\n    except IOError as e:\n        logger.error(f\"Error saving queue to file: {e}\")\n        raise\n</code></pre>"},{"location":"references/utils/BatchQueue/#langbatch.batch_queues.FileBatchQueue.load","title":"load","text":"<pre><code>load() -&gt; Dict[str, List[str]]\n</code></pre> Source code in <code>langbatch\\batch_queues.py</code> <pre><code>def load(self) -&gt; Dict[str, List[str]]:\n    try:\n        if os.path.exists(self.file_path):\n            with open(self.file_path, 'r') as f:\n                return json.load(f)\n        return {\"pending\": [], \"processing\": []}\n    except IOError as e:\n        logger.error(f\"Error loading queue from file: {e}\")\n        raise\n</code></pre>"},{"location":"references/utils/BatchStorage/","title":"BatchStorage Classes","text":""},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.BatchStorage","title":"langbatch.batch_storages.BatchStorage","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for batch storage. Implementations should provide a way to save and load batches.</p> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n\n# Using default FileBatchStorage\nbatch.save(storage=FileBatchStorage()) # same as batch.save()\nbatch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage()) # same as OpenAIChatCompletionBatch.load(\"1ff73c3f\")\n\n# With custom storage\nclass MyCustomBatchStorage(BatchStorage):\n    def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n        # Custom save logic\n\n    def load(self, id: str) -&gt; Tuple[Path, Path]:\n        # Custom load logic\n\ncustom_storage = MyCustomBatchStorage()\n\n# Using custom storage\nbatch.save(storage=custom_storage)\nbatch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=custom_storage)\n</code></pre></p> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>class BatchStorage(ABC):\n    \"\"\"\n    Abstract class for batch storage.\n    Implementations should provide a way to save and load batches.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n\n    # Using default FileBatchStorage\n    batch.save(storage=FileBatchStorage()) # same as batch.save()\n    batch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage()) # same as OpenAIChatCompletionBatch.load(\"1ff73c3f\")\n\n    # With custom storage\n    class MyCustomBatchStorage(BatchStorage):\n        def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n            # Custom save logic\n\n        def load(self, id: str) -&gt; Tuple[Path, Path]:\n            # Custom load logic\n\n    custom_storage = MyCustomBatchStorage()\n\n    # Using custom storage\n    batch.save(storage=custom_storage)\n    batch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=custom_storage)\n    ```\n    \"\"\"\n\n    @abstractmethod\n    def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n        \"\"\"\n        Save the batch data and metadata to the storage.\n\n        Args:\n            id (str): The id of the batch.\n            data_file (Path): The path to the batch data file.\n            meta_data (Dict[str, Any]): The metadata of the batch.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load(self, id: str) -&gt; Tuple[Path, Path]:\n        \"\"\"\n        Load the batch data and metadata from the storage.\n\n        Args:\n            id (str): The id of the batch.\n\n        Returns:\n            Tuple[Path, Path]: The path to the batch data jsonlfile and the path to the metadata jsonfile.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.BatchStorage.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(id: str, data_file: Path, meta_data: Dict[str, Any])\n</code></pre> <p>Save the batch data and metadata to the storage.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> <li> <code>data_file</code>               (<code>Path</code>)           \u2013            <p>The path to the batch data file.</p> </li> <li> <code>meta_data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>The metadata of the batch.</p> </li> </ul> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>@abstractmethod\ndef save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n    \"\"\"\n    Save the batch data and metadata to the storage.\n\n    Args:\n        id (str): The id of the batch.\n        data_file (Path): The path to the batch data file.\n        meta_data (Dict[str, Any]): The metadata of the batch.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.BatchStorage.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(id: str) -&gt; Tuple[Path, Path]\n</code></pre> <p>Load the batch data and metadata from the storage.</p> <p>Parameters:</p> <ul> <li> <code>id</code>               (<code>str</code>)           \u2013            <p>The id of the batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[Path, Path]</code>           \u2013            <p>Tuple[Path, Path]: The path to the batch data jsonlfile and the path to the metadata jsonfile.</p> </li> </ul> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>@abstractmethod\ndef load(self, id: str) -&gt; Tuple[Path, Path]:\n    \"\"\"\n    Load the batch data and metadata from the storage.\n\n    Args:\n        id (str): The id of the batch.\n\n    Returns:\n        Tuple[Path, Path]: The path to the batch data jsonlfile and the path to the metadata jsonfile.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage","title":"langbatch.batch_storages.FileBatchStorage","text":"<p>               Bases: <code>BatchStorage</code></p> <p>Batch storage that saves the batch data and metadata to the file system.</p> <p>Usage: <pre><code>batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\nbatch.start()\n\n# Save the batch\nbatch.save(storage=FileBatchStorage())\n\n# Load the batch\nbatch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage())\n</code></pre></p> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>class FileBatchStorage(BatchStorage):\n    \"\"\"\n    Batch storage that saves the batch data and metadata to the file system.\n\n    Usage:\n    ```python\n    batch = OpenAIChatCompletionBatch(\"path/to/file.jsonl\")\n    batch.start()\n\n    # Save the batch\n    batch.save(storage=FileBatchStorage())\n\n    # Load the batch\n    batch = OpenAIChatCompletionBatch.load(\"1ff73c3f\", storage=FileBatchStorage())\n    ```\n    \"\"\"\n\n    def __init__(self, directory: str = DATA_PATH):\n        \"\"\"\n        Initialize the FileBatchStorage. Will create or use a directory named 'saved_batches' in the given directory to save the batches.\n\n        Args:\n            directory (str): The directory to save the batches. Defaults to the DATA_PATH.\n        \"\"\"\n        self.saved_batches_directory = Path(directory) / \"saved_batches\"\n        self.saved_batches_directory.mkdir(exist_ok=True, parents=True)\n\n    def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n        with open(self.saved_batches_directory / f\"{id}.json\", 'w') as f:\n            json.dump(meta_data, f)\n\n        destination = self.saved_batches_directory / f\"{id}.jsonl\"\n        if not destination.exists(): \n            # if the file does not exist, copy the file from the data_file\n            shutil.copy(data_file, destination)\n\n    def load(self, id: str) -&gt; Tuple[Path, Path]:\n        data_file = self.saved_batches_directory / f\"{id}.jsonl\"\n        json_file = self.saved_batches_directory / f\"{id}.json\"\n\n        if not data_file.is_file() or not json_file.is_file():\n            raise ValueError(f\"Batch with id {id} not found\")\n\n        return data_file, json_file\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.saved_batches_directory","title":"saved_batches_directory  <code>instance-attribute</code>","text":"<pre><code>saved_batches_directory = Path(directory) / 'saved_batches'\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.__init__","title":"__init__","text":"<pre><code>__init__(directory: str = DATA_PATH)\n</code></pre> <p>Initialize the FileBatchStorage. Will create or use a directory named 'saved_batches' in the given directory to save the batches.</p> <p>Parameters:</p> <ul> <li> <code>directory</code>               (<code>str</code>, default:                   <code>DATA_PATH</code> )           \u2013            <p>The directory to save the batches. Defaults to the DATA_PATH.</p> </li> </ul> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>def __init__(self, directory: str = DATA_PATH):\n    \"\"\"\n    Initialize the FileBatchStorage. Will create or use a directory named 'saved_batches' in the given directory to save the batches.\n\n    Args:\n        directory (str): The directory to save the batches. Defaults to the DATA_PATH.\n    \"\"\"\n    self.saved_batches_directory = Path(directory) / \"saved_batches\"\n    self.saved_batches_directory.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.save","title":"save","text":"<pre><code>save(id: str, data_file: Path, meta_data: Dict[str, Any])\n</code></pre> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>def save(self, id: str, data_file: Path, meta_data: Dict[str, Any]):\n    with open(self.saved_batches_directory / f\"{id}.json\", 'w') as f:\n        json.dump(meta_data, f)\n\n    destination = self.saved_batches_directory / f\"{id}.jsonl\"\n    if not destination.exists(): \n        # if the file does not exist, copy the file from the data_file\n        shutil.copy(data_file, destination)\n</code></pre>"},{"location":"references/utils/BatchStorage/#langbatch.batch_storages.FileBatchStorage.load","title":"load","text":"<pre><code>load(id: str) -&gt; Tuple[Path, Path]\n</code></pre> Source code in <code>langbatch\\batch_storages.py</code> <pre><code>def load(self, id: str) -&gt; Tuple[Path, Path]:\n    data_file = self.saved_batches_directory / f\"{id}.jsonl\"\n    json_file = self.saved_batches_directory / f\"{id}.json\"\n\n    if not data_file.is_file() or not json_file.is_file():\n        raise ValueError(f\"Batch with id {id} not found\")\n\n    return data_file, json_file\n</code></pre>"},{"location":"references/utils/RequestQueue/","title":"RequestQueue Classes","text":"<p>Note</p> <p>Please make sure to pass the the correct type of requests to the queue as per the type of Batch you are going to use. For Example, if you are using ChatCompletionBatch as your batch type, then you should pass the requests in the same format as the  ChatCompletionBatch.create() expects. For EmbeddingBatch, refer EmbeddingBatch.create()</p>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RequestQueue","title":"langbatch.request_queues.RequestQueue","text":"<p>               Bases: <code>ABC</code></p> <p>RequestQueue is an abstract class for request queues. Implementations should provide a way to add and retrieve requests.</p> <p>Used in <code>BatchDispatcher</code> to get requests.</p> <p>Usage: <pre><code>request_queue = InMemoryRequestQueue()\nrequest_queue.add_requests([\n    [\n        {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n    ],\n    [\n        {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n        {\"role\": \"assistant\", \"content\": \"George Washington\"},\n        {\"role\": \"user\", \"content\": \"Second?\"}\n    ]\n])\n\nbatch_dispatcher = BatchDispatcher(\n    batch_handler=batch_handler,\n    queue=request_queue\n)\n\nasyncio.create_task(batch_dispatcher.run())\n</code></pre></p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>class RequestQueue(ABC):\n    \"\"\"\n    RequestQueue is an abstract class for request queues.\n    Implementations should provide a way to add and retrieve requests.\n\n    Used in `BatchDispatcher` to get requests.\n\n    Usage:\n    ```python\n    request_queue = InMemoryRequestQueue()\n    request_queue.add_requests([\n        [\n            {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n        ],\n        [\n            {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n            {\"role\": \"assistant\", \"content\": \"George Washington\"},\n            {\"role\": \"user\", \"content\": \"Second?\"}\n        ]\n    ])\n\n    batch_dispatcher = BatchDispatcher(\n        batch_handler=batch_handler,\n        queue=request_queue\n    )\n\n    asyncio.create_task(batch_dispatcher.run())\n    ```\n    \"\"\"\n    @abstractmethod\n    def add_requests(self, requests: List[Any]):\n        \"\"\"\n        Add requests to the queue\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_requests(self, count: int) -&gt; List[Any]:\n        \"\"\"\n        Get requests from the queue\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __len__(self):\n        pass\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RequestQueue.add_requests","title":"add_requests  <code>abstractmethod</code>","text":"<pre><code>add_requests(requests: List[Any])\n</code></pre> <p>Add requests to the queue</p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>@abstractmethod\ndef add_requests(self, requests: List[Any]):\n    \"\"\"\n    Add requests to the queue\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RequestQueue.get_requests","title":"get_requests  <code>abstractmethod</code>","text":"<pre><code>get_requests(count: int) -&gt; List[Any]\n</code></pre> <p>Get requests from the queue</p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>@abstractmethod\ndef get_requests(self, count: int) -&gt; List[Any]:\n    \"\"\"\n    Get requests from the queue\n    \"\"\"\n    pass\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.InMemoryRequestQueue","title":"langbatch.request_queues.InMemoryRequestQueue","text":"<p>               Bases: <code>RequestQueue</code></p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>class InMemoryRequestQueue(RequestQueue):\n    def __init__(self):\n        self.queue = deque()\n\n    def add_requests(self, requests: List[Any]):\n        self.queue.extend(requests)\n        logging.info(f\"Added {len(requests)} requests to queue\")\n\n    def get_requests(self, count: int) -&gt; List[Any]:\n        if count &gt; len(self.queue):\n            count = len(self.queue)\n        return [self.queue.popleft() for _ in range(count)]\n\n    def __len__(self):\n        return len(self.queue)\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.InMemoryRequestQueue.add_requests","title":"add_requests","text":"<pre><code>add_requests(requests: List[Any])\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def add_requests(self, requests: List[Any]):\n    self.queue.extend(requests)\n    logging.info(f\"Added {len(requests)} requests to queue\")\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.InMemoryRequestQueue.get_requests","title":"get_requests","text":"<pre><code>get_requests(count: int) -&gt; List[Any]\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def get_requests(self, count: int) -&gt; List[Any]:\n    if count &gt; len(self.queue):\n        count = len(self.queue)\n    return [self.queue.popleft() for _ in range(count)]\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue","title":"langbatch.request_queues.RedisRequestQueue","text":"<p>               Bases: <code>RequestQueue</code></p> <p>RedisRequestQueue is a request queue that uses a Redis list to store requests.</p> <p>Usage: <pre><code>import os\nimport redis\n\nREDIS_URL = os.environ.get('REDIS_URL')\nredis_client = redis.from_url(REDIS_URL)\n\nrequest_queue = RedisRequestQueue(redis_client, queue_name='turbo_requests')\nrequest_queue.add_requests([\n    [\n        {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n    ],\n    [\n        {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n        {\"role\": \"assistant\", \"content\": \"George Washington\"},\n        {\"role\": \"user\", \"content\": \"Second?\"}\n    ]\n])\n</code></pre></p> Source code in <code>langbatch\\request_queues.py</code> <pre><code>class RedisRequestQueue(RequestQueue):\n    \"\"\"\n    RedisRequestQueue is a request queue that uses a Redis list to store requests.\n\n    Usage:\n    ```python\n    import os\n    import redis\n\n    REDIS_URL = os.environ.get('REDIS_URL')\n    redis_client = redis.from_url(REDIS_URL)\n\n    request_queue = RedisRequestQueue(redis_client, queue_name='turbo_requests')\n    request_queue.add_requests([\n        [\n            {\"role\": \"user\", \"content\": \"How can I learn Python?\"}\n        ],\n        [\n            {\"role\": \"user\", \"content\": \"Who is the first president of the United States?\"},\n            {\"role\": \"assistant\", \"content\": \"George Washington\"},\n            {\"role\": \"user\", \"content\": \"Second?\"}\n        ]\n    ])\n    ```\n    \"\"\"\n    def __init__(self, redis_client: redis.Redis, queue_name: str = 'request_queue'):\n        self.redis_client = redis_client\n        self.queue_name = queue_name\n\n    def add_requests(self, requests: List[Any]):\n        count = len(requests)\n        for request in requests:\n            self.redis_client.rpush(self.queue_name, str(json.dumps(request)))\n        logging.debug(f\"Added {count} requests to queue.\")\n\n    def get_requests(self, count: int) -&gt; List[Any]:\n        size = len(self)\n        if count &gt; size:\n            count = size\n\n        if count == 0:\n            return []\n\n        items = self.redis_client.lpop(self.queue_name, count=count)\n        if items is None:\n            return []\n\n        results = [json.loads(item.decode('utf-8')) for item in items]\n\n        logging.debug(f\"Retrieved {len(results)} requests from queue.\")\n        return results\n\n    def __len__(self):\n        length = self.redis_client.llen(self.queue_name)\n        return length\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue.__init__","title":"__init__","text":"<pre><code>__init__(redis_client: Redis, queue_name: str = 'request_queue')\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def __init__(self, redis_client: redis.Redis, queue_name: str = 'request_queue'):\n    self.redis_client = redis_client\n    self.queue_name = queue_name\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue.add_requests","title":"add_requests","text":"<pre><code>add_requests(requests: List[Any])\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def add_requests(self, requests: List[Any]):\n    count = len(requests)\n    for request in requests:\n        self.redis_client.rpush(self.queue_name, str(json.dumps(request)))\n    logging.debug(f\"Added {count} requests to queue.\")\n</code></pre>"},{"location":"references/utils/RequestQueue/#langbatch.request_queues.RedisRequestQueue.get_requests","title":"get_requests","text":"<pre><code>get_requests(count: int) -&gt; List[Any]\n</code></pre> Source code in <code>langbatch\\request_queues.py</code> <pre><code>def get_requests(self, count: int) -&gt; List[Any]:\n    size = len(self)\n    if count &gt; size:\n        count = size\n\n    if count == 0:\n        return []\n\n    items = self.redis_client.lpop(self.queue_name, count=count)\n    if items is None:\n        return []\n\n    results = [json.loads(item.decode('utf-8')) for item in items]\n\n    logging.debug(f\"Retrieved {len(results)} requests from queue.\")\n    return results\n</code></pre>"}]}